{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fraud_detection_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyf+qwdSNXm5ME3bq8gNDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfolaa/fraud_detection/blob/main/fraud_detection_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBkuHMcekLs0",
        "outputId": "268f31db-5a24-4ae4-f71d-2fce2eaa0e5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlJ4DsLDm3OP"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import glob2\n",
        "import tqdm\n",
        "import json\n",
        "import pandas as pd\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxvxpGmXjwcU",
        "outputId": "a010e335-6608-4628-ddd4-5827852714ae"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v1c4xv3jFmT"
      },
      "source": [
        "def read_json_insert_csv(root_path, json_file, file_csv):\n",
        "  data = json.load(json_file)\n",
        "  df = pd.DataFrame.from_records(data)\n",
        "  # convert file to csv\n",
        "  df.to_csv(f'{root_path}/{file_csv}', \n",
        "            sep='|', \n",
        "            index= None)\n",
        "\n",
        "  # return 1 fichier csv fer json file\n",
        "  return df "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TZcB6c4jX06"
      },
      "source": [
        "def process_json( path_file_json=\"./nirra-log-bot\", dest_path=\"./liberta_leasing\"):\n",
        "  # créer toute l'aborescence du fichier, crée le chemin\n",
        "  os.makedirs(dest_path, exist_ok=True) \n",
        "  # read all json files\n",
        "\n",
        "  json_files = glob2.glob(os.path.join(path_file_json,'*.json'))\n",
        "  json_files = sorted(json_files)\n",
        "  for file_name in tqdm(json_files):\n",
        "    with open(file_name) as json_file:\n",
        "      path_file_csv = file_name.replace(\".json\", \".csv\").split(\"/\")[-1]\n",
        "      read_json_insert_csv(dest_path, json_file, path_file_csv)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA-YDRr_RGOy",
        "outputId": "5700c699-9c0a-4e18-828b-f6159f5e9bbd"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fv5tpRwHg7p"
      },
      "source": [
        "json_files = glob2.glob(os.path.join('./nirra-log-bot','*.json'))\n",
        "#json_files = filter(lambda x: x.endswith('.json'), os.listdir('./nirra-log-bot'))\n",
        "files = sorted(json_files)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJBAro0HsEp"
      },
      "source": [
        "files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5g5SpxFmowA",
        "outputId": "1a993ae4-5180-4d24-9799-edf915a32bff"
      },
      "source": [
        "process_json('./nirra-log-bot', './liberta_leasing')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171/171 [00:03<00:00, 43.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noiqJADKqiG2",
        "outputId": "a2125e2f-609b-40fe-9387-db84fae845a0"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/liberta_leasing/"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/liberta_leasing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFG_FlZCqhVf",
        "outputId": "9f9f0abb-a579-4a5a-9cde-057a7d33d7f3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# use glob to get all the csv files \n",
        "# in the folder\n",
        "path = os.getcwd()\n",
        "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "csv_files = sorted(csv_files)\n",
        "all_df_list = [] # list of all dataframe\n",
        "for f in tqdm(csv_files):    \n",
        "    # read the csv file\n",
        "    #col_names = [\"type\", \"subtype\", \"text\", \"ts\", \"bot_id\"]\n",
        "    df=pd.read_csv(f, sep=\"|\")\n",
        "    all_df_list.append(df) "
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171/171 [00:01<00:00, 145.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzFoFdsXsBqG",
        "outputId": "f7b43e84-6679-4d1f-8597-87bcd8b757f9"
      },
      "source": [
        "print(len(all_df_list))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "8PM0Fv1PL36b",
        "outputId": "34089660-3d29-4e5a-d644-dba36084aec4"
      },
      "source": [
        "all_df_list[0][all_df_list[0]['type']== 'type']"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>subtype</th>\n",
              "      <th>ts</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "      <th>bot_id</th>\n",
              "      <th>bot_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [type, subtype, ts, user, text, bot_id, bot_link]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm7b9wtC3MWT"
      },
      "source": [
        "### Concatener tous la liste des dataframes dans un seul DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeyAENxG1h9m"
      },
      "source": [
        "df_raw = pd.concat(all_df_list, ignore_index=True)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaZ3gAcgEvrt"
      },
      "source": [
        "df_raw.to_csv('/content/drive/MyDrive/datasets/nirra_log_bot.csv', index=None, sep='|')"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyLEBiBuNxJN",
        "outputId": "c963d6c7-4324-4a13-b1d7-8cef75b6f5e3"
      },
      "source": [
        "df_temp = pd.read_csv('/content/drive/MyDrive/datasets/nirra_log_bot.csv', sep='|')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3,6,7,8,9,10,11,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYCxywmRODjf"
      },
      "source": [
        "df_temp.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkOAge1sVOr"
      },
      "source": [
        "def parse_wallet_sms_payload_success(text_type_request):\n",
        "  ''' la fonction permet de parser les types de requete \"Okra WebHook\", \"Wallet success\", \n",
        "      \"SMS Success\" et SMS Payload en object json.\n",
        "      Elle prend en paramètre le text contenu dans le type de requete,\n",
        "      elle retourne un objet de type JSON.'''\n",
        "\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_type_request)\n",
        "  res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "  s = json.loads(res)\n",
        "  out_dict = {} # dictionnary vide\n",
        "  for key, value in s.items():\n",
        "    out_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "\n",
        "\n",
        "  out_dump = json.dumps(out_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "  out_wallet_success = json.loads(out_dump) # convertir le string json en object json.\n",
        "  return out_wallet_success\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXFZHNE_Eft4"
      },
      "source": [
        "def parse_providus_transfer_error_function(text_type_request):\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_type_request)\n",
        "  if len(resul_patt) != 0:\n",
        "    res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "    s = json.loads(res)\n",
        "    out_dict = {} # dictionnary vide\n",
        "    for key, value in s.items():\n",
        "      out_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "    out_dump = json.dumps(out_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "    out_wallet_success = json.loads(out_dump) # convertir le string json en object json.\n",
        "    return out_wallet_success\n",
        "  else:\n",
        "    resultat = 'Transfer to virtual account is not allowed!'\n",
        "    return resultat\n"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv7AQb4Ysb5D"
      },
      "source": [
        "import re\n",
        "\n",
        "# la fonction doit prendre en paramètre quelque chose\n",
        "def parse_and_concatenate_Leadway_Success_Rows(df_raw):\n",
        "  '''Cette fonction permet de parser et de concatener le texte qui a LEADWAY SUCCESS\n",
        "     comme type de requete\n",
        "     elle prend comme paramètre un dataframe et retourne les valeurs suivantes:\n",
        "     - un texte concatené\n",
        "     - l'index de la 1ère ligne qu'on va utiliser ensuite pour l'effacer\n",
        "     - l'index de la dernière ligne qu'on va utiliser ensuite pour l'effacer '''\n",
        "\n",
        "  first_index = 0\n",
        "  last_index = 0\n",
        "  text_leadway_concat = ''\n",
        "  for index, row in df_raw.iterrows():  # boucler sur les colonnes de type text\n",
        "      text_row = row['text']  \n",
        "      if re.search('LEADWAY SUCCESS', text_row):\n",
        "        text_leadway_concat = text_row\n",
        "        first_index = index\n",
        "        first_index +=1\n",
        "        new_df = df_raw[first_index:]\n",
        "        for first_index, new_row in new_df.iterrows():\n",
        "          xxx = new_row['text']      \n",
        "          if not xxx.startswith('['):          \n",
        "            first_index += 1\n",
        "            text_leadway_concat = text_row + xxx       \n",
        "          elif xxx.startswith('['):\n",
        "            last_index = first_index-1\n",
        "            break\n",
        "\n",
        "\n",
        "  return text_leadway_concat, first_index, last_index\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1_on8resgdJ"
      },
      "source": [
        "\n",
        "# use this function when type request is LEADWAY SUCCESS\n",
        "import regex\n",
        "import json\n",
        "\n",
        "\n",
        "def parse_Leadway_Success_Row(text_leadway):\n",
        "  ''' fonction permettant de parser le text concatené pour le type de requet LEADWAY SUCCESS\n",
        "      elle retourner un dictionnaire.'''\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_leadway)\n",
        "  resul_patt[0] = resul_patt[0].replace(\"\\\\\", \"\")\n",
        "  x = resul_patt[0].replace(\"make,\", \"\")\n",
        "  y = x.replace('\"\"makeName\"', '\"makeName\"')\n",
        "  z = json.loads(y)\n",
        "  vehicleMake = z.get('vehicleMake')\n",
        "  leadway_dict = {}\n",
        "  for element in vehicleMake:\n",
        "    leadway_dict[element['id']] = element['makeName']\n",
        "\n",
        "  return leadway_dict"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u470nALIsoTx"
      },
      "source": [
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_Error_Row(error_row):\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"{?[a-z :A-Z 0-9\\\\,=_`']+selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "  res = res.replace(\"'name'\", \"name\").replace(\"`\", \"\").replace(\"'18'\", \"18\").replace(\"'monthly'\", \"monthly\")\n",
        "  res = res+'\"}'\n",
        "  res = res.replace(\"'\", '\"')\n",
        "  s = json.loads(res)\n",
        "  out_error_dict = {} # dictionnary vide\n",
        "  for key, value in s.items():\n",
        "    out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "\n",
        "  out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "  out_error_text = json.loads(out_error_dump) # convertir le string json en object json.\n",
        "  return out_error_text\n"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKcYlDZoRfqD"
      },
      "source": [
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_Error_Row_selfie_Function(error_row):\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"({?[a-z :A-Z 0-9\\\\,=_`']+)selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  # checker si la liste contient au moins un element\n",
        "  if len(resul_patt) !=0 :\n",
        "    res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "    res = res+'\"}'\n",
        "    res = res.replace(\"'monthly'\", \"monthly\").replace(\"'\", '\"').replace('\"\"', '\"').replace('\"monthly \", \"', '\"monthly\"')\n",
        "    s = json.loads(res)\n",
        "    out_error_dict = {} # dictionnary vide\n",
        "    for key, value in s.items():\n",
        "      out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "    out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "    out_error_text = json.loads(out_error_dump)\n",
        "    out_error_text\n",
        "    return out_error_text\n",
        "  #si la liste est vide, on la retourne\n",
        "  else:\n",
        "    return resul_patt"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBN4__k_x7Gf"
      },
      "source": [
        "def parse_Error_Row_selfie_INSERT_INTO_Function(error_row):\n",
        "  error_row = df_raw['text'][28813]\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"({?[a-z :A-Z 0-9\\\\,=_`']+)selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  if len(resul_patt) !=0 :\n",
        "      res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "      res = res+'\"}'\n",
        "      res = res.replace(\"'monthly'\", \"monthly\").replace(\"'\", '\"').replace('\"\"', '\"').replace(\"`\", '\"').replace('monthly, \"', '\"monthly\"').replace('\"name\"', 'name')\n",
        "      res = res.replace('\"product\"', 'product').replace('\"loan_amount\"', 'loan_amount').replace('\"tenor\"', 'tenor').replace('\"loan_purpose\"', 'loan_purpose')\n",
        "      res = res.replace('\"14\"', '14').replace('\"tenor_type\"', 'tenor_type').replace('\"monthly\"', 'monthly')\n",
        "      s = json.loads(res)\n",
        "      out_error_dict = {} # dictionnary vide\n",
        "      for key, value in s.items():\n",
        "        out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "      out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "      out_error_text = json.loads(out_error_dump)\n",
        "      out_error_text\n",
        "      return out_error_text\n",
        "    #si la liste est vide, on la retourne\n",
        "  else:\n",
        "      return resul_patt"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W0AyZz04cn"
      },
      "source": [
        "parse_Error_Row_selfie_INSERT_INTO_Function(df_raw['text'][28813])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "550yp0F11MLD"
      },
      "source": [
        "parse_Error_Row_selfie_INSERT_INTO_Function(df_raw['text'][86636])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZZ0t3_iPhlj"
      },
      "source": [
        "def parse_Error_Row_Function (error_row):\n",
        "  res = []\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  for one_res in resul_patt:\n",
        "    res.append(one_res.replace(\"\\\\\", \" \").replace(\"`\", \"\").replace(\"\\'\", \"\"))\n",
        "  list_json = [json.loads(stuff) for stuff in res]\n",
        "  return list_json"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JECgNVsrrw"
      },
      "source": [
        "import datetime\n",
        "\n",
        "# function to convert date to Timestamp\n",
        "def convertToTimestamp(str):\n",
        "  element = datetime.datetime.strptime(str,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "  return datetime.datetime.timestamp(element)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raIfy0cxVX9z",
        "outputId": "f31406f6-3e4b-4770-8c2e-bbbc2f8cb634"
      },
      "source": [
        "convertToTimestamp('2021-08-12T13:53:23.624Z')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1628776403.624"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWCkinr1V27P"
      },
      "source": [
        "### Prendre la valeur de la colonne Timestamp comme date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8hteMxKsueg"
      },
      "source": [
        "type_request_dictionnary = {}\n",
        "regex_list_api_request = []\n",
        "regex_list_api_request.append('[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "regex_list_api_request.append('/[/a-z 0-9?=&;/_A-Z+]+')\n",
        "regex_list_api_request.append('(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):(\\d\\d).(\\d{3})*[a-zA-Z]')\n",
        "regex_list_api_request.append('[0-9]+')\n",
        "\n",
        "type_request_dictionnary['API REQUEST'] = regex_list_api_request\n",
        "\n",
        "regex_list_client_mobile = []\n",
        "regex_list_client_mobile.append('[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "regex_list_client_mobile.append('(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):(\\d\\d).(\\d{3})*[a-zA-Z]')\n",
        "regex_list_client_mobile.append('[0-9]+')\n",
        "\n",
        "type_request_dictionnary['CLIENT MOBILE LOGIN'] = regex_list_client_mobile\n",
        "\n",
        "type_request_dictionnary['SMS PAYLOAD'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['SMS SUCCESS'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['WALLET SUCCESS'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['LEADWAY SUCCESS'] = '\\{(?:[^{}]|(?R))*}'"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVGf909EUhmO"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU-m87ITsx60"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "\n",
        "def parse_row_error(df_):\n",
        "  log_level_col = []\n",
        "  type_request_col = []\n",
        "  error_code_col = []\n",
        "  error_number_col = []\n",
        "  error_sql_message_col = []\n",
        "  error_sql_state_col = []\n",
        "  error_index_col = []\n",
        "  error_sql_col = []\n",
        "  loan_amount_col = []\n",
        "  loan_purpose_col = []\n",
        "  product_col = []\n",
        "  tenor_col = []\n",
        "  tenor_type_col = []\n",
        "  error_created_by_col = []\n",
        "  error_creator_type_col = []\n",
        "  error_date_created_col = []\n",
        "  error_name_col = []\n",
        "  error_rate_col = []\n",
        "  error_request_col = []\n",
        "  error_status_col = []\n",
        "  error_userID_col = []\n",
        "  date_col = []\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, log_level_col, error_code_col, error_number_col, error_sql_message_col, error_sql_state_col, \n",
        "                    error_index_col, error_sql_col, loan_amount_col, loan_purpose_col, product_col, tenor_col, tenor_type_col,\n",
        "                    error_created_by_col, error_creator_type_col, error_date_created_col, error_name_col, error_rate_col,\n",
        "                    error_request_col, error_status_col, error_userID_col, date_col]\n",
        "\n",
        "  for index, row in df_.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "    \n",
        "    if re.search('error', str_text):\n",
        "      log_level = re.search('error', str_text)\n",
        "      type_of_request = re.search('LOAN ERROR', str_text)\n",
        "      date_col.append(row['ts'])\n",
        "\n",
        "      if 'INSERT INTO' not in str_text:  \n",
        "          try:\n",
        "            log_level_col.append(log_level.group(0))\n",
        "          except AttributeError:\n",
        "            log_level_col.append(None)\n",
        "          try:\n",
        "            type_request_col.append(type_of_request.group(0))\n",
        "          except AttributeError:\n",
        "            type_request_col.append(None)         \n",
        "          try:\n",
        "            loan_error = parse_Error_Row_selfie_Function(str_text) \n",
        "          except json.decoder.JSONDecodeError:\n",
        "            raise          \n",
        "          try:\n",
        "            loan_amount_col.append(loan_error.get('loan_amount'))           \n",
        "          except AttributeError:\n",
        "            loan_amount_col.append(None)\n",
        "          try:\n",
        "            loan_purpose_col.append(loan_error.get('loan_purpose'))              \n",
        "          except AttributeError:\n",
        "            loan_purpose_col.append(None)\n",
        "          try:\n",
        "            product_col.append(loan_error.get('product'))\n",
        "          except AttributeError:\n",
        "            product_col.append(None)\n",
        "          try:\n",
        "            tenor_col.append(loan_error.get('tenor'))\n",
        "          except AttributeError:\n",
        "            tenor_col.append(None)\n",
        "          try:\n",
        "            tenor_type_col.append(loan_error.get('tenor_type'))\n",
        "          except AttributeError:\n",
        "            tenor_type_col.append(None)            \n",
        "          \n",
        "          error_code_col.append(None)\n",
        "          error_number_col.append(None)\n",
        "          error_sql_message_col.append(None)\n",
        "          error_sql_state_col.append(None)\n",
        "          error_index_col.append(None)\n",
        "          error_sql_col.append(None)  \n",
        "          error_userID_col.append(None)\n",
        "          error_status_col.append(None) \n",
        "          error_request_col.append(None)  \n",
        "          error_rate_col.append(None)\n",
        "          error_name_col.append(None)\n",
        "          error_date_created_col.append(None)\n",
        "          error_creator_type_col.append(None)\n",
        "          error_created_by_col.append(None)    \n",
        "      else:     \n",
        "        try:\n",
        "          loan_error = parse_Error_Row_selfie_INSERT_INTO_Function(str_text) \n",
        "          #print(loan_error)\n",
        "        except json.decoder.JSONDecodeError:\n",
        "          raise\n",
        "        loan_amount_col.append(None)\n",
        "        loan_purpose_col.append(None)\n",
        "        product_col.append(None)\n",
        "        tenor_col.append(None)\n",
        "        tenor_type_col.append(None)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "        try:\n",
        "          type_request_col.append(type_of_request.group(0))\n",
        "        except AttributeError:\n",
        "          type_request_col.append(None)\n",
        "        try:\n",
        "          error_code_col.append(loan_error.get('code'))\n",
        "        except AttributeError:\n",
        "          error_code_col.append(None)\n",
        "        try:\n",
        "          error_number_col.append(loan_error.get('errno'))\n",
        "        except AttributeError:\n",
        "          error_number_col.append(None)\n",
        "        try:\n",
        "          error_sql_message_col.append(loan_error.get('sqlMessage'))\n",
        "        except AttributeError:\n",
        "          error_sql_message_col.append(None)\n",
        "        try:\n",
        "          error_sql_state_col.append(loan_error.get('sqlState'))\n",
        "        except AttributeError:\n",
        "          error_sql_state_col.append(None)\n",
        "        try:\n",
        "          error_created_by_col.append(loan_error.get('created_by'))\n",
        "        except AttributeError:\n",
        "          error_created_by_col.append(None)\n",
        "        try:\n",
        "          error_creator_type_col.append(loan_error.get('creator_type'))\n",
        "        except AttributeError:\n",
        "          error_creator_type_col.append(None)\n",
        "        try:\n",
        "          error_date_created_col.append(loan_error.get('date_created'))\n",
        "        except AttributeError:\n",
        "          error_date_created_col.append(None)\n",
        "        try:\n",
        "          error_sql_col.append(loan_error.get('sql'))\n",
        "        except AttributeError:\n",
        "          error_sql_col.append(None)\n",
        "        try:\n",
        "          error_name_col.append(loan_error.get('name'))\n",
        "        except AttributeError:\n",
        "          error_name_col.append(None)\n",
        "        try:\n",
        "          error_rate_col.append(loan_error.get('rate'))\n",
        "        except AttributeError:\n",
        "          error_rate_col.append(None)\n",
        "        try:\n",
        "          error_index_col.append(loan_error.get('index'))\n",
        "        except AttributeError:\n",
        "          error_index_col.append(None)\n",
        "        try:\n",
        "          error_request_col.append(loan_error.get('request'))\n",
        "        except AttributeError:\n",
        "          error_request_col.append(None)\n",
        "        try:\n",
        "          error_status_col.append(loan_error.get('status'))\n",
        "        except AttributeError:\n",
        "          error_status_col.append(None)\n",
        "        try:\n",
        "          error_userID_col.append(loan_error.get('userID'))\n",
        "        except AttributeError:\n",
        "          error_userID_col.append(None)\n",
        "\n",
        "  df_['Type_Request'] = type_request_col\n",
        "  df_['Date'] = date_col\n",
        "  df_['Log_Level'] = log_level_col\n",
        "  df_['Error Code'] = error_code_col\n",
        "  df_['Error Number'] = error_number_col\n",
        "  df_['Error Sql Message'] = error_sql_message_col\n",
        "  df_['Error Sql State'] = error_sql_state_col\n",
        "  df_['Error Index'] = error_index_col\n",
        "  df_['Error Sql'] = error_sql_col\n",
        "  df_['Loan Amount'] = loan_amount_col\n",
        "  df_['Loan Purpose'] = loan_purpose_col\n",
        "  df_['Product'] = product_col\n",
        "  df_['Tenor'] = tenor_col\n",
        "  df_['Tenor Type'] = tenor_type_col\n",
        "  df_['Error created by'] = error_created_by_col\n",
        "  df_['Error creator Type'] = error_creator_type_col \n",
        "  df_['Error Date created'] = error_date_created_col\n",
        "  df_['Error Name'] = error_name_col\n",
        "  df_['Error Rate'] = error_rate_col\n",
        "  df_['Error Request'] = error_request_col\n",
        "  df_['Error Status'] = error_status_col\n",
        "  df_['Error UserID'] = error_userID_col\n",
        "\n",
        "\n",
        "  return df_"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUFaAC7BtJbh"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "error = df_raw[df_raw['text'].str.contains('LOAN ERROR')]\n",
        "df_error = parse_row_error(error)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxv3O9kZDlMa"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_LOAN_PAYLOAD(df_loan_payload):\n",
        "\n",
        "  log_level_col = []\n",
        "  type_request_col = []\n",
        "  loan_amount_col = []\n",
        "  loan_purpose_col = []\n",
        "  product_col = []\n",
        "  tenor_col = []\n",
        "  tenor_type_col = []\n",
        "  date_col = []\n",
        "\n",
        "\n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, log_level_col, loan_amount_col, loan_purpose_col, product_col, tenor_col, tenor_type_col, date_col]\n",
        "          \n",
        "  for index, row in df_loan_payload.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "    \n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        type_of_request = re.search('LOAN PAYLOAD', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "        try:\n",
        "          type_request_col.append(type_of_request.group(0))\n",
        "        except AttributeError:\n",
        "          type_request_col.append(None)         \n",
        "        try:\n",
        "          loan_error = parse_Error_Row_selfie_Function(str_text) \n",
        "        except json.decoder.JSONDecodeError:\n",
        "          raise \n",
        "        try:\n",
        "            loan_amount_col.append(loan_error.get('loan_amount'))           \n",
        "        except AttributeError:\n",
        "            loan_amount_col.append(None)\n",
        "        try:\n",
        "          loan_purpose_col.append(loan_error.get('loan_purpose'))              \n",
        "        except AttributeError:\n",
        "          loan_purpose_col.append(None)\n",
        "        try:\n",
        "          product_col.append(loan_error.get('product'))\n",
        "        except AttributeError:\n",
        "          product_col.append(None)\n",
        "        try:\n",
        "          tenor_col.append(loan_error.get('tenor'))\n",
        "        except AttributeError:\n",
        "          tenor_col.append(None)\n",
        "        try:\n",
        "          tenor_type_col.append(loan_error.get('tenor_type'))\n",
        "        except AttributeError:\n",
        "          tenor_type_col.append(None)  \n",
        "\n",
        "  \n",
        "  df_loan_payload['Type_Request'] = type_request_col\n",
        "  df_loan_payload['Log_Level'] = log_level_col\n",
        "  df_loan_payload['Loan Amount'] = loan_amount_col\n",
        "  df_loan_payload['Loan Purpose'] = loan_purpose_col\n",
        "  df_loan_payload['Product'] = product_col\n",
        "  df_loan_payload['Tenor'] = tenor_col\n",
        "  df_loan_payload['Tenor Type'] = tenor_type_col\n",
        "  df_loan_payload['Date'] = date_col\n",
        "\n",
        "  return df_loan_payload\n"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5cXVXeZUOua"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "loan_payload = df_raw[df_raw['text'].str.contains('LOAN PAYLOAD')]\n",
        "df_loan_payload = parse_row_LOAN_PAYLOAD(loan_payload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQowf3DowAM"
      },
      "source": [
        "df_loan_payload.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my38DIJIaPcF"
      },
      "source": [
        "### Handle DataFrame for API REQUEST Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O9Ghnbls4hN"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_api_request(df_api_request):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_api_request = []\n",
        "  list_column_none_api_request = [message_sms_payload_col, totalsent_col, cost_col, status_col,\n",
        "                                  bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, account_number_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_api_request.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)        \n",
        "        # check if the row contains an email address \n",
        "        # pour tous les types request créer un dictionnaire dans lequel mapper\n",
        "        # key = type de request et value = les regex définis\n",
        "        # pour chaque condition IF créer une liste de colonnes auxquelles affecter None\n",
        "        if 'mailto' in str_text:\n",
        "            if re.search('API REQUEST', str_text):\n",
        "                type_of_request = re.search('API REQUEST', str_text)                \n",
        "                phone_or_email_api_req = re.search(type_request_dictionnary['API REQUEST'][0], str_text)                              \n",
        "                endpoint = re.search(type_request_dictionnary['API REQUEST'][1], str_text)\n",
        "                pattern = type_request_dictionnary['API REQUEST'][2]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for i in range(len(list_column_none_api_request)):\n",
        "                  list_column_none_api_request[i].append(None)\n",
        "                               \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0)) # add type request inside type request column\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)               \n",
        "                try:\n",
        "                  email_col.append(phone_or_email_api_req.group(0)) # add email inside email column\n",
        "                  phone_Col.append(None)  # in this case there is no phone number\n",
        "                except AttributeError:\n",
        "                  email_col.append(None)\n",
        "                try:\n",
        "                  endpoint_col.append(endpoint.group(0)) # add endpoint inside endpoint column\n",
        "                except AttributeError:\n",
        "                  endpoint_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None)\n",
        "              \n",
        "\n",
        "        elif 'mailto' not in str_text:\n",
        "            if re.search('API REQUEST', str_text):\n",
        "                type_of_request = re.search('API REQUEST', str_text)                            \n",
        "                # extract a phone number for API REQUEST\n",
        "                phone_or_email_api_req = re.search(type_request_dictionnary['API REQUEST'][3], str_text)                              \n",
        "                endpoint = re.search(type_request_dictionnary['API REQUEST'][1], str_text)\n",
        "                pattern = type_request_dictionnary['API REQUEST'][2]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for i in range(len(list_column_none_api_request)):\n",
        "                  list_column_none_api_request[i].append(None)\n",
        "\n",
        "                try:\n",
        "                  phone_Col.append(phone_or_email_api_req.group(0)) # add phone number inside phone number column\n",
        "                  email_col.append(None) # in this case there is no email address\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  endpoint_col.append(endpoint.group(0)) # add endpoint inside endpoint column\n",
        "                except AttributeError:\n",
        "                  endpoint_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None)\n",
        "\n",
        "  df_api_request['Type_Request'] = type_request_col\n",
        "  df_api_request['Phone_Number'] = phone_Col\n",
        "  df_api_request['Date'] = date_col\n",
        "  df_api_request['EndPoint'] = endpoint_col\n",
        "  df_api_request['Log_Level'] = log_level_col\n",
        "  df_api_request['Email'] = email_col\n",
        "  df_api_request['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_api_request['Total Sent'] = totalsent_col\n",
        "  df_api_request['Cost'] = cost_col\n",
        "  df_api_request['Status'] = status_col\n",
        "  df_api_request['Account Number'] = account_number_col\n",
        "  df_api_request['Account Name'] = account_name_col\n",
        "  df_api_request['BVN'] = bvn_col\n",
        "  df_api_request['Request Successful'] = requestSuccessful_col\n",
        "  df_api_request['Response Message'] = responseMessage_col\n",
        "  df_api_request['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_api_request"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWb_8xSS8-wo"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XIIds-DtBOP"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "api_request = df_raw[df_raw['text'].str.contains('API REQUEST')]\n",
        "df_api_request = parse_row_api_request(api_request)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQURC4I9brJm"
      },
      "source": [
        "df_api_request.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R69yc9BaHdc"
      },
      "source": [
        "### Handle DataFrame for CLIENT MOBILE LOGIN Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj661cCus7YS"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_client_mobile_login(df_client_mob):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_client_mobile = []\n",
        "  list_column_none_client_mobile = [message_sms_payload_col, totalsent_col, cost_col, status_col,\n",
        "                                  account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_client_mob.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)        \n",
        "        # check if the row contains an email address \n",
        "        # pour tous les types request créer un dictionnaire dans lequel mapper\n",
        "        # key = type de request et value = les regex définis\n",
        "        # pour chaque condition IF créer une liste de colonnes auxquelles affecter None\n",
        "        if 'mailto' in str_text:\n",
        "            if re.search('CLIENT MOBILE LOGIN', str_text):   # CLIENT MOBILE LOGIN with email address\n",
        "                  type_of_request = re.search('CLIENT MOBILE LOGIN', str_text)\n",
        "                  # extract address email for CLIENT MOBILE LOGIN\n",
        "                  phone_or_email_client_mobile = re.search(type_request_dictionnary['CLIENT MOBILE LOGIN'][0], str_text)                               \n",
        "                  pattern = type_request_dictionnary['CLIENT MOBILE LOGIN'][1]\n",
        "                  datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                  datematcher = datepattern.search(str_text)  # extract date for CLIENT MOBILE LOGIN type request\n",
        "                  \n",
        "                  for j in range(len(list_column_none_client_mobile)):\n",
        "                    list_column_none_client_mobile[j].append(None)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0)) # add type request inside type request column\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None) \n",
        "                  try:\n",
        "                    email_col.append(phone_or_email_client_mobile.group(0)) # add email inside email column\n",
        "                    phone_Col.append(None)  # in this case there is no phone number\n",
        "                  except AttributeError:\n",
        "                    email_col.append(None)\n",
        "                  try:\n",
        "                    date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                  except AttributeError:\n",
        "                    date_col.append(None)\n",
        "\n",
        "        elif 'mailto' not in str_text:\n",
        "            if re.search('CLIENT MOBILE LOGIN', str_text): # when type request is CLIENT MOBILE LOGIN, there is no EndPoint\n",
        "                type_of_request = re.search('CLIENT MOBILE LOGIN', str_text)\n",
        "                # extract a phone number for CLIENT MOBILE LOGIN\n",
        "                phone_or_email_client_mobile = re.search(type_request_dictionnary['CLIENT MOBILE LOGIN'][2], str_text)                  \n",
        "                pattern = type_request_dictionnary['CLIENT MOBILE LOGIN'][1]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for j in range(len(list_column_none_client_mobile)):\n",
        "                    list_column_none_client_mobile[j].append(None)\n",
        "\n",
        "                try:\n",
        "                  phone_Col.append(phone_or_email_client_mobile.group(0))\n",
        "                  email_col.append(None)\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None) \n",
        "\n",
        "  df_client_mob['Type_Request'] = type_request_col\n",
        "  df_client_mob['Phone_Number'] = phone_Col\n",
        "  df_client_mob['Date'] = date_col\n",
        "  df_client_mob['EndPoint'] = endpoint_Col\n",
        "  df_client_mob['Log_Level'] = log_level_col\n",
        "  df_client_mob['Email'] = email_col\n",
        "  df_client_mob['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_client_mob['Total Sent'] = totalsent_col\n",
        "  df_client_mob['Cost'] = cost_col\n",
        "  df_client_mob['Status'] = status_col\n",
        "  df_client_mob['Account Number'] = account_number_col\n",
        "  df_client_mob['Account Name'] = account_name_col\n",
        "  df_client_mob['BVN'] = bvn_col\n",
        "  df_client_mob['Request Successful'] = requestSuccessful_col\n",
        "  df_client_mob['Response Message'] = responseMessage_col\n",
        "  df_client_mob['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_client_mob"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnqWyhXp3U-5"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "client_mobile_login = df_raw[df_raw['text'].str.contains('CLIENT MOBILE LOGIN')]\n",
        "df_client_mobile_login = parse_row_client_mobile_login(client_mobile_login)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSzleZV-3t3Z"
      },
      "source": [
        "df_client_mobile_login.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpBB-0cg3hAG"
      },
      "source": [
        "Handle DataFrame for SMS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XVntz9o3mbD"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_sms_payload_function(df_sms_payload):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  \n",
        "  list_column_none_sms_payload = []\n",
        "  list_column_none_sms_payload = [totalsent_col, cost_col, status_col,\n",
        "                                  account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, email_col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_sms_payload.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('SMS PAYLOAD', str_text):\n",
        "                type_of_request = re.search('SMS PAYLOAD', str_text)            \n",
        "                sms_payload = parse_wallet_sms_payload_success(str_text)               \n",
        "                for l in range(len(list_column_none_sms_payload)):\n",
        "                    list_column_none_sms_payload[l].append(None)             \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  phone_Col.append(sms_payload.get('phone'))\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  message_sms_payload_col.append(sms_payload.get('message'))\n",
        "                except AttributeError:\n",
        "                  message_sms_payload_col.append(None)\n",
        "\n",
        "\n",
        "  df_sms_payload['Type_Request'] = type_request_col\n",
        "  df_sms_payload['Phone_Number'] = phone_Col\n",
        "  df_sms_payload['Date'] = date_col\n",
        "  df_sms_payload['EndPoint'] = endpoint_Col\n",
        "  df_sms_payload['Log_Level'] = log_level_col\n",
        "  df_sms_payload['Email'] = email_col\n",
        "  df_sms_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_sms_payload['Total Sent'] = totalsent_col\n",
        "  df_sms_payload['Cost'] = cost_col\n",
        "  df_sms_payload['Status'] = status_col\n",
        "  df_sms_payload['Account Number'] = account_number_col\n",
        "  df_sms_payload['Account Name'] = account_name_col\n",
        "  df_sms_payload['BVN'] = bvn_col\n",
        "  df_sms_payload['Request Successful'] = requestSuccessful_col\n",
        "  df_sms_payload['Response Message'] = responseMessage_col\n",
        "  df_sms_payload['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_sms_payload\n"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOM6gkyh3-B0"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "sms_payload = df_raw[df_raw['text'].str.contains('SMS PAYLOAD')]\n",
        "df_sms_payload = parse_row_sms_payload_function(sms_payload)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzrdhU6Kkbat"
      },
      "source": [
        "df_sms_payload.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrJjMbac4HGd"
      },
      "source": [
        "Handle DataFrame for SMS Success Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8J_KpN4H7J"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_sms_success_function(df_sms_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  \n",
        "  list_column_none_sms_success = []\n",
        "  list_column_none_sms_success = [message_sms_payload_col, account_number_col, bvn_col, requestSuccessful_col, \n",
        "                                  responseMessage_col, responseCode_col, account_name_col, email_col, \n",
        "                                  phone_Col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_sms_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('SMS SUCCESS', str_text): \n",
        "                type_of_request = re.search('SMS SUCCESS', str_text)\n",
        "                sms_success = parse_wallet_sms_payload_success(str_text)                \n",
        "                for m in range(len(list_column_none_sms_success)):\n",
        "                    list_column_none_sms_success[m].append(None) \n",
        "           \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:                 \n",
        "                  totalsent_col.append(sms_success.get('response').get('totalsent '))\n",
        "                except AttributeError:\n",
        "                  totalsent_col.append(None)\n",
        "                try:                 \n",
        "                  cost_col.append(sms_success.get('response').get('cost '))\n",
        "                except AttributeError:\n",
        "                  cost_col.append(None)\n",
        "                try:                 \n",
        "                  status_col.append(sms_success.get('response').get('status '))\n",
        "                except AttributeError:\n",
        "                  status_col.append(None)\n",
        "                     \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)    \n",
        "\n",
        "  df_sms_success['Type_Request'] = type_request_col\n",
        "  df_sms_success['Phone_Number'] = phone_Col\n",
        "  df_sms_success['Date'] = date_col\n",
        "  df_sms_success['EndPoint'] = endpoint_Col\n",
        "  df_sms_success['Log_Level'] = log_level_col\n",
        "  df_sms_success['Email'] = email_col\n",
        "  df_sms_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_sms_success['Total Sent'] = totalsent_col\n",
        "  df_sms_success['Cost'] = cost_col\n",
        "  df_sms_success['Status'] = status_col\n",
        "  df_sms_success['Account Number'] = account_number_col\n",
        "  df_sms_success['Account Name'] = account_name_col\n",
        "  df_sms_success['BVN'] = bvn_col\n",
        "  df_sms_success['Request Successful'] = requestSuccessful_col\n",
        "  df_sms_success['Response Message'] = responseMessage_col\n",
        "  df_sms_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_sms_success\n"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE_Z6cSQ4Ou9"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "sms_success = df_raw[df_raw['text'].str.contains('SMS SUCCESS')]\n",
        "df_sms_success = parse_row_sms_success_function(sms_success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMY6oNKUkoPE"
      },
      "source": [
        "df_sms_success.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgN9dHMi4WhI"
      },
      "source": [
        "Handle DataFrame for WALLET SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqjCjokx4X-5"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_wallet_success_function(df_wallet_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  message_error_col = []\n",
        "\n",
        "  list_column_none_wallet_success = []\n",
        "  list_column_none_wallet_success = [totalsent_col, message_sms_payload_col, cost_col, status_col, \n",
        "                                     email_col, phone_Col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "  \n",
        "  for index, row in df_wallet_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "\n",
        "        if re.search('WALLET SUCCESS', str_text):\n",
        "            type_of_request = re.search('WALLET SUCCESS', str_text)           \n",
        "\n",
        "            if 'Faithfully yours, nginx' in str_text:\n",
        "                error_text = 'Sorry, the page you are looking for is currently unavailable, Please try again later.'\n",
        "                message_error_col.append(error_text)\n",
        "                type_request_col.append(type_of_request.group(0))\n",
        "                account_number_col.append(None)\n",
        "                account_name_col.append(None)\n",
        "                bvn_col.append(None)\n",
        "                requestSuccessful_col.append(None)\n",
        "                responseMessage_col.append(None)\n",
        "                responseCode_col.append(None)\n",
        "            else:\n",
        "                try:\n",
        "                  wallet_success = parse_providus_transfer_error_function(str_text)\n",
        "                except json.decoder.JSONDecodeError:\n",
        "                  #print(str_text)\n",
        "                  raise\n",
        "                message_error_col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  account_number_col.append(wallet_success.get('account_number'))\n",
        "                except AttributeError:\n",
        "                  account_number_col.append(None)\n",
        "                try:\n",
        "                  account_name_col.append(wallet_success.get('account_name'))\n",
        "                except AttributeError:\n",
        "                  account_name_col.append(None)\n",
        "                try:\n",
        "                  bvn_col.append(wallet_success.get('bvn'))\n",
        "                except AttributeError:\n",
        "                  bvn_col.append(None)\n",
        "                try:\n",
        "                  requestSuccessful_col.append(wallet_success.get('requestSuccessful'))\n",
        "                except AttributeError:\n",
        "                  requestSuccessful_col.append(None)\n",
        "                try:\n",
        "                  responseMessage_col.append((wallet_success.get('responseMessage')))\n",
        "                except AttributeError:\n",
        "                  responseMessage_col.append(None)\n",
        "                try:\n",
        "                  responseCode_col.append((wallet_success.get('responseCode')))\n",
        "                except AttributeError:\n",
        "                  responseCode_col.append(None)\n",
        "\n",
        "            for n in range(len(list_column_none_wallet_success)):\n",
        "              list_column_none_wallet_success[n].append(None) \n",
        "\n",
        "                       \n",
        "\n",
        "  df_wallet_success['Type_Request'] = type_request_col\n",
        "  df_wallet_success['Phone_Number'] = phone_Col\n",
        "  df_wallet_success['Date'] = date_col\n",
        "  df_wallet_success['EndPoint'] = endpoint_Col\n",
        "  df_wallet_success['Log_Level'] = log_level_col\n",
        "  df_wallet_success['Email'] = email_col\n",
        "  df_wallet_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_wallet_success['Total Sent'] = totalsent_col\n",
        "  df_wallet_success['Cost'] = cost_col\n",
        "  df_wallet_success['Status'] = status_col\n",
        "  df_wallet_success['Account Number'] = account_number_col\n",
        "  df_wallet_success['Account Name'] = account_name_col\n",
        "  df_wallet_success['BVN'] = bvn_col\n",
        "  df_wallet_success['Request Successful'] = requestSuccessful_col\n",
        "  df_wallet_success['Response Message'] = responseMessage_col\n",
        "  df_wallet_success['Response Code'] = responseCode_col\n",
        "  df_wallet_success['Error Message Wallet'] = message_error_col\n",
        " \n",
        "  return df_wallet_success"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnoeVazJ4fZE"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "wallet_success = df_raw[df_raw['text'].str.contains('WALLET SUCCESS')]\n",
        "df_wallet_success = parse_row_wallet_success_function(wallet_success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5NWvVoga5r-"
      },
      "source": [
        "df_wallet_success.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYItsxk8Ion"
      },
      "source": [
        "### Handle DataFrame for OKRA WEBHOOK Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ2J1GQ88N4b"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_okra_webhook_function(df_okra):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  accountId_col = []\n",
        "  authorization_v_col = []\n",
        "  authorization_id_col = []\n",
        "  authorization_customer_col = []\n",
        "  authorization_account_col = []\n",
        "  authorization_account_id_col = []\n",
        "  authorization_account_manual_col = []\n",
        "  authorization_account_name_col = []\n",
        "  authorization_account_nuban_col = []\n",
        "  authorization_account_bank_col = []\n",
        "  authorization_account_created_at_col = []\n",
        "  authorization_account_last_updated_col = []\n",
        "  authorization_account_balance_col = []\n",
        "  authorization_account_customer_col = []\n",
        "  authorization_account_type_col = []\n",
        "  authorization_account_currency_col = []\n",
        "  authorization_accounts_col = []\n",
        "  authorization_amount_col = []\n",
        "  authorization_bank_col = []\n",
        "  authorization_created_at_col = []\n",
        "  authorization_currency_col = []\n",
        "  authorization_customerDetails_col = []\n",
        "  authorization_disconnect_col = []\n",
        "  authorization_disconnected_at_col = []\n",
        "  authorization_duration_col = []\n",
        "  authorization_env_col = []\n",
        "  authorization_garnish_col = []\n",
        "  authorization_initialAmount_col = []\n",
        "  authorization_initiated_col = []\n",
        "  authorization_last_updated_col = []\n",
        "  authorization_link_col = []\n",
        "  authorization_next_payment_col = []\n",
        "  authorization_owner_col = []\n",
        "  authorization_payLink_col = []\n",
        "  authorization_type_col = []\n",
        "  authorization_used_col = []\n",
        "  authorizationId_col = []\n",
        "  bankId_col = []\n",
        "  bankName_col = []\n",
        "  bankSlug_col = []\n",
        "  bankType_col = []\n",
        "  callbackURL_col = []\n",
        "  callback_code_col = []\n",
        "  callback_type_col = []\n",
        "  callback_url_col = []\n",
        "  code_col = []\n",
        "  country_col = []\n",
        "  current_project_col = []\n",
        "  customerEmail_col = []\n",
        "  customerId_col = []\n",
        "  ended_at_col = []\n",
        "  env_col = []\n",
        "  extras_col = []\n",
        "  identityType_col = []\n",
        "  login_type_col = []\n",
        "  message_col = []\n",
        "  meta_col = []\n",
        "  method_col = []\n",
        "  options_col = []\n",
        "  owner_col = []\n",
        "  record_col = []\n",
        "  recordId_col = []\n",
        "  started_at_col = []\n",
        "  status_webhook_col = []\n",
        "  token_col = []\n",
        "  type_col = []\n",
        "\n",
        "  list_column_none_okra_webhook = []\n",
        "  list_column_none_okra_webhook = [api_request_col, account_number_col, account_name_col, totalsent_col, \n",
        "                                   message_sms_payload_col, cost_col, status_col, responseCode_col,\n",
        "                                   bvn_col, requestSuccessful_col, responseMessage_col, email_col, phone_Col, \n",
        "                                   endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_okra.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        date_col.append(row['ts'])\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('OKRA WEBHOOK', str_text):\n",
        "                  type_of_request = re.search('OKRA WEBHOOK', str_text)\n",
        "                  okra_webhook = parse_wallet_sms_payload_success(str_text) \n",
        "                  if 'authorization' in str_text:\n",
        "                      accountId_col.append(okra_webhook.get('accountId'))\n",
        "                      authorization_v_col.append(okra_webhook.get('authorization').get('__v '))\n",
        "                      authorization_id_col.append(okra_webhook.get('authorization').get('_id '))\n",
        "                      authorization_customer_col.append(okra_webhook.get('authorization').get('customer '))\n",
        "                      authorization_account_col.append(okra_webhook.get('authorization').get('account '))\n",
        "                      authorization_account_id_col.append(okra_webhook.get('authorization').get('account ')[0].get('_id '))\n",
        "                      authorization_account_manual_col.append(okra_webhook.get('authorization').get('account ')[0].get('manual '))\n",
        "                      authorization_account_name_col.append(okra_webhook.get('authorization').get('account ')[0].get('name '))\n",
        "                      authorization_account_nuban_col.append(okra_webhook.get('authorization').get('account ')[0].get('nuban '))\n",
        "                      authorization_account_bank_col.append(okra_webhook.get('authorization').get('account ')[0].get('bank '))\n",
        "                      authorization_account_created_at_col.append(okra_webhook.get('authorization').get('account ')[0].get('created_at '))\n",
        "                      authorization_account_last_updated_col.append(okra_webhook.get('authorization').get('account ')[0].get('last_updated '))\n",
        "                      authorization_account_balance_col.append(okra_webhook.get('authorization').get('account ')[0].get('balance '))\n",
        "                      authorization_account_customer_col.append(okra_webhook.get('authorization').get('account ')[0].get('customer '))\n",
        "                      authorization_account_type_col.append(okra_webhook.get('authorization').get('account ')[0].get('type '))\n",
        "                      authorization_account_currency_col.append(okra_webhook.get('authorization').get('account ')[0].get('currency '))\n",
        "                      authorization_accounts_col.append(okra_webhook.get('authorization').get('accounts '))\n",
        "                      authorization_amount_col.append(okra_webhook.get('authorization').get('amount '))\n",
        "                      authorization_bank_col.append(okra_webhook.get('authorization').get('bank '))\n",
        "                      authorization_created_at_col.append(okra_webhook.get('authorization').get('created_at '))\n",
        "                      authorization_currency_col.append(okra_webhook.get('authorization').get('currency '))\n",
        "                      authorization_customerDetails_col.append(okra_webhook.get('authorization').get('customerDetails '))\n",
        "                      authorization_disconnect_col.append(okra_webhook.get('authorization').get('disconnect '))\n",
        "                      authorization_disconnected_at_col.append(okra_webhook.get('authorization').get('disconnected_at '))\n",
        "                      authorization_duration_col.append(okra_webhook.get('authorization').get('duration '))\n",
        "                      authorization_env_col.append(okra_webhook.get('authorization').get('env '))\n",
        "                      authorization_garnish_col.append(okra_webhook.get('authorization').get('garnish '))\n",
        "                      authorization_initialAmount_col.append(okra_webhook.get('authorization').get('initialAmount '))\n",
        "                      authorization_initiated_col.append(okra_webhook.get('authorization').get('initiated '))\n",
        "                      authorization_last_updated_col.append(okra_webhook.get('authorization').get('last_updated '))\n",
        "                      authorization_link_col.append(okra_webhook.get('authorization').get('link '))\n",
        "                      authorization_next_payment_col.append(okra_webhook.get('authorization').get('next_payment '))\n",
        "                      authorization_owner_col.append(okra_webhook.get('authorization').get('owner '))\n",
        "                      authorization_payLink_col.append(okra_webhook.get('authorization').get('payLink '))\n",
        "                      authorization_type_col.append(okra_webhook.get('authorization').get('type '))\n",
        "                      authorization_used_col.append(okra_webhook.get('authorization').get('used '))\n",
        "                      authorizationId_col.append(None)\n",
        "                      bankId_col.append(None)\n",
        "                      bankName_col.append(None)\n",
        "                      bankSlug_col.append(None)\n",
        "                      bankType_col.append(None)\n",
        "                      callbackURL_col.append(None)\n",
        "                      callback_code_col.append(None)\n",
        "                      callback_type_col.append(None)\n",
        "                      callback_url_col.append(None)\n",
        "                      code_col.append(None)\n",
        "                      country_col.append(None)\n",
        "                      current_project_col.append(None)\n",
        "                      customerEmail_col.append(None)\n",
        "                      customerId_col.append(None)\n",
        "                      ended_at_col.append(None)\n",
        "                      env_col.append(None)\n",
        "                      extras_col.append(None)\n",
        "                      identityType_col.append(None)\n",
        "                      login_type_col.append(None)\n",
        "                      message_col.append(None)\n",
        "                      meta_col.append(None)\n",
        "                      method_col.append(None)\n",
        "                      options_col.append(None)\n",
        "                      owner_col.append(None)\n",
        "                      record_col.append(None)\n",
        "                      recordId_col.append(None)\n",
        "                      started_at_col.append(None)\n",
        "                      status_webhook_col.append(None)\n",
        "                      token_col.append(None)\n",
        "                      type_col.append(None)\n",
        "                      try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                      except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                  else:\n",
        "                      authorizationId_col.append(okra_webhook.get('authorizationId'))\n",
        "                      bankId_col.append(okra_webhook.get('bankId'))\n",
        "                      bankName_col.append(okra_webhook.get('bankName'))\n",
        "                      bankSlug_col.append(okra_webhook.get('bankSlug'))\n",
        "                      bankType_col.append(okra_webhook.get('bankType'))\n",
        "                      callbackURL_col.append(okra_webhook.get('callbackURL'))\n",
        "                      callback_code_col.append(okra_webhook.get('callback_code'))\n",
        "                      callback_type_col.append(okra_webhook.get('callback_type'))\n",
        "                      callback_url_col.append(okra_webhook.get('callback_url'))\n",
        "                      code_col.append(okra_webhook.get('code'))\n",
        "                      country_col.append(okra_webhook.get('country'))\n",
        "                      current_project_col.append(okra_webhook.get('current_project'))\n",
        "                      customerEmail_col.append(okra_webhook.get('customerEmail'))\n",
        "                      customerId_col.append(okra_webhook.get('customerId'))\n",
        "                      ended_at_col.append(okra_webhook.get('ended_at'))\n",
        "                      env_col.append(okra_webhook.get('env'))\n",
        "                      extras_col.append(okra_webhook.get('extras'))\n",
        "                      identityType_col.append(okra_webhook.get('identityType'))\n",
        "                      login_type_col.append(okra_webhook.get('login_type'))\n",
        "                      message_col.append(okra_webhook.get('message'))\n",
        "                      meta_col.append(okra_webhook.get('meta'))\n",
        "                      method_col.append(okra_webhook.get('method'))\n",
        "                      options_col.append(okra_webhook.get('options'))\n",
        "                      owner_col.append(okra_webhook.get('owner'))\n",
        "                      record_col.append(okra_webhook.get('record'))\n",
        "                      recordId_col.append(okra_webhook.get('recordId'))\n",
        "                      started_at_col.append(okra_webhook.get('started_at'))\n",
        "                      status_webhook_col.append(okra_webhook.get('status'))\n",
        "                      token_col.append(okra_webhook.get('token'))\n",
        "                      type_col.append(okra_webhook.get('type'))\n",
        "                      accountId_col.append(None)\n",
        "                      authorization_v_col.append(None)\n",
        "                      authorization_id_col.append(None)\n",
        "                      authorization_customer_col.append(None)\n",
        "                      authorization_account_col.append(None)\n",
        "                      authorization_account_id_col.append(None)\n",
        "                      authorization_account_manual_col.append(None)\n",
        "                      authorization_account_name_col.append(None)\n",
        "                      authorization_account_nuban_col.append(None)\n",
        "                      authorization_account_bank_col.append(None)\n",
        "                      authorization_account_created_at_col.append(None)\n",
        "                      authorization_account_last_updated_col.append(None)\n",
        "                      authorization_account_balance_col.append(None)\n",
        "                      authorization_account_customer_col.append(None)\n",
        "                      authorization_account_type_col.append(None)\n",
        "                      authorization_account_currency_col.append(None)\n",
        "                      authorization_accounts_col.append(None)\n",
        "                      authorization_amount_col.append(None)\n",
        "                      authorization_bank_col.append(None)\n",
        "                      authorization_created_at_col.append(None)\n",
        "                      authorization_currency_col.append(None)\n",
        "                      authorization_customerDetails_col.append(None)\n",
        "                      authorization_disconnect_col.append(None)\n",
        "                      authorization_disconnected_at_col.append(None)\n",
        "                      authorization_duration_col.append(None)\n",
        "                      authorization_env_col.append(None)\n",
        "                      authorization_garnish_col.append(None)\n",
        "                      authorization_initialAmount_col.append(None)\n",
        "                      authorization_initiated_col.append(None)\n",
        "                      authorization_last_updated_col.append(None)\n",
        "                      authorization_link_col.append(None)\n",
        "                      authorization_next_payment_col.append(None)\n",
        "                      authorization_owner_col.append(None)\n",
        "                      authorization_payLink_col.append(None)\n",
        "                      authorization_type_col.append(None)\n",
        "                      authorization_used_col.append(None)\n",
        "                      try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                      except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                  \n",
        "                  for n in range(len(list_column_none_okra_webhook)):\n",
        "                    list_column_none_okra_webhook[n].append(None) \n",
        "\n",
        "       \n",
        "  df_okra['Type_Request'] = type_request_col\n",
        "  df_okra['Phone_Number'] = phone_Col\n",
        "  df_okra['Date'] = date_col\n",
        "  df_okra['EndPoint'] = endpoint_Col\n",
        "  df_okra['Log_Level'] = log_level_col\n",
        "  df_okra['Email'] = email_col\n",
        "  df_okra['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_okra['Total Sent'] = totalsent_col\n",
        "  df_okra['Cost'] = cost_col\n",
        "  df_okra['Status'] = status_col\n",
        "  df_okra['Account Number'] = account_number_col\n",
        "  df_okra['Account Name'] = account_name_col\n",
        "  df_okra['BVN'] = bvn_col\n",
        "  df_okra['Request Successful'] = requestSuccessful_col\n",
        "  df_okra['Response Message'] = responseMessage_col\n",
        "  df_okra['Response Code'] = responseCode_col\n",
        "  df_okra['Account Id'] = accountId_col\n",
        "  df_okra['Authorization_V'] = authorization_v_col\n",
        "  df_okra['Authorization_Id'] = authorization_id_col\n",
        "  df_okra['Authorization_Customer'] = authorization_customer_col\n",
        "  df_okra['Authorization_Owner'] = authorization_owner_col\n",
        "  df_okra['Authorization_Account'] = authorization_account_col\n",
        "  df_okra['Authorization_account_Id'] = authorization_account_id_col\n",
        "  df_okra['Authorization_account_manual'] = authorization_account_manual_col\n",
        "  df_okra['Authorization_account_name'] = authorization_account_name_col\n",
        "  df_okra['Authorization_account_nuban'] = authorization_account_nuban_col\n",
        "  df_okra['Authorization_account_bank'] = authorization_account_bank_col\n",
        "  df_okra['Authorization_account_created_at'] = authorization_account_created_at_col\n",
        "  df_okra['Authorization_account_last_updated'] = authorization_account_last_updated_col\n",
        "  df_okra['Authorization_account_balance'] = authorization_account_balance_col\n",
        "  df_okra['Authorization_account_customer'] = authorization_account_customer_col\n",
        "  df_okra['Authorization_account_type'] = authorization_account_type_col\n",
        "  df_okra['Authorization_account_currency'] = authorization_account_currency_col\n",
        "  df_okra['Authorization_accounts'] = authorization_accounts_col\n",
        "  df_okra['Authorization_amount'] = authorization_amount_col\n",
        "  df_okra['Authorization_bank'] = authorization_bank_col\n",
        "  df_okra['Authorization_created_at'] = authorization_created_at_col\n",
        "  df_okra['Authorization_currency'] = authorization_currency_col \n",
        "  df_okra['Authorization_customerDetails'] = authorization_customerDetails_col\n",
        "  df_okra['Authorization_disconnect'] = authorization_disconnect_col\n",
        "  df_okra['Authorization_disconnected_at'] = authorization_disconnected_at_col\n",
        "  df_okra['Authorization_duration'] = authorization_duration_col\n",
        "  df_okra['Authorization_env'] = authorization_env_col\n",
        "  df_okra['Authorization_garnish'] = authorization_garnish_col\n",
        "  df_okra['Authorization_initialAmount'] = authorization_initialAmount_col\n",
        "  df_okra['Authorization_initiated'] = authorization_initiated_col\n",
        "  df_okra['Authorization_last_updated'] = authorization_last_updated_col\n",
        "  df_okra['Authorization_link'] = authorization_link_col\n",
        "  df_okra['Authorization_next_payment'] = authorization_next_payment_col\n",
        "  df_okra['Authorization_payLink'] = authorization_payLink_col\n",
        "  df_okra['Authorization_type'] = authorization_type_col\n",
        "  df_okra['Authorization_used'] = authorization_used_col\n",
        "  df_okra['AuthorizationId'] = authorizationId_col\n",
        "  df_okra['BankId'] = bankId_col\n",
        "  df_okra['BankName'] = bankName_col\n",
        "  df_okra['bankSlug'] = bankSlug_col\n",
        "  df_okra['bankType'] = bankType_col\n",
        "  df_okra['callbackURL'] = callbackURL_col\n",
        "  df_okra['callback_code'] = callback_code_col\n",
        "  df_okra['callback_type'] = callback_type_col \n",
        "  df_okra['callback_url'] = callback_url_col\n",
        "  df_okra['code'] = code_col\n",
        "  df_okra['country'] = country_col\n",
        "  df_okra['current_project'] = current_project_col\n",
        "  df_okra['customerEmail'] = customerEmail_col\n",
        "  df_okra['customerId'] = customerId_col\n",
        "  df_okra['ended_at'] = ended_at_col\n",
        "  df_okra['env'] = env_col\n",
        "  df_okra['extras'] = extras_col\n",
        "  df_okra['identityType'] = identityType_col\n",
        "  df_okra['login_type'] = login_type_col\n",
        "  df_okra['message'] = message_col\n",
        "  df_okra['meta'] = meta_col\n",
        "  df_okra['method'] = method_col\n",
        "  df_okra['options'] = options_col\n",
        "  df_okra['owner'] = owner_col\n",
        "  df_okra['record'] = record_col \n",
        "  df_okra['recordId'] = recordId_col\n",
        "  df_okra['started_at'] = started_at_col\n",
        "  df_okra['status_webhook'] = status_webhook_col\n",
        "  df_okra['token'] = token_col\n",
        "  df_okra['type'] = type_col\n",
        " \n",
        "  return df_okra\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uiWq1Np8Rxx"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "df_okra_webhook = parse_row_okra_webhook_function(df_raw[df_raw['text'].str.contains('OKRA WEBHOOK')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBybNxmhkfV"
      },
      "source": [
        "df_okra_webhook.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDihv11g8lnJ"
      },
      "source": [
        "Handle DataFrame for LEADWAY SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s46Md54R8phm"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_leadway_function(df_leadway_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_leadway_success = []\n",
        "  list_column_none_leadway_success = [message_sms_payload_col, totalsent_col, cost_col, status_col, \n",
        "                                     account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                     responseCode_col, account_name_col, email_col, phone_Col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_leadway_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        date_col.append(row['ts'])\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('LEADWAY SUCCESS', str_text):\n",
        "                  type_of_request = re.search('LEADWAY SUCCESS', str_text)\n",
        "                  leadway_success_concat_text, index_first_succ, index_last_succ = parse_and_concatenate_Leadway_Success_Rows(df_)\n",
        "                  res_text_leadway = parse_Leadway_Success_Row(leadway_success_concat_text)\n",
        "                  for o in range(len(list_column_none_leadway_success)):\n",
        "                    list_column_none_leadway_success[o].append(None)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "                   \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)  \n",
        "\n",
        "  df_leadway_success['Type_Request'] = type_request_col\n",
        "  df_leadway_success['Phone_Number'] = phone_Col\n",
        "  df_leadway_success['Date'] = date_col\n",
        "  df_leadway_success['EndPoint'] = endpoint_Col\n",
        "  df_leadway_success['Log_Level'] = log_level_col\n",
        "  df_leadway_success['Email'] = email_col\n",
        "  df_leadway_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_leadway_success['Total Sent'] = totalsent_col\n",
        "  df_leadway_success['Cost'] = cost_col\n",
        "  df_leadway_success['Status'] = status_col\n",
        "  df_leadway_success['Account Number'] = account_number_col\n",
        "  df_leadway_success['Account Name'] = account_name_col\n",
        "  df_leadway_success['BVN'] = bvn_col\n",
        "  df_leadway_success['Request Successful'] = requestSuccessful_col\n",
        "  df_leadway_success['Response Message'] = responseMessage_col\n",
        "  df_leadway_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_leadway_success\n"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUIQgzNCcraR"
      },
      "source": [
        "Handle DataFrame for PROVIDUS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSI2WLjFYyn"
      },
      "source": [
        "def parse_PROVIDUS_PAYLOAD_DF(df_providus_payload):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS PAYLOAD\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    beneficiary_account_name_col = []\n",
        "    beneficiary_account_number_col = []\n",
        "    beneficiary_bank_col = []\n",
        "    currency_code_col = []\n",
        "    narration_col = []\n",
        "    source_account_name_col = []\n",
        "    transaction_amount_col = []\n",
        "    transaction_reference_col = []\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "\n",
        "    list_column_none_providus_payload = []\n",
        "    list_column_none_providus_payload = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseMessage_col,\n",
        "                                        responseCode_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    beneficiary_account_name_col, beneficiary_account_number_col, beneficiary_bank_col,\n",
        "                    currency_code_col, narration_col, source_account_name_col, transaction_amount_col,\n",
        "                    transaction_reference_col]\n",
        "\n",
        "    for index, row in df_providus_payload.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS PAYLOAD', str_text):               \n",
        "                    providus_payload = parse_wallet_sms_payload_success(str_text)\n",
        "                    type_of_request = re.search('PROVIDUS PAYLOAD', str_text)\n",
        "                       \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        beneficiary_account_name_col.append(providus_payload.get('beneficiaryAccountName'))\n",
        "                    except AttributeError:\n",
        "                        beneficiary_account_name_col.append(None)\n",
        "                    try:\n",
        "                        beneficiary_account_number_col.append(providus_payload.get('beneficiaryAccountNumber'))\n",
        "                    except AttributeError:\n",
        "                        beneficiary_account_number_col.append(None)\n",
        "                    try:\n",
        "                        beneficiary_bank_col.append(providus_payload.get('beneficiaryBank'))\n",
        "                    except AttributeError:\n",
        "                        beneficiary_bank_col.append(None)\n",
        "                    try:\n",
        "                        currency_code_col.append(providus_payload.get('currencyCode'))\n",
        "                    except AttributeError:\n",
        "                        currency_code_col.append(None)\n",
        "                    try:\n",
        "                        narration_col.append((providus_payload.get('narration')))\n",
        "                    except AttributeError:\n",
        "                        narration_col.append(None)\n",
        "                    try:\n",
        "                        source_account_name_col.append((providus_payload.get('sourceAccountName')))\n",
        "                    except AttributeError:\n",
        "                        source_account_name_col.append(None)\n",
        "                    try:\n",
        "                        transaction_amount_col.append((providus_payload.get('transactionAmount')))\n",
        "                    except AttributeError:\n",
        "                        transaction_amount_col.append(None)\n",
        "                    try:\n",
        "                        transaction_reference_col.append((providus_payload.get('transactionReference')))\n",
        "                    except AttributeError:\n",
        "                        transaction_reference_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_payload)):\n",
        "                        list_column_none_providus_payload[n].append(None) \n",
        "              \n",
        "            elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "                type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "\n",
        "            elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "\n",
        "            elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('VTPASS SUCCESS', str_text)\n",
        "\n",
        "   \n",
        "    df_providus_payload['Beneficiary_Account_Name'] = beneficiary_account_name_col\n",
        "    df_providus_payload['Beneficiary_Account_Number'] = beneficiary_account_number_col\n",
        "    df_providus_payload['Beneficiary_Bank'] = beneficiary_bank_col\n",
        "    df_providus_payload['Currency_Code'] = currency_code_col\n",
        "    df_providus_payload['Narration'] = narration_col\n",
        "    df_providus_payload['Source_Account_Name'] = source_account_name_col\n",
        "    df_providus_payload['Transaction_Amount'] = transaction_amount_col\n",
        "    df_providus_payload['Transaction_Reference'] = transaction_reference_col\n",
        "    df_providus_payload['Type_Request'] = type_request_col\n",
        "    df_providus_payload['Phone_Number'] = phone_col\n",
        "    df_providus_payload['Date'] = date_col\n",
        "    df_providus_payload['EndPoint'] = endpoint_col\n",
        "    df_providus_payload['Log_Level'] = log_level_col\n",
        "    df_providus_payload['Email'] = email_col\n",
        "    df_providus_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_payload['Total Sent'] = totalsent_col\n",
        "    df_providus_payload['Cost'] = cost_col\n",
        "    df_providus_payload['Status'] = status_col\n",
        "    df_providus_payload['Account Number'] = account_number_col\n",
        "    df_providus_payload['Account Name'] = account_name_col\n",
        "    df_providus_payload['BVN'] = bvn_col\n",
        "    df_providus_payload['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_payload['Response Message'] = responseMessage_col\n",
        "    df_providus_payload['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_payload\n"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZo3IJz2Fc4S"
      },
      "source": [
        "providus_payload_df = df_raw[df_raw['text'].str.contains('PROVIDUS PAYLOAD')]\n",
        "df_providus_payload = parse_PROVIDUS_PAYLOAD_DF(providus_payload_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1vjwaKWiDYF"
      },
      "source": [
        "df_providus_payload.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4vA4j3kdCeX"
      },
      "source": [
        "Handle DataFrame for PROVIDUS SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEg3vA6Fc7Ym"
      },
      "source": [
        "def parse_PROVIDUS_SUCCESS_DF(df_providus_success):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS SUCCESS\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    transaction_reference_col = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, transaction_reference_col]\n",
        "\n",
        "    for index, row in df_providus_success.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS SUCCESS', str_text):\n",
        "                    providus_success = parse_wallet_sms_payload_success(str_text)\n",
        "                    type_of_request = re.search('PROVIDUS SUCCESS', str_text)\n",
        "                       \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        responseCode_col.append(providus_success.get('responseCode'))\n",
        "                    except AttributeError:\n",
        "                        responseCode_col.append(None)\n",
        "                    try:\n",
        "                        responseMessage_col.append(providus_success.get('responseMessage'))\n",
        "                    except AttributeError:\n",
        "                        responseMessage_col.append(None)\n",
        "                    try:\n",
        "                        sessionId_col.append(providus_success.get('sessionId'))\n",
        "                    except AttributeError:\n",
        "                        sessionId_col.append(None)\n",
        "                    try:\n",
        "                        transaction_reference_col.append(providus_success.get('transactionReference'))\n",
        "                    except AttributeError:\n",
        "                        transaction_reference_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None) \n",
        "              \n",
        "            elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "                type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "\n",
        "            elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "\n",
        "            elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('VTPASS SUCCESS', str_text)\n",
        "\n",
        "    \n",
        "    df_providus_success['SessionId'] = sessionId_col\n",
        "    df_providus_success['Transaction_Reference'] = transaction_reference_col\n",
        "    df_providus_success['Type_Request'] = type_request_col\n",
        "    df_providus_success['Phone_Number'] = phone_col\n",
        "    df_providus_success['Date'] = date_col\n",
        "    df_providus_success['EndPoint'] = endpoint_col\n",
        "    df_providus_success['Log_Level'] = log_level_col\n",
        "    df_providus_success['Email'] = email_col\n",
        "    df_providus_success['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_success['Total Sent'] = totalsent_col\n",
        "    df_providus_success['Cost'] = cost_col\n",
        "    df_providus_success['Status'] = status_col\n",
        "    df_providus_success['Account Number'] = account_number_col\n",
        "    df_providus_success['Account Name'] = account_name_col\n",
        "    df_providus_success['BVN'] = bvn_col\n",
        "    df_providus_success['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_success['Response Message'] = responseMessage_col\n",
        "    df_providus_success['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_success\n"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWCAd27TdGCe"
      },
      "source": [
        "providus_success_df = df_raw[df_raw['text'].str.contains('PROVIDUS SUCCESS')]\n",
        "df_providus_success = parse_PROVIDUS_SUCCESS_DF(providus_success_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqmXhbXFiW29"
      },
      "source": [
        "df_providus_success.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5AkmLaNdbcS"
      },
      "source": [
        "Handle DataFrame for VTPASS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5XJfcXdb40"
      },
      "source": [
        "def parse_VTPASS_PAYLOAD_DF(df_vtpass_payload):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"VTPASS PAYLOAD\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    amount_col = []\n",
        "    request_id_col = []\n",
        "    serviceID_col = []\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    amount_col, request_id_col, serviceID_col]\n",
        "\n",
        "    for index, row in df_vtpass_payload.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('VTPASS PAYLOAD', str_text):\n",
        "                    providus_success = parse_wallet_sms_payload_success(str_text)\n",
        "                    type_of_request = re.search('VTPASS PAYLOAD', str_text)\n",
        "                       \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        amount_col.append(providus_success.get('amount'))\n",
        "                    except AttributeError:\n",
        "                        amount_col.append(None)\n",
        "                    try:\n",
        "                        phone_col.append(providus_success.get('phone'))\n",
        "                    except AttributeError:\n",
        "                        phone_col.append(None)\n",
        "                    try:\n",
        "                        request_id_col.append(providus_success.get('request_id'))\n",
        "                    except AttributeError:\n",
        "                        request_id_col.append(None)\n",
        "                    try:\n",
        "                        serviceID_col.append(providus_success.get('serviceID'))\n",
        "                    except AttributeError:\n",
        "                        serviceID_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None) \n",
        "              \n",
        "\n",
        "\n",
        "    df_vtpass_payload['Service_ID'] = serviceID_col\n",
        "    df_vtpass_payload['Amount'] = amount_col\n",
        "    df_vtpass_payload['Request_Id'] = request_id_col\n",
        "    df_vtpass_payload['Type_Request'] = type_request_col\n",
        "    df_vtpass_payload['Phone_Number'] = phone_col\n",
        "    df_vtpass_payload['Date'] = date_col\n",
        "    df_vtpass_payload['EndPoint'] = endpoint_col\n",
        "    df_vtpass_payload['Log_Level'] = log_level_col\n",
        "    df_vtpass_payload['Email'] = email_col\n",
        "    df_vtpass_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_vtpass_payload['Total Sent'] = totalsent_col\n",
        "    df_vtpass_payload['Cost'] = cost_col\n",
        "    df_vtpass_payload['Status'] = status_col\n",
        "    df_vtpass_payload['Account Number'] = account_number_col\n",
        "    df_vtpass_payload['Account Name'] = account_name_col\n",
        "    df_vtpass_payload['BVN'] = bvn_col\n",
        "    df_vtpass_payload['Request Successful'] = requestSuccessful_col\n",
        "    df_vtpass_payload['Response Message'] = responseMessage_col\n",
        "    df_vtpass_payload['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_vtpass_payload\n"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPHvlp86deaH"
      },
      "source": [
        "vtpass_payload = df_raw[df_raw['text'].str.contains('VTPASS PAYLOAD')]\n",
        "df_vtpass_payload = parse_VTPASS_PAYLOAD_DF(vtpass_payload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJPBtSi_im1d"
      },
      "source": [
        "df_vtpass_payload.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV2OBj_kxLGp"
      },
      "source": [
        "Handle DataFrame for LEADWAY ERROR Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWThX6Dhd-4G"
      },
      "source": [
        "def parse_LEADWAY_ERROR_DF(df_leadway_error):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"LEADWAY ERROR\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    error_code_col = []\n",
        "    error_number_col = []\n",
        "    port_col = []\n",
        "    syscall_col = []\n",
        "    address_col = []\n",
        "  \n",
        "    #list_column_none_level_log_error = []\n",
        "    # columns set to None\n",
        "    list_column_none = [message_sms_payload_col, totalsent_col, cost_col, status_col, email_col,\n",
        "                        endpoint_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                        responseCode_col, account_name_col, account_number_col, phone_col]\n",
        "    \n",
        "\n",
        "    # columns which will be populated\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    error_code_col, error_number_col, port_col, syscall_col, \n",
        "                    address_col]\n",
        "\n",
        "    for index, row in df_leadway_error.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        if re.search('error', str_text):\n",
        "            log_level = re.search('error', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "                \n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)\n",
        "\n",
        "            if re.search('LEADWAY ERROR', str_text):\n",
        "                type_of_request = re.search('LEADWAY ERROR', str_text)\n",
        "                leadway_error = parse_wallet_sms_payload_success(str_text)\n",
        "                try:\n",
        "                    error_code_col.append(leadway_error.get('code'))\n",
        "                except AttributeError:\n",
        "                    error_code_col.append(None)\n",
        "                try:\n",
        "                    error_number_col.append(leadway_error.get('errno'))\n",
        "                except AttributeError:\n",
        "                    error_number_col.append(None)\n",
        "                try:\n",
        "                    address_col.append(leadway_error.get('address'))\n",
        "                except AttributeError:\n",
        "                    address_col.append(None)\n",
        "                try:\n",
        "                    port_col.append(leadway_error.get('port'))\n",
        "                except AttributeError:\n",
        "                    port_col.append(None)\n",
        "                try:\n",
        "                    syscall_col.append(leadway_error.get('syscall'))\n",
        "                except AttributeError:\n",
        "                    syscall_col.append(None)      \n",
        "                try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "\n",
        "                for p in range(len(list_column_none)):\n",
        "                    list_column_none[p].append(None)\n",
        "\n",
        "    # set columns to their corresponding list values\n",
        "\n",
        "    df_leadway_error['Type_Request'] = type_request_col\n",
        "    df_leadway_error['Phone_Number'] =phone_col\n",
        "    df_leadway_error['Date'] = date_col\n",
        "    df_leadway_error['EndPoint'] = endpoint_col\n",
        "    df_leadway_error['Log_Level'] = log_level_col\n",
        "    df_leadway_error['Email'] = email_col\n",
        "    df_leadway_error['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_leadway_error['Total Sent'] = totalsent_col\n",
        "    df_leadway_error['Cost'] = cost_col\n",
        "    df_leadway_error['Status'] = status_col\n",
        "    df_leadway_error['Account Number'] = account_number_col\n",
        "    df_leadway_error['Account Name'] = account_name_col\n",
        "    df_leadway_error['BVN'] = bvn_col\n",
        "    df_leadway_error['Request Successful'] = requestSuccessful_col\n",
        "    df_leadway_error['Response Message'] = responseMessage_col\n",
        "    df_leadway_error['Response Code'] = responseCode_col\n",
        "    df_leadway_error['Error Code'] = error_code_col\n",
        "    df_leadway_error['Error Number'] = error_number_col\n",
        "    df_leadway_error['Error Port'] = port_col\n",
        "    df_leadway_error['Error Syscall'] = syscall_col\n",
        "    df_leadway_error['Error Address'] = address_col\n",
        "\n",
        "\n",
        "    return df_leadway_error\n"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viASPHzLeBOc"
      },
      "source": [
        "lead_error = df_raw[df_raw['text'].str.contains('LEADWAY ERROR')]\n",
        "df_lead_error = parse_LEADWAY_ERROR_DF(lead_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b136VEIsiyUv"
      },
      "source": [
        "df_lead_error.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg7u65AZxluN"
      },
      "source": [
        "Handle DataFrame for PROVIDUS TRANSFER ERROR Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsgEs6lNxYET"
      },
      "source": [
        "def parse_PROVIDUS_TRANSFER_ERROR_DF(df_providus_transfer_error):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS TRANSFER ERROR\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    error_code_col = []\n",
        "    error_number_col = []\n",
        "    port_col = []\n",
        "    syscall_col = []\n",
        "    address_col = []\n",
        "  \n",
        "    #list_column_none_level_log_error = []\n",
        "    # columns set to None\n",
        "    list_column_none = [message_sms_payload_col, totalsent_col, cost_col, status_col, email_col,\n",
        "                        endpoint_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                        responseCode_col, account_name_col, account_number_col, phone_col]\n",
        "    \n",
        "\n",
        "    # columns which will be populated\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    error_code_col, error_number_col, port_col, syscall_col, \n",
        "                    address_col]  \n",
        "\n",
        "    for index, row in df_providus_transfer_error.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        if re.search('error', str_text):\n",
        "            log_level = re.search('error', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)\n",
        "\n",
        "            if re.search('PROVIDUS TRANSFER ERROR', str_text):\n",
        "                type_of_request = re.search('PROVIDUS TRANSFER ERROR', str_text)\n",
        "                leadway_error = parse_providus_transfer_error_function(str_text)\n",
        "                try:\n",
        "                    error_code_col.append(leadway_error.get('code'))\n",
        "                except AttributeError:\n",
        "                    error_code_col.append(None)\n",
        "                try:\n",
        "                    error_number_col.append(leadway_error.get('errno'))\n",
        "                except AttributeError:\n",
        "                    error_number_col.append(None)\n",
        "                try:\n",
        "                    address_col.append(leadway_error.get('address'))\n",
        "                except AttributeError:\n",
        "                    address_col.append(None)\n",
        "                try:\n",
        "                    port_col.append(leadway_error.get('port'))\n",
        "                except AttributeError:\n",
        "                    port_col.append(None)\n",
        "                try:\n",
        "                    syscall_col.append(leadway_error.get('syscall'))\n",
        "                except AttributeError:\n",
        "                    syscall_col.append(None)      \n",
        "                try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "\n",
        "                for p in range(len(list_column_none)):\n",
        "                    list_column_none[p].append(None)\n",
        "\n",
        "    # set columns to their corresponding list values\n",
        "\n",
        "    df_providus_transfer_error['Type_Request'] = type_request_col\n",
        "    df_providus_transfer_error['Phone_Number'] =phone_col\n",
        "    df_providus_transfer_error['Date'] = date_col\n",
        "    df_providus_transfer_error['EndPoint'] = endpoint_col\n",
        "    df_providus_transfer_error['Log_Level'] = log_level_col\n",
        "    df_providus_transfer_error['Email'] = email_col\n",
        "    df_providus_transfer_error['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_transfer_error['Total Sent'] = totalsent_col\n",
        "    df_providus_transfer_error['Cost'] = cost_col\n",
        "    df_providus_transfer_error['Status'] = status_col\n",
        "    df_providus_transfer_error['Account Number'] = account_number_col\n",
        "    df_providus_transfer_error['Account Name'] = account_name_col\n",
        "    df_providus_transfer_error['BVN'] = bvn_col\n",
        "    df_providus_transfer_error['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_transfer_error['Response Message'] = responseMessage_col\n",
        "    df_providus_transfer_error['Response Code'] = responseCode_col\n",
        "    df_providus_transfer_error['Error Code'] = error_code_col\n",
        "    df_providus_transfer_error['Error Number'] = error_number_col\n",
        "    df_providus_transfer_error['Error Port'] = port_col\n",
        "    df_providus_transfer_error['Error Syscall'] = syscall_col\n",
        "    df_providus_transfer_error['Error Address'] = address_col\n",
        "\n",
        "\n",
        "    return df_providus_transfer_error\n"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLJ1mqMJxbCn"
      },
      "source": [
        "providus_transfer_error = df_raw[df_raw['text'].str.contains('PROVIDUS TRANSFER ERROR')]\n",
        "df_providus_transfer_error = parse_PROVIDUS_TRANSFER_ERROR_DF(providus_transfer_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXv7dx4wi7a0"
      },
      "source": [
        "df_providus_transfer_error.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjy32dtmyQJS"
      },
      "source": [
        "Handle DataFrame for PROVIDUS TRANSFER SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmn8hk_ZyGAz"
      },
      "source": [
        "\n",
        "def parse_PROVIDUS_TRANSFER_SUCCESS_DF(df_providus_transfer_success):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS TRANSFER SUCCESS\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    transaction_reference_col = []\n",
        "    message_error_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col]\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, transaction_reference_col]\n",
        "\n",
        "    for index, row in df_providus_transfer_success.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None) \n",
        "\n",
        "            if re.search('PROVIDUS TRANSFER SUCCESS', str_text):\n",
        "              \n",
        "              if 'Faithfully yours, nginx' in str_text:\n",
        "                  error_text = 'Sorry, the page you are looking for is currently unavailable, Please try again later.'\n",
        "                  message_error_col.append(error_text)\n",
        "                  responseCode_col.append(None)\n",
        "                  responseMessage_col.append(None)\n",
        "                  sessionId_col.append(None)\n",
        "                  transaction_reference_col.append(None)\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "              else:\n",
        "                  message_error_col.append(None)\n",
        "                  try:\n",
        "                    providus_transfer_success = parse_wallet_sms_payload_success(str_text)\n",
        "                  except json.decoder.JSONDecodeError:\n",
        "                    print(str_text)\n",
        "                    raise\n",
        "\n",
        "                  type_of_request = re.search('PROVIDUS TRANSFER SUCCESS', str_text)\n",
        "                      \n",
        "                  try:\n",
        "                      type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                      type_request_col.append(None)\n",
        "                  try:\n",
        "                      responseCode_col.append(providus_transfer_success.get('responseCode'))\n",
        "                  except AttributeError:\n",
        "                      responseCode_col.append(None)\n",
        "                  try:\n",
        "                      responseMessage_col.append(providus_transfer_success.get('responseMessage'))\n",
        "                  except AttributeError:\n",
        "                      responseMessage_col.append(None)\n",
        "                  try:\n",
        "                      sessionId_col.append(providus_transfer_success.get('sessionId'))\n",
        "                  except AttributeError:\n",
        "                      sessionId_col.append(None)\n",
        "                  try:\n",
        "                      transaction_reference_col.append(providus_transfer_success.get('transactionReference'))\n",
        "                  except AttributeError:\n",
        "                      transaction_reference_col.append(None)\n",
        "              for n in range(len(list_column_none_providus_success)):\n",
        "                  list_column_none_providus_success[n].append(None) \n",
        "    \n",
        "    df_providus_transfer_success['SessionId'] = sessionId_col\n",
        "    df_providus_transfer_success['Transaction_Reference'] = transaction_reference_col\n",
        "    df_providus_transfer_success['Type_Request'] = type_request_col\n",
        "    df_providus_transfer_success['Phone_Number'] = phone_col\n",
        "    df_providus_transfer_success['Date'] = date_col\n",
        "    df_providus_transfer_success['EndPoint'] = endpoint_col\n",
        "    df_providus_transfer_success['Log_Level'] = log_level_col\n",
        "    df_providus_transfer_success['Email'] = email_col\n",
        "    df_providus_transfer_success['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_transfer_success['Total Sent'] = totalsent_col\n",
        "    df_providus_transfer_success['Cost'] = cost_col\n",
        "    df_providus_transfer_success['Status'] = status_col\n",
        "    df_providus_transfer_success['Account Number'] = account_number_col\n",
        "    df_providus_transfer_success['Account Name'] = account_name_col\n",
        "    df_providus_transfer_success['BVN'] = bvn_col\n",
        "    df_providus_transfer_success['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_transfer_success['Response Message'] = responseMessage_col\n",
        "    df_providus_transfer_success['Response Code'] = responseCode_col\n",
        "    df_providus_transfer_success['Error Message Providus Transfer'] = message_error_col\n",
        " \n",
        "    return df_providus_transfer_success\n"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZPumx0gyI9w"
      },
      "source": [
        "providus_transfer_success = df_raw[df_raw['text'].str.contains('PROVIDUS TRANSFER SUCCESS')]\n",
        "df_providus_transfer_success = parse_PROVIDUS_TRANSFER_SUCCESS_DF(providus_transfer_success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqVwdhijKr0"
      },
      "source": [
        "df_providus_transfer_success.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYqmcPFSyvo4"
      },
      "source": [
        "def parse_PROVIDUS_SETTLEMENT_INFO_DF(df_providus_settlement_info):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS SETTLEMENT INFO\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    channelId_col = []\n",
        "    feeAmount_col = []\n",
        "    currency_col = []\n",
        "    initiation_tranRef_col = []\n",
        "    settled_amount_col = []\n",
        "    settlementId_col = []\n",
        "    source_account_name_col = []\n",
        "    source_account_number_col = []\n",
        "    source_bank_name_col = []\n",
        "    tran_date_time_col = []\n",
        "    tran_remarks_col = []\n",
        "    transaction_amount_col = []\n",
        "    vat_amount_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseCode_col, responseMessage_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, channelId_col, feeAmount_col, currency_col, initiation_tranRef_col,\n",
        "                    settled_amount_col, settlementId_col, source_account_name_col, source_account_number_col,\n",
        "                    source_bank_name_col, tran_date_time_col, tran_remarks_col, transaction_amount_col, vat_amount_col]\n",
        "\n",
        "    for index, row in df_providus_settlement_info.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            log_level = re.search('info', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS SETTLEMENT INFO', str_text):\n",
        "                    providus_settlement_info = parse_wallet_sms_payload_success(str_text)\n",
        "                    #print(providus_settlement_info)\n",
        "                    type_of_request = re.search('PROVIDUS SETTLEMENT INFO', str_text)\n",
        "          \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        account_number_col.append(providus_settlement_info.get('accountNumber'))\n",
        "                    except AttributeError:\n",
        "                        account_number_col.append(None)\n",
        "                    try:\n",
        "                        channelId_col.append(providus_settlement_info.get('channelId'))\n",
        "                    except AttributeError:\n",
        "                        channelId_col.append(None)\n",
        "                    try:\n",
        "                        sessionId_col.append(providus_settlement_info.get('sessionId'))\n",
        "                    except AttributeError:\n",
        "                        sessionId_col.append(None)\n",
        "                    try:\n",
        "                        currency_col.append(providus_settlement_info.get('currency'))\n",
        "                    except AttributeError:\n",
        "                        currency_col.append(None)\n",
        "                    try:\n",
        "                        feeAmount_col.append(providus_settlement_info.get('feeAmount'))\n",
        "                    except AttributeError:\n",
        "                        feeAmount_col.append(None)\n",
        "                    try:\n",
        "                        initiation_tranRef_col.append(providus_settlement_info.get('initiationTranRef'))\n",
        "                    except AttributeError:\n",
        "                        initiation_tranRef_col.append(None)\n",
        "                    try:\n",
        "                        settled_amount_col.append(providus_settlement_info.get('settledAmount'))\n",
        "                    except AttributeError:\n",
        "                        settled_amount_col.append(None)\n",
        "                    try:\n",
        "                        settlementId_col.append(providus_settlement_info.get('settlementId'))\n",
        "                    except AttributeError:\n",
        "                        settlementId_col.append(None)\n",
        "                    try:\n",
        "                        source_account_name_col.append(providus_settlement_info.get('sourceAccountName'))\n",
        "                    except AttributeError:\n",
        "                        source_account_name_col.append(None)\n",
        "                    try:\n",
        "                        source_account_number_col.append(providus_settlement_info.get('sourceAccountNumber'))\n",
        "                    except AttributeError:\n",
        "                        source_account_number_col.append(None)\n",
        "                    try:\n",
        "                        source_bank_name_col.append(providus_settlement_info.get('sourceBankName'))\n",
        "                    except AttributeError:\n",
        "                        source_bank_name_col.append(None)\n",
        "                    try:\n",
        "                        tran_date_time_col.append(providus_settlement_info.get('tranDateTime'))\n",
        "                    except AttributeError:\n",
        "                        tran_date_time_col.append(None)\n",
        "                    try:\n",
        "                        tran_remarks_col.append(providus_settlement_info.get('tranRemarks'))\n",
        "                    except AttributeError:\n",
        "                        tran_remarks_col.append(None)\n",
        "                    try:\n",
        "                        transaction_amount_col.append(providus_settlement_info.get('transactionAmount'))\n",
        "                    except AttributeError:\n",
        "                        transaction_amount_col.append(None)\n",
        "                    try:\n",
        "                        vat_amount_col.append(providus_settlement_info.get('vatAmount'))\n",
        "                    except AttributeError:\n",
        "                        vat_amount_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None) \n",
        "\n",
        "    \n",
        "    df_providus_settlement_info['Vat_Amount'] = vat_amount_col\n",
        "    df_providus_settlement_info['Transaction_Amount'] = transaction_amount_col\n",
        "    df_providus_settlement_info['Tran_Remarks'] = tran_remarks_col\n",
        "    df_providus_settlement_info['Tran_Date_Time'] = tran_date_time_col\n",
        "    df_providus_settlement_info['Source_Bank_Name'] = source_bank_name_col\n",
        "    df_providus_settlement_info['Source_Account_Number'] = source_account_number_col\n",
        "    df_providus_settlement_info['Source_Account_Name'] = source_account_name_col\n",
        "    df_providus_settlement_info['SettlementId'] = settlementId_col\n",
        "    df_providus_settlement_info['settled_Amount'] = settled_amount_col\n",
        "    df_providus_settlement_info['Initiation_TranRef'] = initiation_tranRef_col\n",
        "    df_providus_settlement_info['FeeAmount'] = feeAmount_col\n",
        "    df_providus_settlement_info['Currency'] = currency_col\n",
        "    df_providus_settlement_info['ChannelId'] = channelId_col\n",
        "    df_providus_settlement_info['SessionId'] = sessionId_col\n",
        "    df_providus_settlement_info['Type_Request'] = type_request_col\n",
        "    df_providus_settlement_info['Phone_Number'] = phone_col\n",
        "    df_providus_settlement_info['Date'] = date_col\n",
        "    df_providus_settlement_info['EndPoint'] = endpoint_col\n",
        "    df_providus_settlement_info['Log_Level'] = log_level_col\n",
        "    df_providus_settlement_info['Email'] = email_col\n",
        "    df_providus_settlement_info['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_settlement_info['Total Sent'] = totalsent_col\n",
        "    df_providus_settlement_info['Cost'] = cost_col\n",
        "    df_providus_settlement_info['Status'] = status_col\n",
        "    df_providus_settlement_info['Account Number'] = account_number_col\n",
        "    df_providus_settlement_info['Account Name'] = account_name_col\n",
        "    df_providus_settlement_info['BVN'] = bvn_col\n",
        "    df_providus_settlement_info['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_settlement_info['Response Message'] = responseMessage_col\n",
        "    df_providus_settlement_info['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_settlement_info\n"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxUKvX0yzEj"
      },
      "source": [
        "providus_settlement_info = df_raw[df_raw['text'].str.contains('PROVIDUS SETTLEMENT INFO')]\n",
        "df_providus_settlement_info = parse_PROVIDUS_SETTLEMENT_INFO_DF(providus_settlement_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxmYw6dGjSYv"
      },
      "source": [
        "df_providus_settlement_info.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-g1LdhAzRIy"
      },
      "source": [
        "def parse_PROVIDUS_VERIFY_SETTLEMENT_INFO_DF(df_providus_verify_settlement_info):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS VERIFY SETTLEMENT INFO\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    channelId_col = []\n",
        "    feeAmount_col = []\n",
        "    currency_col = []\n",
        "    initiation_tranRef_col = []\n",
        "    settled_amount_col = []\n",
        "    settlementId_col = []\n",
        "    source_account_name_col = []\n",
        "    source_account_number_col = []\n",
        "    source_bank_name_col = []\n",
        "    tran_date_time_col = []\n",
        "    tran_remarks_col = []\n",
        "    transaction_amount_col = []\n",
        "    vat_amount_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col, \n",
        "                                        account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, channelId_col, feeAmount_col, currency_col, initiation_tranRef_col,\n",
        "                    settled_amount_col, settlementId_col, source_account_name_col, source_account_number_col,\n",
        "                    source_bank_name_col, tran_date_time_col, tran_remarks_col, transaction_amount_col, vat_amount_col]\n",
        "\n",
        "    for index, row in df_providus_verify_settlement_info.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            log_level = re.search('info', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS VERIFY SETTLEMENT INFO', str_text):\n",
        "                    providus_verify_settlement_info = parse_wallet_sms_payload_success(str_text)\n",
        "                    print(providus_verify_settlement_info)\n",
        "                    type_of_request = re.search('PROVIDUS VERIFY SETTLEMENT INFO', str_text)\n",
        "\n",
        "\n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        account_number_col.append(providus_verify_settlement_info.get('accountNumber'))\n",
        "                    except AttributeError:\n",
        "                        account_number_col.append(None)\n",
        "                    try:\n",
        "                        channelId_col.append(providus_verify_settlement_info.get('channelId'))\n",
        "                    except AttributeError:\n",
        "                        channelId_col.append(None)\n",
        "                    try:\n",
        "                        sessionId_col.append(providus_verify_settlement_info.get('sessionId'))\n",
        "                    except AttributeError:\n",
        "                        sessionId_col.append(None)\n",
        "                    try:\n",
        "                        currency_col.append(providus_verify_settlement_info.get('currency'))\n",
        "                    except AttributeError:\n",
        "                        currency_col.append(None)\n",
        "                    try:\n",
        "                        feeAmount_col.append(providus_verify_settlement_info.get('feeAmount'))\n",
        "                    except AttributeError:\n",
        "                        feeAmount_col.append(None)\n",
        "                    try:\n",
        "                        initiation_tranRef_col.append(providus_verify_settlement_info.get('initiationTranRef'))\n",
        "                    except AttributeError:\n",
        "                        initiation_tranRef_col.append(None)\n",
        "                    try:\n",
        "                        settled_amount_col.append(providus_verify_settlement_info.get('settledAmount'))\n",
        "                    except AttributeError:\n",
        "                        settled_amount_col.append(None)\n",
        "                    try:\n",
        "                        settlementId_col.append(providus_verify_settlement_info.get('settlementId'))\n",
        "                    except AttributeError:\n",
        "                        settlementId_col.append(None)\n",
        "                    try:\n",
        "                        source_account_name_col.append(providus_verify_settlement_info.get('sourceAccountName'))\n",
        "                    except AttributeError:\n",
        "                        source_account_name_col.append(None)\n",
        "                    try:\n",
        "                        source_account_number_col.append(providus_verify_settlement_info.get('sourceAccountNumber'))\n",
        "                    except AttributeError:\n",
        "                        source_account_number_col.append(None)\n",
        "                    try:\n",
        "                        source_bank_name_col.append(providus_verify_settlement_info.get('sourceBankName'))\n",
        "                    except AttributeError:\n",
        "                        source_bank_name_col.append(None)\n",
        "                    try:\n",
        "                        tran_date_time_col.append(providus_verify_settlement_info.get('tranDateTime'))\n",
        "                    except AttributeError:\n",
        "                        tran_date_time_col.append(None)\n",
        "                    try:\n",
        "                        tran_remarks_col.append(providus_verify_settlement_info.get('tranRemarks'))\n",
        "                    except AttributeError:\n",
        "                        tran_remarks_col.append(None)\n",
        "                    try:\n",
        "                        transaction_amount_col.append(providus_verify_settlement_info.get('transactionAmount'))\n",
        "                    except AttributeError:\n",
        "                        transaction_amount_col.append(None)\n",
        "                    try:\n",
        "                        vat_amount_col.append(providus_verify_settlement_info.get('vatAmount'))\n",
        "                    except AttributeError:\n",
        "                        vat_amount_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None)  \n",
        "    \n",
        "    df_providus_verify_settlement_info['Vat_Amount'] = vat_amount_col\n",
        "    df_providus_verify_settlement_info['Transaction_Amount'] = transaction_amount_col\n",
        "    df_providus_verify_settlement_info['Tran_Remarks'] = tran_remarks_col\n",
        "    df_providus_verify_settlement_info['Tran_Date_Time'] = tran_date_time_col\n",
        "    df_providus_verify_settlement_info['Source_Bank_Name'] = source_bank_name_col\n",
        "    df_providus_verify_settlement_info['Source_Account_Number'] = source_account_number_col\n",
        "    df_providus_verify_settlement_info['Source_Account_Name'] = source_account_name_col\n",
        "    df_providus_verify_settlement_info['SettlementId'] = settlementId_col\n",
        "    df_providus_verify_settlement_info['settled_Amount'] = settled_amount_col\n",
        "    df_providus_verify_settlement_info['Initiation_TranRef'] = initiation_tranRef_col\n",
        "    df_providus_verify_settlement_info['FeeAmount'] = feeAmount_col\n",
        "    df_providus_verify_settlement_info['Currency'] = currency_col\n",
        "    df_providus_verify_settlement_info['ChannelId'] = channelId_col\n",
        "    df_providus_verify_settlement_info['SessionId'] = sessionId_col\n",
        "    df_providus_verify_settlement_info['Type_Request'] = type_request_col\n",
        "    df_providus_verify_settlement_info['Phone_Number'] = phone_col\n",
        "    df_providus_verify_settlement_info['Date'] = date_col\n",
        "    df_providus_verify_settlement_info['EndPoint'] = endpoint_col\n",
        "    df_providus_verify_settlement_info['Log_Level'] = log_level_col\n",
        "    df_providus_verify_settlement_info['Email'] = email_col\n",
        "    df_providus_verify_settlement_info['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_verify_settlement_info['Total Sent'] = totalsent_col\n",
        "    df_providus_verify_settlement_info['Cost'] = cost_col\n",
        "    df_providus_verify_settlement_info['Status'] = status_col\n",
        "    df_providus_verify_settlement_info['Account Number'] = account_number_col\n",
        "    df_providus_verify_settlement_info['Account Name'] = account_name_col\n",
        "    df_providus_verify_settlement_info['BVN'] = bvn_col\n",
        "    df_providus_verify_settlement_info['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_verify_settlement_info['Response Message'] = responseMessage_col\n",
        "    df_providus_verify_settlement_info['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_verify_settlement_info\n"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRUSLfr4zVJR"
      },
      "source": [
        "providus_verify_settlement_info = df_raw[df_raw['text'].str.contains('PROVIDUS VERIFY SETTLEMENT INFO')]\n",
        "df_providus_verify_settlement_info = parse_PROVIDUS_VERIFY_SETTLEMENT_INFO_DF(providus_verify_settlement_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcWUJ829jaOY"
      },
      "source": [
        "df_providus_verify_settlement_info.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0EsxMifCEU"
      },
      "source": [
        "### on fait la concatenation par ligne car dans chaque df on les "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRCfDmlrlBTd"
      },
      "source": [
        "pdList = [df_api_request, df_client_mobile_login, df_loan_payload, df_error, df_lead_error, df_okra_webhook, df_providus_payload, df_providus_settlement_info,\n",
        "          df_providus_success, df_providus_transfer_error, df_providus_transfer_success, df_providus_verify_settlement_info,\n",
        "          df_sms_payload, df_sms_success, df_vtpass_payload, df_wallet_success]  # List of our dataframes\n",
        "df_final = pd.concat(pdList, axis=0)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8grT-h74Wth"
      },
      "source": [
        "df_final.sort_values('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YloqA19OgRLS"
      },
      "source": [
        "df_phone_number = df_final['Phone_Number'].unique()"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgGut93ChEuB"
      },
      "source": [
        "df_final[df_final['Phone_Number']=='07051839788']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOvXUqm5zfvk"
      },
      "source": [
        "df_final.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cy3Ph7Oxni-"
      },
      "source": [
        "# à modifier avec regex\n",
        "#list_index_loan_amount = list(df_final[df_final['text'].str.contains('loan')].index)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA3mWTUB1JMB"
      },
      "source": [
        "df_with_loan_amount = df_final[df_final['text'].str.contains('loan_amount')].copy()"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GW60fb9CSF2"
      },
      "source": [
        "df_final.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDLgeXIC16i-"
      },
      "source": [
        "df_with_loan_amount.to_csv('/content/drive/MyDrive/datasets/final/loan_amount/loan_amount.csv', index=None)"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7rt55LsyR9g"
      },
      "source": [
        "index_loan_amount = list(df_with_loan_amount.index)"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuFSWwNFuHM"
      },
      "source": [
        "index_loan_amount[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHsuAFnLETs5"
      },
      "source": [
        "df_final.iloc[67110:67130]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca0qFWGCyvsY",
        "outputId": "8ac518a5-759e-4382-e7cc-9675b9f6d9d7"
      },
      "source": [
        "index_loan_amount[500]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67129"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzz2PRT3Sr5"
      },
      "source": [
        "df_final.iloc[66629]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvvJSxrLB5vz"
      },
      "source": [
        ""
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dYiyqpV4zio"
      },
      "source": [
        "df_final.iloc[66624:66630]\n",
        "#1- filtrer le dataset sur la colonne Phone_Number avec le filtre != null ou != o\n",
        "#2- ensuite on récupère les index\n",
        "#3- on prend le plus grand index \n",
        "#4- on récupère le Phone_Number du grand index \n",
        "#5- on met ce phone number dans la colonne Phone_Number de la ligne contenant  \n",
        "# \"LOAN PAYLOAD\" ou \"LOAN ERROR\" sans numéro de téléphone\n",
        "# ensuite on repart dans la liste index_loan_amount, on récupère le prochain\n",
        "# indexe pour exécuter les précédantes opérations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhjAdhmDCt7X"
      },
      "source": [
        "df_final[pd.isna(df_final['Phone_Number'])].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztwIFc6NhBvo"
      },
      "source": [
        "phone_list = []\n",
        "for i in index_loan_amount:\n",
        "  df_essai = df_final.iloc[i-4:i+1]\n",
        "  #phone_list.append(df_essai[df_essai['Phone_Number'] != None ].index)\n",
        "  #df_final['Phone_Number'][i] = df_essai.iloc[max(phone_list)]['Phone_Number']\n",
        "  #df['b'].notnull()\n",
        "  phone_list.append(df_essai[df_essai['Phone_Number'].notnull()].index)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT5iY0VJg5Vl"
      },
      "source": [
        "phone_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho22lWd3-sfQ"
      },
      "source": [
        "df_final.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HH3TASA5_L"
      },
      "source": [
        "#df_final[df_final['Loan Amount'] == 50000.0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klzQhg_I2iIv"
      },
      "source": [
        "import matplotlib.pyplot\n",
        "#df_final.plot.bar()"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcX5DFI46pyw"
      },
      "source": [
        "pct_null = df_final.isnull().sum() / len(df_final)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COx28G_58AEl"
      },
      "source": [
        "pct_null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1faUt6C6yEX"
      },
      "source": [
        "missing_features = pct_null[pct_null > 0.998].index\n",
        "df_final.drop(missing_features, axis=1, inplace=True)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBTmOzZF8Mh6"
      },
      "source": [
        "df_final.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmWjA-7c2oKe"
      },
      "source": [
        "#master_df.info(verbose=True, null_counts=True)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILWbhc724al"
      },
      "source": [
        "# prendre toute une ligne du dataframe qui contient des éléments de types object (String),\n",
        "# partout où on trouve des NaN, les remplacer par string vide '', ensuite où on trouve\n",
        "# une valeur, on fait la concatenation ou le Join.\n",
        "#df_final.apply(lambda row: ('').join(row['type'].fillna('').values), axis=1)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6YhR06BVJgL"
      },
      "source": [
        "df_final.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUt_0DLGoICH"
      },
      "source": [
        "obj = df_final.columns.to_series().groupby(df_final.dtypes).groups\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yckg2mjsmcXf",
        "outputId": "9d668c4e-b233-4636-8faf-e7b87b311293"
      },
      "source": [
        "from pandas.api.types import is_object_dtype\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "obj_cols = []\n",
        "numeric_cols = []\n",
        "for y in tqdm(df_final.columns):\n",
        "    if (is_object_dtype(df_final[y])):\n",
        "        obj_cols.append(y)\n",
        "    elif (is_numeric_dtype(df_final[y])):\n",
        "        numeric_cols.append(y)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 42300.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti4_YphAv4jK"
      },
      "source": [
        "df_final[df_final['Loan Amount'] == 5000.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6rhrpxL1Ruj"
      },
      "source": [
        "#df_final.apply(lambda row: ('').join(row['ts'].fillna('').values), axis=1)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNuyqcMM1bCU"
      },
      "source": [
        "#df_final['ts'].fillna(0).sum(axis=1)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpdCBMBQ6uNR"
      },
      "source": [
        "#df_final.apply(lambda row: ('').join(row['subtype'].fillna('').values), axis=1)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ywNqkPT7jbB"
      },
      "source": [
        "#df_final['Loan Amount'].fillna(0).sum(axis=1)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipUndaAClsTk"
      },
      "source": [
        "### Export DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-OennYvluQn"
      },
      "source": [
        "df_final.to_csv('/content/drive/MyDrive/datasets/final/nirra_log_bot_final.csv', index=None)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smVnWy7aOAxM",
        "outputId": "15fa63f9-6c78-448d-e986-effc933f7913"
      },
      "source": [
        "len(df_final['Phone_Number'].unique())"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5764"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSp3sqTVOSC9"
      },
      "source": [
        "df_final['Phone_Number'].unique() # récupérer la liste des différents numéros de téléphone\n",
        "all_users_phone_number = [element for element in df_final['Phone_Number'].unique() if element != None] # all userId\n",
        "#all_users_phone_number # liste de tous les différents numéros de téléphone."
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCdVwWuhOype",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac61829-37dd-4671-eb9c-dc6c3b95d392"
      },
      "source": [
        "all_df_phone_number = [] # to get list of dataframes\n",
        "for phoneNumber in tqdm(all_users_phone_number):\n",
        "  df_phone_number = df_final[df_final['Phone_Number'] == phoneNumber] # récupérer les dataframes avec seulement les numéros de téléphone\n",
        "  all_df_phone_number.append(df_phone_number)  # mettre chaque dataframe dans la"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5763/5763 [00:34<00:00, 165.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFhigDtPIgb"
      },
      "source": [
        "### sauvegarder les dataframes sur disque"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OJHQyIyQv66"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# tqdm permet\n",
        "for phoneNumber in tqdm(all_users_phone_number):\n",
        "  df_phone_number = df_final[df_final['Phone_Number'] == phoneNumber]\n",
        "  df_phone_number.to_csv(f'/content/drive/MyDrive/datasets/final/files/{phoneNumber}.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZh2y-x8PdIv"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/final/files/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jg94X1ZPzDf"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# use glob to get all the csv files \n",
        "# in the folder\n",
        "path = os.getcwd()\n",
        "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "all_df_list = [] # list of all dataframe\n",
        "# loop over the list of csv files\n",
        "number_transactions = []\n",
        "all_df_list_with_number_transaction = []\n",
        "list_df_with_diff_ts = []\n",
        "\n",
        "for f in tqdm(csv_files):    \n",
        "    # read the csv file\n",
        "    df = pd.read_csv(f)  \n",
        "    all_df_list.append(df)   \n",
        "\n",
        "for element in all_df_list:\n",
        "  element['Number_Transactions'] = element.shape[0] # faire la somme des lignes et mettre dans la colonne Number_Transactions\n",
        "  all_df_list_with_number_transaction.append(element)# mettre tous les éléments dans une nouvelle liste\n",
        "\n",
        "#Faire la différence entre les timestamp\n",
        "for ele_with_num_trans in all_df_list_with_number_transaction:\n",
        "  df_ts = pd.DataFrame(ele_with_num_trans['ts'])\n",
        "  ele_with_num_trans['ts_diff'] = df_ts.diff(axis=0)\n",
        "  list_df_with_diff_ts.append(ele_with_num_trans)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny-UhHEXTN7y"
      },
      "source": [
        "df_amountmore_than_10000 = df_final[df_final['Loan Amount'] > 10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygJUx_NSUZUT"
      },
      "source": [
        "df_amountmore_than_10000[['Phone_Number', 'Loan Amount']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2pcP8awSqNi"
      },
      "source": [
        "### Entrainer premièrement le modèle global"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UElUbtcVSxG3"
      },
      "source": [
        "# avoir la distribution des nombres de transactions de tous les utilisateurs et faire un histogramme\n",
        "list_number_transactions = []\n",
        "for ele in tqdm(list_df_with_diff_ts):\n",
        "  if len(ele) > 0:\n",
        "    list_number_transactions.append(ele['Number_Transactions'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NWWTqVcZ8OU"
      },
      "source": [
        "max(list_number_transactions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGmvbJ6Mh5iL"
      },
      "source": [
        "### Tracer l'histogramme logarithme en fonction de la liste contenant le nombre de transactions pour chaque utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN-ZVXZUXI4W"
      },
      "source": [
        "df_liste_transactions = pd.DataFrame(list_number_transactions, columns=['Number_Transactions_per_user'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvGEP4sHX-XP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# tracer un histogramme logarithmique\n",
        "y = pd.Series(list_number_transactions)\n",
        "hist, bins, _ = plt.hist(y, bins=8)\n",
        "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
        "plt.yscale('log', nonposy='clip')\n",
        "plt.hist(y, bins=logbins)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBFF5uVtouAo"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure(\n",
        "    data=[go.Bar(y=list_number_transactions)],\n",
        "    layout_title_text=\"A Figure Displayed with fig.show()\"\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrbjuOgVggif"
      },
      "source": [
        "### récupérer la liste des utilisateurs qui pourraient etre problématiques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwmkUFm-ggIK"
      },
      "source": [
        "### selon l'histogramme on a environs une vingtaine d'utilisateurs qui ont effectué entre 400 et plus de 800 transactions\n",
        "# on peut essayer de récupérer la liste de ces utilisateurs et étudier leurs comportements\n",
        "# sauvegarder sur le disque la liste des utilisateurs ayant plus de 400 transactions\n",
        "liste_utilisateurs_problematiques = []\n",
        "for one_df_diff_ts in tqdm(list_df_with_diff_ts):\n",
        "  if one_df_diff_ts.shape[0] >= 300:\n",
        "    liste_utilisateurs_problematiques.append(one_df_diff_ts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7RQoVbkxDN9"
      },
      "source": [
        "### Consolidation de tous les dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iclIWHuQEE1H"
      },
      "source": [
        "list_df_with_diff_ts[0]['Loan Amount']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyFEisahEikP"
      },
      "source": [
        "for ele in list_df_with_diff_ts:\n",
        "  if len(ele['Loan Amount']) != 0:\n",
        "    if ele['Loan Amount'][0] == 20000.0:\n",
        "      print(ele.head(5))\n",
        "      raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB4PZWuFEXst"
      },
      "source": [
        "[element for element in list_df_with_diff_ts if element['Loan Amount'][0] == 20000.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sozm0a2fqyHV"
      },
      "source": [
        "master_df = pd.concat(list_df_with_diff_ts, axis=0).reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-rlQJZ9xH0u"
      },
      "source": [
        "master_df[master_df['Loan Amount'] > 20000.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0_WKNCardQI"
      },
      "source": [
        "df_endpoint = pd.DataFrame(master_df['EndPoint'].str.contains('decline'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUBja6nLr_VZ"
      },
      "source": [
        "df_endpoint.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moMLUwSmivPA"
      },
      "source": [
        "len(liste_utilisateurs_problematiques)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTo0M5oBtT92"
      },
      "source": [
        "liste_utilisateurs_problematiques[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDG_gn8wjLMd"
      },
      "source": [
        "### sauvegarder ces utilisateurs dans le disque en local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAAWQ3KJjKT3"
      },
      "source": [
        "for elem in tqdm(liste_utilisateurs_problematiques):\n",
        "  phone = elem['Phone_Number'][0]\n",
        "  elem.to_csv(f'/content/drive/MyDrive/datasets/final/users_problematics/{phone}.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8w2z-JgRnGz"
      },
      "source": [
        "### Création des times series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OEzZ5khRodR"
      },
      "source": [
        "all_df_res = []\n",
        "new_endpoint_col = []\n",
        "for new_data_frame in tqdm(liste_utilisateurs_problematiques):\n",
        "  new_data_frame['EndPoint'].fillna('', inplace=True) # modifie la colonne\n",
        "\n",
        "  new_data_frame['Appplication_get'] = new_data_frame['EndPoint'].str.contains('applications/get').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Payment'] = new_data_frame['EndPoint'].str.contains('payment').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Declined'] = new_data_frame['EndPoint'].str.contains('decline').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Preapprouved'] = new_data_frame['EndPoint'].str.contains('preapproved').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Accepted'] = new_data_frame['EndPoint'].str.contains('accept').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Lifestyle'] = new_data_frame['EndPoint'].str.contains('lifestyle').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Wallet_balance'] = new_data_frame['EndPoint'].str.contains('wallet/balance').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Adverts'] = new_data_frame['EndPoint'].str.contains('adverts').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Bank'] = new_data_frame['EndPoint'].str.contains('bank').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Updated'] = new_data_frame['EndPoint'].str.contains('update').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Products'] = new_data_frame['EndPoint'].str.contains('products').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Locations'] = new_data_frame['EndPoint'].str.contains('locations').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Client_Get'] = new_data_frame['EndPoint'].str.contains('client/get').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Client_V2_Create'] = new_data_frame['EndPoint'].str.contains('client/v2/create').astype(np.float).astype('Int32')\n",
        "  \n",
        "  \n",
        "  new_data_frame['Dummy'] = 0\n",
        "  new_data_frame['Payment_cumulative'] = new_data_frame.groupby(['Dummy'])['Payment'].cumsum()\n",
        "  new_data_frame['Decline_cumulative'] = new_data_frame.groupby(['Dummy'])['Declined'].cumsum()\n",
        "  new_data_frame['Application_get_cumulative'] = new_data_frame.groupby(['Dummy'])['Appplication_get'].cumsum()\n",
        "  new_data_frame['Preapprouved_cumulative'] = new_data_frame.groupby(['Dummy'])['Preapprouved'].cumsum()\n",
        "  new_data_frame['Accepted_cumulative'] = new_data_frame.groupby(['Dummy'])['Accepted'].cumsum()\n",
        "  new_data_frame['Lifestyle_cumulative'] = new_data_frame.groupby(['Dummy'])['Lifestyle'].cumsum()\n",
        "  new_data_frame['Wallet_balance_cumulative'] = new_data_frame.groupby(['Dummy'])['Wallet_balance'].cumsum()\n",
        "  new_data_frame['Adverts_cumulative'] = new_data_frame.groupby(['Dummy'])['Adverts'].cumsum()\n",
        "  new_data_frame['Bank_cumulative'] = new_data_frame.groupby(['Dummy'])['Bank'].cumsum()\n",
        "  new_data_frame['Update_cumulative'] = new_data_frame.groupby(['Dummy'])['Updated'].cumsum()\n",
        "  new_data_frame['Products_cumulative'] = new_data_frame.groupby(['Dummy'])['Products'].cumsum()\n",
        "  new_data_frame['Locations_cumulative'] = new_data_frame.groupby(['Dummy'])['Locations'].cumsum()\n",
        "  new_data_frame['Client_Get_cumulative'] = new_data_frame.groupby(['Dummy'])['Client_Get'].cumsum()\n",
        "  new_data_frame['Client_Get_cumulative'] = new_data_frame.groupby(['Dummy'])['Client_V2_Create'].cumsum()\n",
        "  all_df_res.append(new_data_frame)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJeRHJ4z7CO"
      },
      "source": [
        "all_df_res[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A9RzsG_vI1G"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/final/files_with_new_characteristics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQTeMCGpvkfG"
      },
      "source": [
        "### Sauvegarder les nouveaux datasets dans un nouveau dossier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZqSV2HDvdew"
      },
      "source": [
        "from tqdm import tqdm\n",
        " #tqdm permet\n",
        "for elem in tqdm(all_df_res):\n",
        "  phone = elem['Phone_Number'][0]\n",
        "  elem.to_csv(f'/content/drive/MyDrive/datasets/final/files_with_new_characteristics/{phone}.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe3iLxZEwlTb"
      },
      "source": [
        "all_df_res[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UBFCkJyv9DV"
      },
      "source": [
        "### Parcourir le dossier contenant les dataframes et convertir dans une nouvelle colonne \"TS_to_DateTime\" la colonne \"ts\" en Datetime et mettre dans la liste all_df_list_new"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HxU7fWRvxPd"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def list_load_all_df(path='/content/drive/My Drive/datasets/final/files_with_new_characteristics'):\n",
        "  #path = os.getcwd()\n",
        "  csv_files_new = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "  all_df_list_new = []\n",
        "  # loop over the list of csv files\n",
        "  for f in tqdm(csv_files_new):    \n",
        "      # read the csv file\n",
        "      df_new = pd.read_csv(f)\n",
        "      df_new['TS_to_DateTime'] = df_new['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "      all_df_list_new.append(df_new)\n",
        "  return all_df_list_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oavoJKfUwkeW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Récupérer le nombre d'opérations consécutives (opérations qui se déroulent dans l'interval de 5 min)\n",
        "def df_nombre_operations_consecutives(each_df_2):\n",
        "  '''\n",
        "    cette fonction prend comme paramètre un dataframe et retourne un dataframe avec les nouvelles colonnes\n",
        "  '''\n",
        "  each_df = each_df_2.copy() \n",
        "  size_df = each_df.shape[0]\n",
        "  ts_delta=0\n",
        "  count = 0\n",
        "  last_ts = 0\n",
        "  number_consecutive_operations = []\n",
        "  time_consecutive_operations = []\n",
        "  phone_number_each_df = []\n",
        "  # on récupère la 1ere date ou la 1ère lige de la colonne \"TS_to_DateTime\"\n",
        "  start_ts = each_df.iloc[0]['TS_to_DateTime']\n",
        "  result = pd.DataFrame(columns=['Phone_Number', 'Number_consecutive_operations', 'Time_consecutive_operations',\n",
        "                                                                'Number_operations_per_minute', 'Max_number_operation_per_minute',\n",
        "                                                                'Min_number_operation_per_minute', 'Mean_operation_per_minute',\n",
        "                                                                'Median_operation_per_minute'])\n",
        "  \n",
        "  for i in range(size_df-1):\n",
        "    ts_delta = each_df.iloc[i+1]['TS_to_DateTime'] - each_df.iloc[i]['TS_to_DateTime']\n",
        "    # si l'interval de temps pendant lequel les opérations se sont effectuées est <= à 5 min \n",
        "    if (ts_delta.total_seconds()/60) <= 5:\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      # on compte les opérations effectuées en 5 minutes\n",
        "      count +=1\n",
        "    else :\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      ts_delta = (last_ts - start_ts).total_seconds()/60\n",
        "      # liste des temps en minutes pendant lequel les opérations se sont déroulées\n",
        "      time_consecutive_operations.append(ts_delta)\n",
        "      # nombres d'opérations consécutives (c-à-d opérations qui se déroulées dans un interval de 5 min)\n",
        "      number_consecutive_operations.append(count)\n",
        "      phone_number_each_df.append(each_df.iloc[i+1]['Phone_Number'])\n",
        "      count = 0\n",
        "      start_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "\n",
        "      df_phone_number_each_df = pd.DataFrame(phone_number_each_df)  \n",
        "      df_number_consecutive_operations = pd.DataFrame(number_consecutive_operations)\n",
        "      df_time_consecutive_operations = pd.DataFrame(time_consecutive_operations)\n",
        "      pdList_consecutive_operations = [df_phone_number_each_df, df_number_consecutive_operations, df_time_consecutive_operations] # List of our dataframes\n",
        "      df_final_consecutive_operations = pd.concat(pdList_consecutive_operations, axis=1)\n",
        "      df_final_consecutive_operations.columns=['Phone_Number', 'Number_consecutive_operations', 'Time_consecutive_operations']\n",
        "      df_final_consecutive_operations['Number_operations_per_minute'] = df_final_consecutive_operations['Number_consecutive_operations']/df_final_consecutive_operations['Time_consecutive_operations']\n",
        "      df_final_consecutive_operations['Max_number_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].max()\n",
        "      df_final_consecutive_operations['Min_number_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].min()\n",
        "      df_final_consecutive_operations['Mean_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].mean()\n",
        "      df_final_consecutive_operations['Median_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].median()\n",
        "      result = df_final_consecutive_operations\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8PzeNFmwqUe"
      },
      "source": [
        "def df_one_operation(each_df):\n",
        "\n",
        "  copy_df = each_df.copy()\n",
        "  copy_df['Number_consecutive_operations'] = 0\n",
        "  copy_df['Time_consecutive_operations'] = 0\n",
        "  copy_df['Number_operations_per_minute'] = 0\n",
        "  copy_df['Max_number_operation_per_minute'] = 0\n",
        "  copy_df['Min_number_operation_per_minute'] = 0\n",
        "  copy_df['Mean_operation_per_minute'] = 0\n",
        "  copy_df['Median_operation_per_minute'] = 0\n",
        "  return copy_df['Phone_Number'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5vGSKHgwt51"
      },
      "source": [
        "res_all_df = list_load_all_df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KobGvvDxZsx"
      },
      "source": [
        "_list_all_df = []\n",
        "for element in tqdm(res_all_df):\n",
        "  # pour debugger il faut utiliser try -  except et utiliser pass\n",
        "  # ainsi, en cas de problème on pourrait savoir dans quel DataFrame il se trouve\n",
        "  if element.shape[0] > 1:\n",
        "    _list_all_df.append(df_nombre_operations_consecutives(element))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKxolzfxg7S"
      },
      "source": [
        "_list_all_df[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er6gBTX7x5TK"
      },
      "source": [
        "def give_dataframe(phone_number, path_df='/content/drive/My Drive/datasets/final/files_with_new_characteristics'):\n",
        "  df = pd.read_csv(f'{path_df}/{phone_number}.csv')\n",
        "  df['TS_to_DateTime'] = df['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3OlWvAOzJBt"
      },
      "source": [
        "def give_dataframe_complete_path(path_complet_df='/content/drive/MyDrive/datasets/final/files_with_new_characteristics/2349152152920.csv'):\n",
        "  df_2 = pd.read_csv(path_complet_df)\n",
        "  df_2['TS_to_DateTime'] = df_2['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "  return df_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8OMi3Qhzqv6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3BvIFfzrIJ"
      },
      "source": [
        "import statistics\n",
        "\n",
        "def calculate_metric(list_df, str_key_word='/client/'):\n",
        "  '''cette fonction permet de calculer la moyenne, la mediane, la valeur min, max et le seuil\n",
        "    en se référant sur la liste contenant la somme des opérations effectuées dans chaque dataframe. ''' \n",
        "  sum_operation = []\n",
        "  list_phone_number_user = []\n",
        "  for one_df in list_df:\n",
        "    one_df['EndPoint'].fillna('', inplace=True)\n",
        "    # tenir compte du cast d'un boolean en float avec la propriété astype(float)\n",
        "    sum_operation.append(one_df['EndPoint'].str.contains(str_key_word).astype(float).sum())\n",
        "    list_phone_number_user.append(one_df['Phone_Number'][0])\n",
        "\n",
        "  mini = min(sum_operation)\n",
        "  maxi = max(sum_operation)\n",
        "  mediane = statistics.median(sum_operation)\n",
        "  moyenne = statistics.mean(sum_operation)\n",
        "  ecart_type = statistics.pstdev(sum_operation)\n",
        "  threshold = mediane + (3*ecart_type) # les valeurs au délà de 3* écart type sont considérées comme outliers, on considère cette valeur comme notre seuil\n",
        "  print(f' print 3 écart-type: {3*ecart_type}')\n",
        "  print(f'somme operation est: {sum_operation}')\n",
        "  print(f'la liste des numéros de téléphone des utilisateur est: {list_phone_number_user}')\n",
        "  print(f'le max est: {maxi}\\nle min est: {mini}\\nla mediane est: {mediane}\\nla moyenne est {moyenne}\\necart type est {ecart_type}\\nle seuil est {threshold}')\n",
        "  return list_phone_number_user, sum_operation, mini, maxi, mediane, moyenne, ecart_type, threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdh4eBWpzxw9"
      },
      "source": [
        "list_phone_number_user_client, sum_operation_client, min_client, max_client, mediane_client, moyenne_client, et_client, threshold_client = calculate_metric(res_all_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU8D4zdvzxVx"
      },
      "source": [
        "list_phone_number_user_client, sum_operation_client, min_client, max_client, mediane_client, moyenne_client, et_client, threshold_client = calculate_metric(res_all_df)\n",
        "list_phone_number_user_decline, sum_operation_decline, mini_decline, maxi_decline, mediane_decline, moyenne_decline, et_decline, threshold_decline = calculate_metric(res_all_df, str_key_word='/decline/')\n",
        "list_phone_number_user_lifestyle, sum_operation_lifestyle, min_lifestyle, max_lifestyle, mediane_lifestyle, moyenne_lifestyle, et_lifestyle, threshold_lifestyle = calculate_metric(res_all_df, str_key_word='lifestyle')\n",
        "list_phone_number_user_payment, sum_operation_payment, min_payment, max_payment, mediane_payment, moyenne_payment, et_payment, threshold_payment = calculate_metric(res_all_df, str_key_word='payment')\n",
        "list_phone_number_user_app_get, sum_operation_app_get, min_app_get, max_app_get, mediane_app_get, moyenne_app_get, et_app_get, threshold_app_get = calculate_metric(res_all_df, str_key_word='applications/get')\n",
        "list_phone_number_user_preapproved, sum_operation_preapproved, min_preapproved, max_preapproved, mediane_preapproved, moyenne_preapproved, et_preapproved, threshold_preapproved = calculate_metric(res_all_df, str_key_word='preapproved')\n",
        "list_phone_number_user_accept, sum_operation_accept, min_accept, max_accept, mediane_accept, moyenne_accept, et_accept, threshold_accept = calculate_metric(res_all_df, str_key_word='accept')\n",
        "list_phone_number_user_wallet_balance, sum_operation_wallet_balance, min_wallet_balance, max_wallet_balance, mediane_wallet_balance, moyenne_wallet_balance, et_wallet_balance, threshold_wallet_balance = calculate_metric(res_all_df, str_key_word='wallet/balance')\n",
        "list_phone_number_user_adverts, sum_operation_adverts, min_adverts, max_adverts, mediane_adverts, moyenne_adverts, et_adverts, threshold_adverts = calculate_metric(res_all_df, str_key_word='adverts')\n",
        "list_phone_number_user_bank, sum_operation_bank, min_bank, max_bank, mediane_bank, moyenne_bank, et_bank, threshold_bank = calculate_metric(res_all_df, str_key_word='bank')\n",
        "list_phone_number_user_update, sum_operation_update, min_update, max_update, mediane_update, moyenne_update, et_update, threshold_update = calculate_metric(res_all_df, str_key_word='update')\n",
        "list_phone_number_user_product, sum_operation_product, min_product, max_product, mediane_product, moyenne_product, et_product, threshold_product = calculate_metric(res_all_df, str_key_word='products')\n",
        "list_phone_number_user_location, sum_operation_Location, min_Location, max_Location, mediane_Location, moyenne_Location, et_Location, threshold_Location = calculate_metric(res_all_df, str_key_word='locations')\n",
        "list_phone_number_user_client_get, sum_operation_client_get, min_client_get, max_client_get, mediane_client_get, moyenne_client_get, et_client_get, threshold_client_get = calculate_metric(res_all_df, str_key_word='client/get')\n",
        "list_phone_number_user_client_v2_create, sum_operation_client_v2_create, min_client_v2_create, max_client_v2_create, mediane_client_v2_create, moyenne_client_v2_create, et_client_v2_create, threshold_client_v2_create = calculate_metric(res_all_df, str_key_word='client/v2/create')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Z73BNJ1Ogm"
      },
      "source": [
        "### Plot des utilisateurs en fonctions des opérations effectuées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzMB3Fbd1QdL"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_users(list_operations, title_operation='lifestyle', threshold=threshold_lifestyle):\n",
        "  bins = np.linspace(math.ceil(min(list_operations)), \n",
        "                    math.floor(max(list_operations)),\n",
        "                    10) # fixed number of bins\n",
        "  plt.axvline(x=threshold, color='red') # pour tracer le threshold \n",
        "  plt.xlim([min(list_operations)-5, max(list_operations)+5])\n",
        "\n",
        "  plt.hist(list_operations, bins=bins, alpha=0.5)\n",
        "  plt.title(f'Distribution des operations ({title_operation}) avec seuil {threshold}')\n",
        "  plt.xlabel(f'somme des opérations {title_operation}')\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvyPtS8x1YBn"
      },
      "source": [
        "### si les données sont à la moyenne plus 3 écart-types alors, il y'a de fortes chances qu'elles soient des outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vom5kIGP1afx"
      },
      "source": [
        "plot_users(sum_operation_lifestyle, 'lifestyle', threshold_lifestyle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvQhV7Vm1xxb"
      },
      "source": [
        "plot_users(sum_operation_accept, 'accept', threshold_accept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7slwD3Gy16tj"
      },
      "source": [
        "plot_users(sum_operation_adverts, 'adverts', threshold_adverts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhnkbV6T2Aa1"
      },
      "source": [
        "plot_users(sum_operation_app_get, 'app_get', threshold_app_get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnsCiTVD2DX2"
      },
      "source": [
        "plot_users(sum_operation_bank, 'bank', threshold_bank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ED97932Giq"
      },
      "source": [
        "plot_users(sum_operation_client, 'client', threshold_client)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7072ck_R3usA"
      },
      "source": [
        "plot_users(sum_operation_client_get, 'client_get', threshold_client_get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rIyxztg3x7U"
      },
      "source": [
        "plot_users(sum_operation_client_v2_create, 'client_v2_create', threshold_client_v2_create)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nMsQmV33eg"
      },
      "source": [
        "plot_users(sum_operation_decline, 'decline', threshold_decline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwP81Hk936kh"
      },
      "source": [
        "plot_users(sum_operation_Location, 'locations', threshold_Location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPoISSKQ3_Ht"
      },
      "source": [
        "plot_users(sum_operation_payment, 'payment', threshold_payment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO3v9dec4F8G"
      },
      "source": [
        "plot_users(sum_operation_preapproved, 'preapproved', threshold_preapproved)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ffhf9lq4JIY"
      },
      "source": [
        "plot_users(sum_operation_product, 'products', threshold_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Np4Usg4f8v"
      },
      "source": [
        "plot_users(sum_operation_update, 'updated', threshold_update)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX3TDdnp4lQ-"
      },
      "source": [
        "plot_users(sum_operation_wallet_balance, 'wallet_balance', threshold_wallet_balance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hznCVyQo404A"
      },
      "source": [
        "### Regrouper les utilisateurs en fonction du seuil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU3WdW7h4xjj"
      },
      "source": [
        "def group_users(list_users, list_phone_number_user, threshold):\n",
        "  '''cette fonction permet de regrouper les utilisateurs\n",
        "    en fonction de si leurs nombres d'opérations sont supérieurs ou inférieurs\n",
        "    à un seuil.  '''\n",
        "\n",
        "  list_users_great_than_threshold = []\n",
        "  list_users_less_than_threshold = []\n",
        "  dict_res = {}\n",
        "\n",
        "  for i, element in enumerate(list_users):\n",
        "    if element > threshold:\n",
        "      dict_res[list_phone_number_user[i]] = 1\n",
        "      list_users_great_than_threshold.append(list_phone_number_user[i])\n",
        "    elif element < threshold:\n",
        "      dict_res[list_phone_number_user[i]] = 0\n",
        "      list_users_less_than_threshold.append(list_phone_number_user[i])\n",
        "  \n",
        "  return dict_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY03FvSv45-N"
      },
      "source": [
        "dict_client = group_users(sum_operation_client, list_phone_number_user_client, threshold_client)\n",
        "dict_accept = group_users(sum_operation_accept, list_phone_number_user_accept, threshold_accept)\n",
        "dict_adverts = group_users(sum_operation_adverts, list_phone_number_user_adverts, threshold_adverts)\n",
        "dict_app_get = group_users(sum_operation_app_get, list_phone_number_user_app_get, threshold_app_get)\n",
        "dict_bank = group_users(sum_operation_bank, list_phone_number_user_bank, threshold_bank)\n",
        "dict_client_get = group_users(sum_operation_client_get, list_phone_number_user_client_get, threshold_client_get)\n",
        "dict_client_v2_create = group_users(sum_operation_client_v2_create, list_phone_number_user_client_v2_create, threshold_client_v2_create)\n",
        "dict_decline = group_users(sum_operation_decline, list_phone_number_user_decline, threshold_decline)\n",
        "dict_lifestyle = group_users(sum_operation_lifestyle, list_phone_number_user_lifestyle, threshold_lifestyle)\n",
        "dict_location = group_users(sum_operation_Location, list_phone_number_user_location, threshold_Location)\n",
        "dict_payment = group_users(sum_operation_payment, list_phone_number_user_payment, threshold_payment)\n",
        "dict_preapproved = group_users(sum_operation_preapproved, list_phone_number_user_preapproved, threshold_preapproved)\n",
        "dict_product = group_users(sum_operation_product, list_phone_number_user_product, threshold_product)\n",
        "dict_update = group_users(sum_operation_update, list_phone_number_user_update, threshold_update)\n",
        "dict_wallet_balance = group_users(sum_operation_wallet_balance, list_phone_number_user_wallet_balance, threshold_wallet_balance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U09ItskE5DOC"
      },
      "source": [
        "### Créer des DataFrame en fonction des utilisateurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXr7KFmF5AaP"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_df(one_dict, key_word='great_less_than_threshold'):\n",
        "\n",
        "  list_key = []\n",
        "  list_value = []\n",
        "  for key in one_dict:\n",
        "    list_key.append(key)\n",
        "    list_value.append(one_dict[key])\n",
        "\n",
        "  dict_res ={'phone_number_user': list_key, key_word: list_value}\n",
        "  return pd.DataFrame(dict_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou7-50Kq5Ldx"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de endpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txh-7vCc5Mxv"
      },
      "source": [
        "# Création du dataframe en fonction de l'utilisation du threshold de l'endpoint \"client\"\n",
        "df_client=create_df(dict_client, key_word='great_less_than_threshold_client')\n",
        "df_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SApUu8Bp5Saq"
      },
      "source": [
        "# vérifions les utilisateurs qui ont un seuil = 1\n",
        "df_client[df_client['great_less_than_threshold_client'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2_T0AH05YxB"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"accept\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGaTERP5WKZ"
      },
      "source": [
        "df_accept = create_df(dict_accept, key_word='great_less_than_threshold_accept')\n",
        "df_accept"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZacxHO45eOe"
      },
      "source": [
        "df_accept[df_accept['great_less_than_threshold_accept'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2zredbj8K40"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"adverts\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSEJeQR5kW2"
      },
      "source": [
        "df_adverts = create_df(dict_adverts, key_word='great_less_than_threshold_adverts')\n",
        "df_adverts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GibGOIjC5kCg"
      },
      "source": [
        "df_adverts[df_adverts['great_less_than_threshold_adverts'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Mmdg9p8ROy"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"application get\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDZ47vW08M6T"
      },
      "source": [
        "df_app_get = create_df(dict_app_get, key_word='great_less_than_threshold_app_get')\n",
        "df_app_get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnIduftg8axr"
      },
      "source": [
        "df_app_get[df_app_get['great_less_than_threshold_app_get'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJ0gVoA8k1m"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"bank\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhiDjkYI8efA"
      },
      "source": [
        "df_bank = create_df(dict_bank, key_word='great_less_than_threshold_bank')\n",
        "df_bank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y8gnSOV8iLb"
      },
      "source": [
        "df_bank[df_bank['great_less_than_threshold_bank'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd3D586P8mad"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"client get\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdzM04Hc8mvi"
      },
      "source": [
        "df_client_get = create_df(dict_client_get, key_word='great_less_than_threshold_client_get')\n",
        "df_client_get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrhkn9Us8m7-"
      },
      "source": [
        "df_client_get[df_client_get['great_less_than_threshold_client_get'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKtVk0PH86t5"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"client v2 create\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRJg6XBs8z_W"
      },
      "source": [
        "df_client_v2_create = create_df(dict_client_v2_create, key_word='great_less_than_threshold_client_v2_create')\n",
        "df_client_v2_create"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1DvYDet88Du"
      },
      "source": [
        "df_client_v2_create[df_client_v2_create['great_less_than_threshold_client_v2_create'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUFI0Vj7-bZ9"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"decline\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PRTGobv-cwc"
      },
      "source": [
        "df_decline = create_df(dict_decline, key_word='great_less_than_threshold_decline')\n",
        "df_decline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a94eiVsr-dTL"
      },
      "source": [
        "df_decline[df_decline['great_less_than_threshold_decline'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9wjmMr-nLn"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"payment\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSg9VTBU-oj2"
      },
      "source": [
        "df_payment = create_df(dict_payment, key_word='great_less_than_threshold_payment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_5Osjaw-o5X"
      },
      "source": [
        "df_payment[df_payment['great_less_than_threshold_payment'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YDBIWRD-1Tr"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"lifestyle\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__1LTJe-2d5"
      },
      "source": [
        "df_lifestyle = create_df(dict_lifestyle, key_word='great_less_than_threshold_lifestyle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VTLGzfN-8tW"
      },
      "source": [
        "df_lifestyle[df_lifestyle['great_less_than_threshold_lifestyle'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jVJm8Tu_AoS"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"location\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kYIq2JJ_CBA"
      },
      "source": [
        "df_location = create_df(dict_location, key_word='great_less_than_threshold_location')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E21MZd6R_JM6"
      },
      "source": [
        "df_location[df_location['great_less_than_threshold_location'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkfG2CB6_XBv"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"preapproved\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ9INCO9_ckB"
      },
      "source": [
        "df_preapproved = create_df(dict_preapproved, key_word='great_less_than_threshold_preapproved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTC_hNd_dCJ"
      },
      "source": [
        "df_preapproved[df_preapproved['great_less_than_threshold_preapproved'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O38pbzk_ptf"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"products\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk9t1WVj_rNl"
      },
      "source": [
        "df_product = create_df(dict_product, key_word='great_less_than_threshold_products')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fVgcWSi_rqu"
      },
      "source": [
        "df_product[df_product['great_less_than_threshold_products'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8YAlnKR_2RQ"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"update\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvO6a3Ld_yiW"
      },
      "source": [
        "df_update = create_df(dict_update, key_word='great_less_than_threshold_update')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWLKwkBO_yR4"
      },
      "source": [
        "df_update[df_update['great_less_than_threshold_update'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGnSW9U3AA-S"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"wallet balance\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDTYrFu4ACHv"
      },
      "source": [
        "df_wallet_balance = create_df(dict_wallet_balance, key_word='great_less_than_threshold_wallet_balance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lcN60vqACri"
      },
      "source": [
        "df_wallet_balance[df_wallet_balance['great_less_than_threshold_wallet_balance'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfSD3B6VAMB3"
      },
      "source": [
        "## Faire la jointure des dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAJQl-i2AQKL"
      },
      "source": [
        "### premièrement: ordonner les colonnes de numéros de téléphone par ordre croissant\n",
        "### NB: Toujours re-indexer (.reset_index(drop=True)) les dataframe avant de faire des JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXdKXnZTARSa"
      },
      "source": [
        "# Ordonnons les dataset, ensuite ré-indexons les index ce qui nous facilitera l'opération de concatenation.\n",
        "sorted_df_client = df_client.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_accept = df_accept.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_adverts = df_adverts.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_app_get = df_app_get.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_bank = df_bank.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_client_get = df_client_get.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_client_v2_create = df_client_v2_create.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_decline = df_decline.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_payment = df_payment.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_lifestyle = df_lifestyle.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_location = df_location.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_preapproved = df_preapproved.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_product = df_product.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_update = df_update.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_wallet_balance = df_wallet_balance.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwfw3annAZ-U"
      },
      "source": [
        "### Concatenons tous les dataframes pour avoir un dataframe avec lequel on va entrainer nos modèles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch652n2RAcAo"
      },
      "source": [
        "list_users = [sorted_df_client, sorted_df_accept['great_less_than_threshold_accept'], sorted_df_adverts['great_less_than_threshold_adverts'], sorted_df_app_get['great_less_than_threshold_app_get'], \n",
        "              sorted_df_bank['great_less_than_threshold_bank'], sorted_df_client_get['great_less_than_threshold_client_get'], \n",
        "              sorted_df_client_v2_create['great_less_than_threshold_client_v2_create'], sorted_df_decline['great_less_than_threshold_decline'], \n",
        "              sorted_df_payment['great_less_than_threshold_payment'], sorted_df_lifestyle['great_less_than_threshold_lifestyle'], \n",
        "              sorted_df_location['great_less_than_threshold_location'], sorted_df_preapproved['great_less_than_threshold_preapproved'], \n",
        "              sorted_df_product['great_less_than_threshold_products'], sorted_df_update['great_less_than_threshold_update'], \n",
        "              sorted_df_wallet_balance['great_less_than_threshold_wallet_balance']]  # List of our dataframes\n",
        "df_final_users = pd.concat(list_users, axis=1)\n",
        "df_final_users.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zX8XsnlApVy"
      },
      "source": [
        "# Faire la somme de toutes les colonnes et mettre dans le résultat dans la colonne \"score_total\"\n",
        "df_final_users['score_total'] = df_final_users.iloc[:, 1:16].sum(axis=1)\n",
        "df_final_users['score_total'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0CkLTfOAtTB"
      },
      "source": [
        "list_score_total = df_final_users['score_total'].values # convertir une series en list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7rMOpssA5pY"
      },
      "source": [
        "list_score_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBc7YPRFA_Aj"
      },
      "source": [
        "import numpy as np\n",
        "med = np.median(list_score_total)\n",
        "moy = np.mean(list_score_total)\n",
        "ecart_type_score_total = np.std(list_score_total)\n",
        "threshold_score_total = med + 3*ecart_type_score_total\n",
        "threshold_score_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbiwciKDBCoI"
      },
      "source": [
        "plot_users(df_final_users['score_total'].values, 'score_total' , threshold_score_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hem3YJT0BLst"
      },
      "source": [
        "### Récupérer les utilisateurs qui sont au delà du seuil.\n",
        "### les utilisateurs qui sont au délà du seuil peuvent etre considérés comme ceux qui ont effectué des transactions douteuses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IvgcXr0BKc_"
      },
      "source": [
        "df_resultats = df_final_users[df_final_users['score_total'] > threshold_score_total]\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhUHOLPeBcLF"
      },
      "source": [
        "df_final_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlAGp5R3Bh-U"
      },
      "source": [
        "### Selon les résultats affichés, la composantes pc2 de PCA répresente les personnes qui ont effectués beaucoup d'achats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYOxOMUBqhd"
      },
      "source": [
        "### on drop la prémière colonne parce que les données qu'on veut passer à notre modèle de clustering doivent etre des nombres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27MlLI8kBjJ2"
      },
      "source": [
        "df_result = df_final_users.drop('phone_number_user', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71lzaD3sByAP"
      },
      "source": [
        "### Ensuite on convertit notre dataframe en liste pour le passer au modèle de clustering KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WgG9WtB0aZ"
      },
      "source": [
        "# convertir le dataframe en list\n",
        "list_resultats = df_result.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Ho0joWD56E"
      },
      "source": [
        "print(list_resultats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PS-K60zE83R"
      },
      "source": [
        "cleanedList = [[x for x in y if not np.isnan(x)] for y in list_resultats]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adOBiT2AEb3_"
      },
      "source": [
        "print(cleanedList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMJVdL1hB6Iz"
      },
      "source": [
        "### Algorithme de clustering KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjgbYe9B5Fk"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkDMcq4qCEuP"
      },
      "source": [
        "### on fait la standardisation parce que la dernière colonne du dataframe a des valeurs prépondérantes (9, 3, etc...) par rapport aux autres pour que toutes les valeurs du dataframe soient comprises entre -1 et 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EEvryCgCGHN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "Scaled_data = StandardScaler().fit_transform(cleanedList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHYNP33FCMzo"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import random\n",
        "\n",
        "# Scale data before applying PCA\n",
        "#scaling=StandardScaler()\n",
        " \n",
        "# Use fit and transform method\n",
        "#scaling.fit(df1)\n",
        "#Scaled_data=scaling.transform(df1)\n",
        " \n",
        "# Set the n_components=2\n",
        "principal=PCA(n_components=2)\n",
        "principal.fit(Scaled_data)\n",
        "x=principal.transform(Scaled_data)\n",
        " \n",
        "# Check the dimensions of data after PCA\n",
        "#print(x.shape)\n",
        "\n",
        "principal.components_\n",
        "\n",
        "df_sortie = pd.DataFrame(x, columns=['pc1', 'pc2'])\n",
        "#print(df_sortie)\n",
        "df_filtre = df_sortie[(df_sortie['pc1'] >= 7) & (df_sortie['pc2'] >= 6)]\n",
        "print(df_filtre)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(x[:,0]+random.uniform(0, 1)/10, x[:,1]+random.uniform(0, 1)/100, c='blue', cmap='plasma', marker='x', alpha=0.4)\n",
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMBr6KTbCx61"
      },
      "source": [
        "df_filtre_1 = df_sortie[(df_sortie['pc1'] <= 5) & (df_sortie['pc2'] < -1)]\n",
        "print(df_filtre_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhk7v4WhC-io"
      },
      "source": [
        "### Fonction pour faire le plot de tous nos points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12hJ1ndDHMj"
      },
      "source": [
        "cleanedList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCRmlkb6C9jN"
      },
      "source": [
        "def scatter_plot(list_points):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in list_points:\n",
        "    x.append(i[0])\n",
        "    y.append(i[1])\n",
        "\n",
        "  plt.scatter(x,y, s=200)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qti72J6fDEfH"
      },
      "source": [
        "scatter_plot(cleanedList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxBE-OyYDQOr"
      },
      "source": [
        "# on choisit trois clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXHKKxe6DTZE"
      },
      "source": [
        "y_pred = kmeans.fit_predict(cleanedList)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}