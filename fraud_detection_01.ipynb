{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fraud_detection_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7NFye/9GF7R0ifFIamfkW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfolaa/fraud_detection/blob/main/fraud_detection_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBkuHMcekLs0",
        "outputId": "cf5e376a-f486-428d-da57-27c12ba0ee5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlJ4DsLDm3OP"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import glob2\n",
        "import tqdm\n",
        "import json\n",
        "import pandas as pd\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxvxpGmXjwcU",
        "outputId": "10e22634-c883-4e68-b798-8eb57f0a54bf"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v1c4xv3jFmT"
      },
      "source": [
        "def read_json_insert_csv(root_path, json_file, file_csv):\n",
        "  data = json.load(json_file)\n",
        "  df = pd.DataFrame.from_records(data)\n",
        "  # convert file to csv\n",
        "  df.to_csv(f'{root_path}/{file_csv}', \n",
        "            sep='|', \n",
        "            index= None)\n",
        "\n",
        "  # return 1 fichier csv fer json file\n",
        "  return df "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TZcB6c4jX06"
      },
      "source": [
        "def process_json( path_file_json=\"./nirra-log-bot\", dest_path=\"./liberta_leasing\"):\n",
        "  # créer toute l'aborescence du fichier, crée le chemin\n",
        "  os.makedirs(dest_path, exist_ok=True) \n",
        "  # read all json files\n",
        "\n",
        "  json_files = glob2.glob(os.path.join(path_file_json,'*.json'))\n",
        "  json_files = sorted(json_files)\n",
        "  for file_name in tqdm.tqdm(json_files):\n",
        "    with open(file_name) as json_file:\n",
        "      path_file_csv = file_name.replace(\".json\", \".csv\").split(\"/\")[-1]\n",
        "      read_json_insert_csv(dest_path, json_file, path_file_csv)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA-YDRr_RGOy",
        "outputId": "2ac79589-ae31-4d4d-f015-73ab26323d02"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fv5tpRwHg7p"
      },
      "source": [
        "json_files = glob2.glob(os.path.join('./nirra-log-bot','*.json'))\n",
        "#json_files = filter(lambda x: x.endswith('.json'), os.listdir('./nirra-log-bot'))\n",
        "files = sorted(json_files)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJBAro0HsEp"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5g5SpxFmowA",
        "outputId": "dd504b6f-d409-4337-ea43-74c184c9ea04"
      },
      "source": [
        "process_json('./nirra-log-bot', './liberta_leasing')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171/171 [00:03<00:00, 54.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noiqJADKqiG2",
        "outputId": "3cdf20e0-dbf0-4cdd-fba5-2df0c4c0bfc9"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/liberta_leasing/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/liberta_leasing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFG_FlZCqhVf",
        "outputId": "f13cf609-0354-432c-fd5e-ef62bd68946b"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# use glob to get all the csv files \n",
        "# in the folder\n",
        "path = os.getcwd()\n",
        "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "csv_files = sorted(csv_files)\n",
        "all_df_list = [] # list of all dataframe\n",
        "for f in tqdm(csv_files):    \n",
        "    # read the csv file\n",
        "    #col_names = [\"type\", \"subtype\", \"text\", \"ts\", \"bot_id\"]\n",
        "    df=pd.read_csv(f, sep=\"|\")\n",
        "    all_df_list.append(df) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171/171 [00:00<00:00, 173.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzFoFdsXsBqG",
        "outputId": "b3d346eb-2bbe-4895-cde8-3197aa465c25"
      },
      "source": [
        "print(len(all_df_list))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "8PM0Fv1PL36b",
        "outputId": "451fda8f-7812-41b7-92f4-bedb7eb22aeb"
      },
      "source": [
        "all_df_list[0][all_df_list[0]['type']== 'type']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>subtype</th>\n",
              "      <th>ts</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "      <th>bot_id</th>\n",
              "      <th>bot_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [type, subtype, ts, user, text, bot_id, bot_link]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm7b9wtC3MWT"
      },
      "source": [
        "### Concatener tous la liste des dataframes dans un seul DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeyAENxG1h9m"
      },
      "source": [
        "df_raw = pd.concat(all_df_list, ignore_index=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaZ3gAcgEvrt"
      },
      "source": [
        "df_raw.to_csv('/content/drive/MyDrive/datasets/nirra_log_bot.csv', index=None, sep='|')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyLEBiBuNxJN",
        "outputId": "d9b19088-af8c-4b61-c782-744f65cab02c"
      },
      "source": [
        "df_temp = pd.read_csv('/content/drive/MyDrive/datasets/nirra_log_bot.csv', sep='|')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3,6,7,8,9,10,11,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYCxywmRODjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931f8c21-cb63-42aa-d1bc-888dc852f7ff"
      },
      "source": [
        "df_temp.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 95963 entries, 0 to 95962\n",
            "Data columns (total 14 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   type           95963 non-null  object \n",
            " 1   subtype        95962 non-null  object \n",
            " 2   ts             95963 non-null  float64\n",
            " 3   user           7 non-null      object \n",
            " 4   text           95963 non-null  object \n",
            " 5   bot_id         95957 non-null  object \n",
            " 6   bot_link       1 non-null      object \n",
            " 7   client_msg_id  1 non-null      object \n",
            " 8   team           1 non-null      object \n",
            " 9   user_team      1 non-null      object \n",
            " 10  source_team    1 non-null      object \n",
            " 11  user_profile   1 non-null      object \n",
            " 12  blocks         1 non-null      object \n",
            " 13  inviter        1 non-null      object \n",
            "dtypes: float64(1), object(13)\n",
            "memory usage: 10.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkOAge1sVOr"
      },
      "source": [
        "def parse_wallet_sms_payload_success(text_type_request):\n",
        "  ''' la fonction permet de parser les types de requete \"Okra WebHook\", \"Wallet success\", \n",
        "      \"SMS Success\" et SMS Payload en object json.\n",
        "      Elle prend en paramètre le text contenu dans le type de requete,\n",
        "      elle retourne un objet de type JSON.'''\n",
        "\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_type_request)\n",
        "  res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "  s = json.loads(res)\n",
        "  out_dict = {} # dictionnary vide\n",
        "  for key, value in s.items():\n",
        "    out_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "\n",
        "\n",
        "  out_dump = json.dumps(out_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "  out_wallet_success = json.loads(out_dump) # convertir le string json en object json.\n",
        "  return out_wallet_success\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXFZHNE_Eft4"
      },
      "source": [
        "def parse_providus_transfer_error_function(text_type_request):\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_type_request)\n",
        "  if len(resul_patt) != 0:\n",
        "    res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "    s = json.loads(res)\n",
        "    out_dict = {} # dictionnary vide\n",
        "    for key, value in s.items():\n",
        "      out_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "    out_dump = json.dumps(out_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "    out_wallet_success = json.loads(out_dump) # convertir le string json en object json.\n",
        "    return out_wallet_success\n",
        "  else:\n",
        "    resultat = 'Transfer to virtual account is not allowed!'\n",
        "    return resultat\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv7AQb4Ysb5D"
      },
      "source": [
        "import re\n",
        "\n",
        "# la fonction doit prendre en paramètre quelque chose\n",
        "def parse_and_concatenate_Leadway_Success_Rows(df_raw):\n",
        "  '''Cette fonction permet de parser et de concatener le texte qui a LEADWAY SUCCESS\n",
        "     comme type de requete\n",
        "     elle prend comme paramètre un dataframe et retourne les valeurs suivantes:\n",
        "     - un texte concatené\n",
        "     - l'index de la 1ère ligne qu'on va utiliser ensuite pour l'effacer\n",
        "     - l'index de la dernière ligne qu'on va utiliser ensuite pour l'effacer '''\n",
        "\n",
        "  first_index = 0\n",
        "  last_index = 0\n",
        "  text_leadway_concat = ''\n",
        "  for index, row in df_raw.iterrows():  # boucler sur les colonnes de type text\n",
        "      text_row = row['text']  \n",
        "      if re.search('LEADWAY SUCCESS', text_row):\n",
        "        text_leadway_concat = text_row\n",
        "        first_index = index\n",
        "        first_index +=1\n",
        "        new_df = df_raw[first_index:]\n",
        "        for first_index, new_row in new_df.iterrows():\n",
        "          xxx = new_row['text']      \n",
        "          if not xxx.startswith('['):          \n",
        "            first_index += 1\n",
        "            text_leadway_concat = text_row + xxx       \n",
        "          elif xxx.startswith('['):\n",
        "            last_index = first_index-1\n",
        "            break\n",
        "\n",
        "\n",
        "  return text_leadway_concat, first_index, last_index\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1_on8resgdJ"
      },
      "source": [
        "\n",
        "# use this function when type request is LEADWAY SUCCESS\n",
        "import regex\n",
        "import json\n",
        "\n",
        "\n",
        "def parse_Leadway_Success_Row(text_leadway):\n",
        "  ''' fonction permettant de parser le text concatené pour le type de requet LEADWAY SUCCESS\n",
        "      elle retourner un dictionnaire.'''\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_leadway)\n",
        "  resul_patt[0] = resul_patt[0].replace(\"\\\\\", \"\")\n",
        "  x = resul_patt[0].replace(\"make,\", \"\")\n",
        "  y = x.replace('\"\"makeName\"', '\"makeName\"')\n",
        "  z = json.loads(y)\n",
        "  vehicleMake = z.get('vehicleMake')\n",
        "  leadway_dict = {}\n",
        "  for element in vehicleMake:\n",
        "    leadway_dict[element['id']] = element['makeName']\n",
        "\n",
        "  return leadway_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u470nALIsoTx"
      },
      "source": [
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_Error_Row(error_row):\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"{?[a-z :A-Z 0-9\\\\,=_`']+selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "  res = res.replace(\"'name'\", \"name\").replace(\"`\", \"\").replace(\"'18'\", \"18\").replace(\"'monthly'\", \"monthly\")\n",
        "  res = res+'\"}'\n",
        "  res = res.replace(\"'\", '\"')\n",
        "  s = json.loads(res)\n",
        "  out_error_dict = {} # dictionnary vide\n",
        "  for key, value in s.items():\n",
        "    out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "\n",
        "  out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "  out_error_text = json.loads(out_error_dump) # convertir le string json en object json.\n",
        "  return out_error_text\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKcYlDZoRfqD"
      },
      "source": [
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_Error_Row_selfie_Function(error_row):\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"({?[a-z :A-Z 0-9\\\\,=_`']+)selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  # checker si la liste contient au moins un element\n",
        "  if len(resul_patt) !=0 :\n",
        "    res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "    res = res+'\"}'\n",
        "    res = res.replace(\"'monthly'\", \"monthly\").replace(\"'\", '\"').replace('\"\"', '\"').replace('\"monthly \", \"', '\"monthly\"')\n",
        "    s = json.loads(res)\n",
        "    out_error_dict = {} # dictionnary vide\n",
        "    for key, value in s.items():\n",
        "      out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "    out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "    out_error_text = json.loads(out_error_dump)\n",
        "    out_error_text\n",
        "    return out_error_text\n",
        "  #si la liste est vide, on la retourne\n",
        "  else:\n",
        "    return resul_patt"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBN4__k_x7Gf"
      },
      "source": [
        "def parse_Error_Row_selfie_INSERT_INTO_Function(error_row):\n",
        "  error_row = df_raw['text'][28813]\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"({?[a-z :A-Z 0-9\\\\,=_`']+)selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  if len(resul_patt) !=0 :\n",
        "      res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "      res = res+'\"}'\n",
        "      res = res.replace(\"'monthly'\", \"monthly\").replace(\"'\", '\"').replace('\"\"', '\"').replace(\"`\", '\"').replace('monthly, \"', '\"monthly\"').replace('\"name\"', 'name')\n",
        "      res = res.replace('\"product\"', 'product').replace('\"loan_amount\"', 'loan_amount').replace('\"tenor\"', 'tenor').replace('\"loan_purpose\"', 'loan_purpose')\n",
        "      res = res.replace('\"14\"', '14').replace('\"tenor_type\"', 'tenor_type').replace('\"monthly\"', 'monthly')\n",
        "      s = json.loads(res)\n",
        "      out_error_dict = {} # dictionnary vide\n",
        "      for key, value in s.items():\n",
        "        out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "      out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "      out_error_text = json.loads(out_error_dump)\n",
        "      out_error_text\n",
        "      return out_error_text\n",
        "    #si la liste est vide, on la retourne\n",
        "  else:\n",
        "      return resul_patt"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W0AyZz04cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58951598-ba5b-4f31-882c-55fcb5ac01dc"
      },
      "source": [
        "parse_Error_Row_selfie_INSERT_INTO_Function(df_raw['text'][28813])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "550yp0F11MLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c7bdf1-0333-40df-957b-31715c7a40c3"
      },
      "source": [
        "parse_Error_Row_selfie_INSERT_INTO_Function(df_raw['text'][86636])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZZ0t3_iPhlj"
      },
      "source": [
        "def parse_Error_Row_Function (error_row):\n",
        "  res = []\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  for one_res in resul_patt:\n",
        "    res.append(one_res.replace(\"\\\\\", \" \").replace(\"`\", \"\").replace(\"\\'\", \"\"))\n",
        "  list_json = [json.loads(stuff) for stuff in res]\n",
        "  return list_json"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JECgNVsrrw"
      },
      "source": [
        "import datetime\n",
        "\n",
        "# function to convert date to Timestamp\n",
        "def convertToTimestamp(str):\n",
        "  element = datetime.datetime.strptime(str,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "  return datetime.datetime.timestamp(element)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raIfy0cxVX9z",
        "outputId": "98389677-891d-423a-e5eb-ba3259b327cc"
      },
      "source": [
        "convertToTimestamp('2021-08-12T13:53:23.624Z')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1628776403.624"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWCkinr1V27P"
      },
      "source": [
        "### Prendre la valeur de la colonne Timestamp comme date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8hteMxKsueg"
      },
      "source": [
        "type_request_dictionnary = {}\n",
        "regex_list_api_request = []\n",
        "regex_list_api_request.append('[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "regex_list_api_request.append('/[/a-z 0-9?=&;/_A-Z+]+')\n",
        "regex_list_api_request.append('(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):(\\d\\d).(\\d{3})*[a-zA-Z]')\n",
        "regex_list_api_request.append('[0-9]+')\n",
        "\n",
        "type_request_dictionnary['API REQUEST'] = regex_list_api_request\n",
        "\n",
        "regex_list_client_mobile = []\n",
        "regex_list_client_mobile.append('[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "regex_list_client_mobile.append('(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):(\\d\\d).(\\d{3})*[a-zA-Z]')\n",
        "regex_list_client_mobile.append('[0-9]+')\n",
        "\n",
        "type_request_dictionnary['CLIENT MOBILE LOGIN'] = regex_list_client_mobile\n",
        "\n",
        "type_request_dictionnary['SMS PAYLOAD'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['SMS SUCCESS'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['WALLET SUCCESS'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['LEADWAY SUCCESS'] = '\\{(?:[^{}]|(?R))*}'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVGf909EUhmO"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU-m87ITsx60"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "\n",
        "def parse_row_error(df_):\n",
        "  log_level_col = []\n",
        "  type_request_col = []\n",
        "  error_code_col = []\n",
        "  error_number_col = []\n",
        "  error_sql_message_col = []\n",
        "  error_sql_state_col = []\n",
        "  error_index_col = []\n",
        "  error_sql_col = []\n",
        "  loan_amount_col = []\n",
        "  loan_purpose_col = []\n",
        "  product_col = []\n",
        "  tenor_col = []\n",
        "  tenor_type_col = []\n",
        "  error_created_by_col = []\n",
        "  error_creator_type_col = []\n",
        "  error_date_created_col = []\n",
        "  error_name_col = []\n",
        "  error_rate_col = []\n",
        "  error_request_col = []\n",
        "  error_status_col = []\n",
        "  error_userID_col = []\n",
        "  date_col = []\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, log_level_col, error_code_col, error_number_col, error_sql_message_col, error_sql_state_col, \n",
        "                    error_index_col, error_sql_col, loan_amount_col, loan_purpose_col, product_col, tenor_col, tenor_type_col,\n",
        "                    error_created_by_col, error_creator_type_col, error_date_created_col, error_name_col, error_rate_col,\n",
        "                    error_request_col, error_status_col, error_userID_col, date_col]\n",
        "\n",
        "  for index, row in df_.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "    \n",
        "    if re.search('error', str_text):\n",
        "      log_level = re.search('error', str_text)\n",
        "      type_of_request = re.search('LOAN ERROR', str_text)\n",
        "      date_col.append(row['ts'])\n",
        "\n",
        "      if 'INSERT INTO' not in str_text:  \n",
        "          try:\n",
        "            log_level_col.append(log_level.group(0))\n",
        "          except AttributeError:\n",
        "            log_level_col.append(None)\n",
        "          try:\n",
        "            type_request_col.append(type_of_request.group(0))\n",
        "          except AttributeError:\n",
        "            type_request_col.append(None)         \n",
        "          try:\n",
        "            loan_error = parse_Error_Row_selfie_Function(str_text) \n",
        "          except json.decoder.JSONDecodeError:\n",
        "            raise          \n",
        "          try:\n",
        "            loan_amount_col.append(loan_error.get('loan_amount'))           \n",
        "          except AttributeError:\n",
        "            loan_amount_col.append(None)\n",
        "          try:\n",
        "            loan_purpose_col.append(loan_error.get('loan_purpose'))              \n",
        "          except AttributeError:\n",
        "            loan_purpose_col.append(None)\n",
        "          try:\n",
        "            product_col.append(loan_error.get('product'))\n",
        "          except AttributeError:\n",
        "            product_col.append(None)\n",
        "          try:\n",
        "            tenor_col.append(loan_error.get('tenor'))\n",
        "          except AttributeError:\n",
        "            tenor_col.append(None)\n",
        "          try:\n",
        "            tenor_type_col.append(loan_error.get('tenor_type'))\n",
        "          except AttributeError:\n",
        "            tenor_type_col.append(None)            \n",
        "          \n",
        "          error_code_col.append(None)\n",
        "          error_number_col.append(None)\n",
        "          error_sql_message_col.append(None)\n",
        "          error_sql_state_col.append(None)\n",
        "          error_index_col.append(None)\n",
        "          error_sql_col.append(None)  \n",
        "          error_userID_col.append(None)\n",
        "          error_status_col.append(None) \n",
        "          error_request_col.append(None)  \n",
        "          error_rate_col.append(None)\n",
        "          error_name_col.append(None)\n",
        "          error_date_created_col.append(None)\n",
        "          error_creator_type_col.append(None)\n",
        "          error_created_by_col.append(None)    \n",
        "      else:     \n",
        "        try:\n",
        "          loan_error = parse_Error_Row_selfie_INSERT_INTO_Function(str_text) \n",
        "          #print(loan_error)\n",
        "        except json.decoder.JSONDecodeError:\n",
        "          raise\n",
        "        loan_amount_col.append(None)\n",
        "        loan_purpose_col.append(None)\n",
        "        product_col.append(None)\n",
        "        tenor_col.append(None)\n",
        "        tenor_type_col.append(None)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "        try:\n",
        "          type_request_col.append(type_of_request.group(0))\n",
        "        except AttributeError:\n",
        "          type_request_col.append(None)\n",
        "        try:\n",
        "          error_code_col.append(loan_error.get('code'))\n",
        "        except AttributeError:\n",
        "          error_code_col.append(None)\n",
        "        try:\n",
        "          error_number_col.append(loan_error.get('errno'))\n",
        "        except AttributeError:\n",
        "          error_number_col.append(None)\n",
        "        try:\n",
        "          error_sql_message_col.append(loan_error.get('sqlMessage'))\n",
        "        except AttributeError:\n",
        "          error_sql_message_col.append(None)\n",
        "        try:\n",
        "          error_sql_state_col.append(loan_error.get('sqlState'))\n",
        "        except AttributeError:\n",
        "          error_sql_state_col.append(None)\n",
        "        try:\n",
        "          error_created_by_col.append(loan_error.get('created_by'))\n",
        "        except AttributeError:\n",
        "          error_created_by_col.append(None)\n",
        "        try:\n",
        "          error_creator_type_col.append(loan_error.get('creator_type'))\n",
        "        except AttributeError:\n",
        "          error_creator_type_col.append(None)\n",
        "        try:\n",
        "          error_date_created_col.append(loan_error.get('date_created'))\n",
        "        except AttributeError:\n",
        "          error_date_created_col.append(None)\n",
        "        try:\n",
        "          error_sql_col.append(loan_error.get('sql'))\n",
        "        except AttributeError:\n",
        "          error_sql_col.append(None)\n",
        "        try:\n",
        "          error_name_col.append(loan_error.get('name'))\n",
        "        except AttributeError:\n",
        "          error_name_col.append(None)\n",
        "        try:\n",
        "          error_rate_col.append(loan_error.get('rate'))\n",
        "        except AttributeError:\n",
        "          error_rate_col.append(None)\n",
        "        try:\n",
        "          error_index_col.append(loan_error.get('index'))\n",
        "        except AttributeError:\n",
        "          error_index_col.append(None)\n",
        "        try:\n",
        "          error_request_col.append(loan_error.get('request'))\n",
        "        except AttributeError:\n",
        "          error_request_col.append(None)\n",
        "        try:\n",
        "          error_status_col.append(loan_error.get('status'))\n",
        "        except AttributeError:\n",
        "          error_status_col.append(None)\n",
        "        try:\n",
        "          error_userID_col.append(loan_error.get('userID'))\n",
        "        except AttributeError:\n",
        "          error_userID_col.append(None)\n",
        "\n",
        "  df_['Type_Request'] = type_request_col\n",
        "  df_['Date'] = date_col\n",
        "  df_['Log_Level'] = log_level_col\n",
        "  df_['Error Code'] = error_code_col\n",
        "  df_['Error Number'] = error_number_col\n",
        "  df_['Error Sql Message'] = error_sql_message_col\n",
        "  df_['Error Sql State'] = error_sql_state_col\n",
        "  df_['Error Index'] = error_index_col\n",
        "  df_['Error Sql'] = error_sql_col\n",
        "  df_['Loan Amount'] = loan_amount_col\n",
        "  df_['Loan Purpose'] = loan_purpose_col\n",
        "  df_['Product'] = product_col\n",
        "  df_['Tenor'] = tenor_col\n",
        "  df_['Tenor Type'] = tenor_type_col\n",
        "  df_['Error created by'] = error_created_by_col\n",
        "  df_['Error creator Type'] = error_creator_type_col \n",
        "  df_['Error Date created'] = error_date_created_col\n",
        "  df_['Error Name'] = error_name_col\n",
        "  df_['Error Rate'] = error_rate_col\n",
        "  df_['Error Request'] = error_request_col\n",
        "  df_['Error Status'] = error_status_col\n",
        "  df_['Error UserID'] = error_userID_col\n",
        "\n",
        "\n",
        "  return df_"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUFaAC7BtJbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb67a3a-5e82-4618-a41f-946978adc526"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "error = df_raw[df_raw['text'].str.contains('LOAN ERROR')]\n",
        "df_error = parse_row_error(error)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:172: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:173: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:175: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:176: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:177: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:178: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:179: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:183: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:184: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:188: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:189: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:190: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:191: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:192: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:193: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxv3O9kZDlMa"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_LOAN_PAYLOAD(df_loan_payload):\n",
        "\n",
        "  log_level_col = []\n",
        "  type_request_col = []\n",
        "  loan_amount_col = []\n",
        "  loan_purpose_col = []\n",
        "  product_col = []\n",
        "  tenor_col = []\n",
        "  tenor_type_col = []\n",
        "  date_col = []\n",
        "\n",
        "\n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, log_level_col, loan_amount_col, loan_purpose_col, product_col, tenor_col, tenor_type_col, date_col]\n",
        "          \n",
        "  for index, row in df_loan_payload.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "    \n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        type_of_request = re.search('LOAN PAYLOAD', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "        try:\n",
        "          type_request_col.append(type_of_request.group(0))\n",
        "        except AttributeError:\n",
        "          type_request_col.append(None)         \n",
        "        try:\n",
        "          loan_error = parse_Error_Row_selfie_Function(str_text) \n",
        "        except json.decoder.JSONDecodeError:\n",
        "          raise \n",
        "        try:\n",
        "            loan_amount_col.append(loan_error.get('loan_amount'))           \n",
        "        except AttributeError:\n",
        "            loan_amount_col.append(None)\n",
        "        try:\n",
        "          loan_purpose_col.append(loan_error.get('loan_purpose'))              \n",
        "        except AttributeError:\n",
        "          loan_purpose_col.append(None)\n",
        "        try:\n",
        "          product_col.append(loan_error.get('product'))\n",
        "        except AttributeError:\n",
        "          product_col.append(None)\n",
        "        try:\n",
        "          tenor_col.append(loan_error.get('tenor'))\n",
        "        except AttributeError:\n",
        "          tenor_col.append(None)\n",
        "        try:\n",
        "          tenor_type_col.append(loan_error.get('tenor_type'))\n",
        "        except AttributeError:\n",
        "          tenor_type_col.append(None)  \n",
        "\n",
        "  \n",
        "  df_loan_payload['Type_Request'] = type_request_col\n",
        "  df_loan_payload['Log_Level'] = log_level_col\n",
        "  df_loan_payload['Loan Amount'] = loan_amount_col\n",
        "  df_loan_payload['Loan Purpose'] = loan_purpose_col\n",
        "  df_loan_payload['Product'] = product_col\n",
        "  df_loan_payload['Tenor'] = tenor_col\n",
        "  df_loan_payload['Tenor Type'] = tenor_type_col\n",
        "  df_loan_payload['Date'] = date_col\n",
        "\n",
        "  return df_loan_payload\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5cXVXeZUOua"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "loan_payload = df_raw[df_raw['text'].str.contains('LOAN PAYLOAD')]\n",
        "df_loan_payload = parse_row_LOAN_PAYLOAD(loan_payload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQowf3DowAM"
      },
      "source": [
        "df_loan_payload.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my38DIJIaPcF"
      },
      "source": [
        "### Handle DataFrame for API REQUEST Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O9Ghnbls4hN"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_api_request(df_api_request):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_api_request = []\n",
        "  list_column_none_api_request = [message_sms_payload_col, totalsent_col, cost_col, status_col,\n",
        "                                  bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, account_number_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_api_request.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)        \n",
        "        # check if the row contains an email address \n",
        "        # pour tous les types request créer un dictionnaire dans lequel mapper\n",
        "        # key = type de request et value = les regex définis\n",
        "        # pour chaque condition IF créer une liste de colonnes auxquelles affecter None\n",
        "        if 'mailto' in str_text:\n",
        "            if re.search('API REQUEST', str_text):\n",
        "                type_of_request = re.search('API REQUEST', str_text)                \n",
        "                phone_or_email_api_req = re.search(type_request_dictionnary['API REQUEST'][0], str_text)                              \n",
        "                endpoint = re.search(type_request_dictionnary['API REQUEST'][1], str_text)\n",
        "                pattern = type_request_dictionnary['API REQUEST'][2]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for i in range(len(list_column_none_api_request)):\n",
        "                  list_column_none_api_request[i].append(None)\n",
        "                               \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0)) # add type request inside type request column\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)               \n",
        "                try:\n",
        "                  email_col.append(phone_or_email_api_req.group(0)) # add email inside email column\n",
        "                  phone_Col.append(None)  # in this case there is no phone number\n",
        "                except AttributeError:\n",
        "                  email_col.append(None)\n",
        "                try:\n",
        "                  endpoint_col.append(endpoint.group(0)) # add endpoint inside endpoint column\n",
        "                except AttributeError:\n",
        "                  endpoint_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None)\n",
        "              \n",
        "\n",
        "        elif 'mailto' not in str_text:\n",
        "            if re.search('API REQUEST', str_text):\n",
        "                type_of_request = re.search('API REQUEST', str_text)                            \n",
        "                # extract a phone number for API REQUEST\n",
        "                phone_or_email_api_req = re.search(type_request_dictionnary['API REQUEST'][3], str_text)                              \n",
        "                endpoint = re.search(type_request_dictionnary['API REQUEST'][1], str_text)\n",
        "                pattern = type_request_dictionnary['API REQUEST'][2]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for i in range(len(list_column_none_api_request)):\n",
        "                  list_column_none_api_request[i].append(None)\n",
        "\n",
        "                try:\n",
        "                  phone_Col.append(phone_or_email_api_req.group(0)) # add phone number inside phone number column\n",
        "                  email_col.append(None) # in this case there is no email address\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  endpoint_col.append(endpoint.group(0)) # add endpoint inside endpoint column\n",
        "                except AttributeError:\n",
        "                  endpoint_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None)\n",
        "\n",
        "  df_api_request['Type_Request'] = type_request_col\n",
        "  df_api_request['Phone_Number'] = phone_Col\n",
        "  df_api_request['Date'] = date_col\n",
        "  df_api_request['EndPoint'] = endpoint_col\n",
        "  df_api_request['Log_Level'] = log_level_col\n",
        "  df_api_request['Email'] = email_col\n",
        "  df_api_request['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_api_request['Total Sent'] = totalsent_col\n",
        "  df_api_request['Cost'] = cost_col\n",
        "  df_api_request['Status'] = status_col\n",
        "  df_api_request['Account Number'] = account_number_col\n",
        "  df_api_request['Account Name'] = account_name_col\n",
        "  df_api_request['BVN'] = bvn_col\n",
        "  df_api_request['Request Successful'] = requestSuccessful_col\n",
        "  df_api_request['Response Message'] = responseMessage_col\n",
        "  df_api_request['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_api_request"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWb_8xSS8-wo"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XIIds-DtBOP"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "api_request = df_raw[df_raw['text'].str.contains('API REQUEST')]\n",
        "df_api_request = parse_row_api_request(api_request)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQURC4I9brJm"
      },
      "source": [
        "df_api_request.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R69yc9BaHdc"
      },
      "source": [
        "### Handle DataFrame for CLIENT MOBILE LOGIN Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj661cCus7YS"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_client_mobile_login(df_client_mob):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_client_mobile = []\n",
        "  list_column_none_client_mobile = [message_sms_payload_col, totalsent_col, cost_col, status_col,\n",
        "                                  account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_client_mob.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)        \n",
        "        # check if the row contains an email address \n",
        "        # pour tous les types request créer un dictionnaire dans lequel mapper\n",
        "        # key = type de request et value = les regex définis\n",
        "        # pour chaque condition IF créer une liste de colonnes auxquelles affecter None\n",
        "        if 'mailto' in str_text:\n",
        "            if re.search('CLIENT MOBILE LOGIN', str_text):   # CLIENT MOBILE LOGIN with email address\n",
        "                  type_of_request = re.search('CLIENT MOBILE LOGIN', str_text)\n",
        "                  # extract address email for CLIENT MOBILE LOGIN\n",
        "                  phone_or_email_client_mobile = re.search(type_request_dictionnary['CLIENT MOBILE LOGIN'][0], str_text)                               \n",
        "                  pattern = type_request_dictionnary['CLIENT MOBILE LOGIN'][1]\n",
        "                  datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                  datematcher = datepattern.search(str_text)  # extract date for CLIENT MOBILE LOGIN type request\n",
        "                  \n",
        "                  for j in range(len(list_column_none_client_mobile)):\n",
        "                    list_column_none_client_mobile[j].append(None)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0)) # add type request inside type request column\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None) \n",
        "                  try:\n",
        "                    email_col.append(phone_or_email_client_mobile.group(0)) # add email inside email column\n",
        "                    phone_Col.append(None)  # in this case there is no phone number\n",
        "                  except AttributeError:\n",
        "                    email_col.append(None)\n",
        "                  try:\n",
        "                    date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                  except AttributeError:\n",
        "                    date_col.append(None)\n",
        "\n",
        "        elif 'mailto' not in str_text:\n",
        "            if re.search('CLIENT MOBILE LOGIN', str_text): # when type request is CLIENT MOBILE LOGIN, there is no EndPoint\n",
        "                type_of_request = re.search('CLIENT MOBILE LOGIN', str_text)\n",
        "                # extract a phone number for CLIENT MOBILE LOGIN\n",
        "                phone_or_email_client_mobile = re.search(type_request_dictionnary['CLIENT MOBILE LOGIN'][2], str_text)                  \n",
        "                pattern = type_request_dictionnary['CLIENT MOBILE LOGIN'][1]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for j in range(len(list_column_none_client_mobile)):\n",
        "                    list_column_none_client_mobile[j].append(None)\n",
        "\n",
        "                try:\n",
        "                  phone_Col.append(phone_or_email_client_mobile.group(0))\n",
        "                  email_col.append(None)\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None) \n",
        "\n",
        "  df_client_mob['Type_Request'] = type_request_col\n",
        "  df_client_mob['Phone_Number'] = phone_Col\n",
        "  df_client_mob['Date'] = date_col\n",
        "  df_client_mob['EndPoint'] = endpoint_Col\n",
        "  df_client_mob['Log_Level'] = log_level_col\n",
        "  df_client_mob['Email'] = email_col\n",
        "  df_client_mob['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_client_mob['Total Sent'] = totalsent_col\n",
        "  df_client_mob['Cost'] = cost_col\n",
        "  df_client_mob['Status'] = status_col\n",
        "  df_client_mob['Account Number'] = account_number_col\n",
        "  df_client_mob['Account Name'] = account_name_col\n",
        "  df_client_mob['BVN'] = bvn_col\n",
        "  df_client_mob['Request Successful'] = requestSuccessful_col\n",
        "  df_client_mob['Response Message'] = responseMessage_col\n",
        "  df_client_mob['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_client_mob"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnqWyhXp3U-5"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "client_mobile_login = df_raw[df_raw['text'].str.contains('CLIENT MOBILE LOGIN')]\n",
        "df_client_mobile_login = parse_row_client_mobile_login(client_mobile_login)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSzleZV-3t3Z"
      },
      "source": [
        "df_client_mobile_login.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpBB-0cg3hAG"
      },
      "source": [
        "Handle DataFrame for SMS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XVntz9o3mbD"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_sms_payload_function(df_sms_payload):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  \n",
        "  list_column_none_sms_payload = []\n",
        "  list_column_none_sms_payload = [totalsent_col, cost_col, status_col,\n",
        "                                  account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, email_col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_sms_payload.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('SMS PAYLOAD', str_text):\n",
        "                type_of_request = re.search('SMS PAYLOAD', str_text)            \n",
        "                sms_payload = parse_wallet_sms_payload_success(str_text)               \n",
        "                for l in range(len(list_column_none_sms_payload)):\n",
        "                    list_column_none_sms_payload[l].append(None)             \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  phone_Col.append(sms_payload.get('phone'))\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  message_sms_payload_col.append(sms_payload.get('message'))\n",
        "                except AttributeError:\n",
        "                  message_sms_payload_col.append(None)\n",
        "\n",
        "\n",
        "  df_sms_payload['Type_Request'] = type_request_col\n",
        "  df_sms_payload['Phone_Number'] = phone_Col\n",
        "  df_sms_payload['Date'] = date_col\n",
        "  df_sms_payload['EndPoint'] = endpoint_Col\n",
        "  df_sms_payload['Log_Level'] = log_level_col\n",
        "  df_sms_payload['Email'] = email_col\n",
        "  df_sms_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_sms_payload['Total Sent'] = totalsent_col\n",
        "  df_sms_payload['Cost'] = cost_col\n",
        "  df_sms_payload['Status'] = status_col\n",
        "  df_sms_payload['Account Number'] = account_number_col\n",
        "  df_sms_payload['Account Name'] = account_name_col\n",
        "  df_sms_payload['BVN'] = bvn_col\n",
        "  df_sms_payload['Request Successful'] = requestSuccessful_col\n",
        "  df_sms_payload['Response Message'] = responseMessage_col\n",
        "  df_sms_payload['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_sms_payload\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOM6gkyh3-B0"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "sms_payload = df_raw[df_raw['text'].str.contains('SMS PAYLOAD')]\n",
        "df_sms_payload = parse_row_sms_payload_function(sms_payload)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzrdhU6Kkbat"
      },
      "source": [
        "df_sms_payload.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrJjMbac4HGd"
      },
      "source": [
        "Handle DataFrame for SMS Success Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8J_KpN4H7J"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_sms_success_function(df_sms_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  \n",
        "  list_column_none_sms_success = []\n",
        "  list_column_none_sms_success = [message_sms_payload_col, account_number_col, bvn_col, requestSuccessful_col, \n",
        "                                  responseMessage_col, responseCode_col, account_name_col, email_col, \n",
        "                                  phone_Col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_sms_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('SMS SUCCESS', str_text): \n",
        "                type_of_request = re.search('SMS SUCCESS', str_text)\n",
        "                sms_success = parse_wallet_sms_payload_success(str_text)                \n",
        "                for m in range(len(list_column_none_sms_success)):\n",
        "                    list_column_none_sms_success[m].append(None) \n",
        "           \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:                 \n",
        "                  totalsent_col.append(sms_success.get('response').get('totalsent '))\n",
        "                except AttributeError:\n",
        "                  totalsent_col.append(None)\n",
        "                try:                 \n",
        "                  cost_col.append(sms_success.get('response').get('cost '))\n",
        "                except AttributeError:\n",
        "                  cost_col.append(None)\n",
        "                try:                 \n",
        "                  status_col.append(sms_success.get('response').get('status '))\n",
        "                except AttributeError:\n",
        "                  status_col.append(None)\n",
        "                     \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)    \n",
        "\n",
        "  df_sms_success['Type_Request'] = type_request_col\n",
        "  df_sms_success['Phone_Number'] = phone_Col\n",
        "  df_sms_success['Date'] = date_col\n",
        "  df_sms_success['EndPoint'] = endpoint_Col\n",
        "  df_sms_success['Log_Level'] = log_level_col\n",
        "  df_sms_success['Email'] = email_col\n",
        "  df_sms_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_sms_success['Total Sent'] = totalsent_col\n",
        "  df_sms_success['Cost'] = cost_col\n",
        "  df_sms_success['Status'] = status_col\n",
        "  df_sms_success['Account Number'] = account_number_col\n",
        "  df_sms_success['Account Name'] = account_name_col\n",
        "  df_sms_success['BVN'] = bvn_col\n",
        "  df_sms_success['Request Successful'] = requestSuccessful_col\n",
        "  df_sms_success['Response Message'] = responseMessage_col\n",
        "  df_sms_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_sms_success\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE_Z6cSQ4Ou9"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "sms_success = df_raw[df_raw['text'].str.contains('SMS SUCCESS')]\n",
        "df_sms_success = parse_row_sms_success_function(sms_success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMY6oNKUkoPE"
      },
      "source": [
        "df_sms_success.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgN9dHMi4WhI"
      },
      "source": [
        "Handle DataFrame for WALLET SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqjCjokx4X-5"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_wallet_success_function(df_wallet_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  message_error_col = []\n",
        "\n",
        "  list_column_none_wallet_success = []\n",
        "  list_column_none_wallet_success = [totalsent_col, message_sms_payload_col, cost_col, status_col, \n",
        "                                     email_col, phone_Col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "  \n",
        "  for index, row in df_wallet_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        date_col.append(row['ts'])\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "\n",
        "        if re.search('WALLET SUCCESS', str_text):\n",
        "            type_of_request = re.search('WALLET SUCCESS', str_text)           \n",
        "\n",
        "            if 'Faithfully yours, nginx' in str_text:\n",
        "                error_text = 'Sorry, the page you are looking for is currently unavailable, Please try again later.'\n",
        "                message_error_col.append(error_text)\n",
        "                type_request_col.append(type_of_request.group(0))\n",
        "                account_number_col.append(None)\n",
        "                account_name_col.append(None)\n",
        "                bvn_col.append(None)\n",
        "                requestSuccessful_col.append(None)\n",
        "                responseMessage_col.append(None)\n",
        "                responseCode_col.append(None)\n",
        "            else:\n",
        "                try:\n",
        "                  wallet_success = parse_providus_transfer_error_function(str_text)\n",
        "                except json.decoder.JSONDecodeError:\n",
        "                  #print(str_text)\n",
        "                  raise\n",
        "                message_error_col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  account_number_col.append(wallet_success.get('account_number'))\n",
        "                except AttributeError:\n",
        "                  account_number_col.append(None)\n",
        "                try:\n",
        "                  account_name_col.append(wallet_success.get('account_name'))\n",
        "                except AttributeError:\n",
        "                  account_name_col.append(None)\n",
        "                try:\n",
        "                  bvn_col.append(wallet_success.get('bvn'))\n",
        "                except AttributeError:\n",
        "                  bvn_col.append(None)\n",
        "                try:\n",
        "                  requestSuccessful_col.append(wallet_success.get('requestSuccessful'))\n",
        "                except AttributeError:\n",
        "                  requestSuccessful_col.append(None)\n",
        "                try:\n",
        "                  responseMessage_col.append((wallet_success.get('responseMessage')))\n",
        "                except AttributeError:\n",
        "                  responseMessage_col.append(None)\n",
        "                try:\n",
        "                  responseCode_col.append((wallet_success.get('responseCode')))\n",
        "                except AttributeError:\n",
        "                  responseCode_col.append(None)\n",
        "\n",
        "            for n in range(len(list_column_none_wallet_success)):\n",
        "              list_column_none_wallet_success[n].append(None) \n",
        "\n",
        "                       \n",
        "\n",
        "  df_wallet_success['Type_Request'] = type_request_col\n",
        "  df_wallet_success['Phone_Number'] = phone_Col\n",
        "  df_wallet_success['Date'] = date_col\n",
        "  df_wallet_success['EndPoint'] = endpoint_Col\n",
        "  df_wallet_success['Log_Level'] = log_level_col\n",
        "  df_wallet_success['Email'] = email_col\n",
        "  df_wallet_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_wallet_success['Total Sent'] = totalsent_col\n",
        "  df_wallet_success['Cost'] = cost_col\n",
        "  df_wallet_success['Status'] = status_col\n",
        "  df_wallet_success['Account Number'] = account_number_col\n",
        "  df_wallet_success['Account Name'] = account_name_col\n",
        "  df_wallet_success['BVN'] = bvn_col\n",
        "  df_wallet_success['Request Successful'] = requestSuccessful_col\n",
        "  df_wallet_success['Response Message'] = responseMessage_col\n",
        "  df_wallet_success['Response Code'] = responseCode_col\n",
        "  df_wallet_success['Error Message Wallet'] = message_error_col\n",
        " \n",
        "  return df_wallet_success"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnoeVazJ4fZE"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "wallet_success = df_raw[df_raw['text'].str.contains('WALLET SUCCESS')]\n",
        "df_wallet_success = parse_row_wallet_success_function(wallet_success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5NWvVoga5r-"
      },
      "source": [
        "df_wallet_success.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYItsxk8Ion"
      },
      "source": [
        "### Handle DataFrame for OKRA WEBHOOK Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ2J1GQ88N4b"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_okra_webhook_function(df_okra):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  accountId_col = []\n",
        "  authorization_v_col = []\n",
        "  authorization_id_col = []\n",
        "  authorization_customer_col = []\n",
        "  authorization_account_col = []\n",
        "  authorization_account_id_col = []\n",
        "  authorization_account_manual_col = []\n",
        "  authorization_account_name_col = []\n",
        "  authorization_account_nuban_col = []\n",
        "  authorization_account_bank_col = []\n",
        "  authorization_account_created_at_col = []\n",
        "  authorization_account_last_updated_col = []\n",
        "  authorization_account_balance_col = []\n",
        "  authorization_account_customer_col = []\n",
        "  authorization_account_type_col = []\n",
        "  authorization_account_currency_col = []\n",
        "  authorization_accounts_col = []\n",
        "  authorization_amount_col = []\n",
        "  authorization_bank_col = []\n",
        "  authorization_created_at_col = []\n",
        "  authorization_currency_col = []\n",
        "  authorization_customerDetails_col = []\n",
        "  authorization_disconnect_col = []\n",
        "  authorization_disconnected_at_col = []\n",
        "  authorization_duration_col = []\n",
        "  authorization_env_col = []\n",
        "  authorization_garnish_col = []\n",
        "  authorization_initialAmount_col = []\n",
        "  authorization_initiated_col = []\n",
        "  authorization_last_updated_col = []\n",
        "  authorization_link_col = []\n",
        "  authorization_next_payment_col = []\n",
        "  authorization_owner_col = []\n",
        "  authorization_payLink_col = []\n",
        "  authorization_type_col = []\n",
        "  authorization_used_col = []\n",
        "  authorizationId_col = []\n",
        "  bankId_col = []\n",
        "  bankName_col = []\n",
        "  bankSlug_col = []\n",
        "  bankType_col = []\n",
        "  callbackURL_col = []\n",
        "  callback_code_col = []\n",
        "  callback_type_col = []\n",
        "  callback_url_col = []\n",
        "  code_col = []\n",
        "  country_col = []\n",
        "  current_project_col = []\n",
        "  customerEmail_col = []\n",
        "  customerId_col = []\n",
        "  ended_at_col = []\n",
        "  env_col = []\n",
        "  extras_col = []\n",
        "  identityType_col = []\n",
        "  login_type_col = []\n",
        "  message_col = []\n",
        "  meta_col = []\n",
        "  method_col = []\n",
        "  options_col = []\n",
        "  owner_col = []\n",
        "  record_col = []\n",
        "  recordId_col = []\n",
        "  started_at_col = []\n",
        "  status_webhook_col = []\n",
        "  token_col = []\n",
        "  type_col = []\n",
        "\n",
        "  list_column_none_okra_webhook = []\n",
        "  list_column_none_okra_webhook = [api_request_col, account_number_col, account_name_col, totalsent_col, \n",
        "                                   message_sms_payload_col, cost_col, status_col, responseCode_col,\n",
        "                                   bvn_col, requestSuccessful_col, responseMessage_col, email_col, phone_Col, \n",
        "                                   endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_okra.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        date_col.append(row['ts'])\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('OKRA WEBHOOK', str_text):\n",
        "                  type_of_request = re.search('OKRA WEBHOOK', str_text)\n",
        "                  okra_webhook = parse_wallet_sms_payload_success(str_text) \n",
        "                  if 'authorization' in str_text:\n",
        "                      accountId_col.append(okra_webhook.get('accountId'))\n",
        "                      authorization_v_col.append(okra_webhook.get('authorization').get('__v '))\n",
        "                      authorization_id_col.append(okra_webhook.get('authorization').get('_id '))\n",
        "                      authorization_customer_col.append(okra_webhook.get('authorization').get('customer '))\n",
        "                      authorization_account_col.append(okra_webhook.get('authorization').get('account '))\n",
        "                      authorization_account_id_col.append(okra_webhook.get('authorization').get('account ')[0].get('_id '))\n",
        "                      authorization_account_manual_col.append(okra_webhook.get('authorization').get('account ')[0].get('manual '))\n",
        "                      authorization_account_name_col.append(okra_webhook.get('authorization').get('account ')[0].get('name '))\n",
        "                      authorization_account_nuban_col.append(okra_webhook.get('authorization').get('account ')[0].get('nuban '))\n",
        "                      authorization_account_bank_col.append(okra_webhook.get('authorization').get('account ')[0].get('bank '))\n",
        "                      authorization_account_created_at_col.append(okra_webhook.get('authorization').get('account ')[0].get('created_at '))\n",
        "                      authorization_account_last_updated_col.append(okra_webhook.get('authorization').get('account ')[0].get('last_updated '))\n",
        "                      authorization_account_balance_col.append(okra_webhook.get('authorization').get('account ')[0].get('balance '))\n",
        "                      authorization_account_customer_col.append(okra_webhook.get('authorization').get('account ')[0].get('customer '))\n",
        "                      authorization_account_type_col.append(okra_webhook.get('authorization').get('account ')[0].get('type '))\n",
        "                      authorization_account_currency_col.append(okra_webhook.get('authorization').get('account ')[0].get('currency '))\n",
        "                      authorization_accounts_col.append(okra_webhook.get('authorization').get('accounts '))\n",
        "                      authorization_amount_col.append(okra_webhook.get('authorization').get('amount '))\n",
        "                      authorization_bank_col.append(okra_webhook.get('authorization').get('bank '))\n",
        "                      authorization_created_at_col.append(okra_webhook.get('authorization').get('created_at '))\n",
        "                      authorization_currency_col.append(okra_webhook.get('authorization').get('currency '))\n",
        "                      authorization_customerDetails_col.append(okra_webhook.get('authorization').get('customerDetails '))\n",
        "                      authorization_disconnect_col.append(okra_webhook.get('authorization').get('disconnect '))\n",
        "                      authorization_disconnected_at_col.append(okra_webhook.get('authorization').get('disconnected_at '))\n",
        "                      authorization_duration_col.append(okra_webhook.get('authorization').get('duration '))\n",
        "                      authorization_env_col.append(okra_webhook.get('authorization').get('env '))\n",
        "                      authorization_garnish_col.append(okra_webhook.get('authorization').get('garnish '))\n",
        "                      authorization_initialAmount_col.append(okra_webhook.get('authorization').get('initialAmount '))\n",
        "                      authorization_initiated_col.append(okra_webhook.get('authorization').get('initiated '))\n",
        "                      authorization_last_updated_col.append(okra_webhook.get('authorization').get('last_updated '))\n",
        "                      authorization_link_col.append(okra_webhook.get('authorization').get('link '))\n",
        "                      authorization_next_payment_col.append(okra_webhook.get('authorization').get('next_payment '))\n",
        "                      authorization_owner_col.append(okra_webhook.get('authorization').get('owner '))\n",
        "                      authorization_payLink_col.append(okra_webhook.get('authorization').get('payLink '))\n",
        "                      authorization_type_col.append(okra_webhook.get('authorization').get('type '))\n",
        "                      authorization_used_col.append(okra_webhook.get('authorization').get('used '))\n",
        "                      authorizationId_col.append(None)\n",
        "                      bankId_col.append(None)\n",
        "                      bankName_col.append(None)\n",
        "                      bankSlug_col.append(None)\n",
        "                      bankType_col.append(None)\n",
        "                      callbackURL_col.append(None)\n",
        "                      callback_code_col.append(None)\n",
        "                      callback_type_col.append(None)\n",
        "                      callback_url_col.append(None)\n",
        "                      code_col.append(None)\n",
        "                      country_col.append(None)\n",
        "                      current_project_col.append(None)\n",
        "                      customerEmail_col.append(None)\n",
        "                      customerId_col.append(None)\n",
        "                      ended_at_col.append(None)\n",
        "                      env_col.append(None)\n",
        "                      extras_col.append(None)\n",
        "                      identityType_col.append(None)\n",
        "                      login_type_col.append(None)\n",
        "                      message_col.append(None)\n",
        "                      meta_col.append(None)\n",
        "                      method_col.append(None)\n",
        "                      options_col.append(None)\n",
        "                      owner_col.append(None)\n",
        "                      record_col.append(None)\n",
        "                      recordId_col.append(None)\n",
        "                      started_at_col.append(None)\n",
        "                      status_webhook_col.append(None)\n",
        "                      token_col.append(None)\n",
        "                      type_col.append(None)\n",
        "                      try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                      except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                  else:\n",
        "                      authorizationId_col.append(okra_webhook.get('authorizationId'))\n",
        "                      bankId_col.append(okra_webhook.get('bankId'))\n",
        "                      bankName_col.append(okra_webhook.get('bankName'))\n",
        "                      bankSlug_col.append(okra_webhook.get('bankSlug'))\n",
        "                      bankType_col.append(okra_webhook.get('bankType'))\n",
        "                      callbackURL_col.append(okra_webhook.get('callbackURL'))\n",
        "                      callback_code_col.append(okra_webhook.get('callback_code'))\n",
        "                      callback_type_col.append(okra_webhook.get('callback_type'))\n",
        "                      callback_url_col.append(okra_webhook.get('callback_url'))\n",
        "                      code_col.append(okra_webhook.get('code'))\n",
        "                      country_col.append(okra_webhook.get('country'))\n",
        "                      current_project_col.append(okra_webhook.get('current_project'))\n",
        "                      customerEmail_col.append(okra_webhook.get('customerEmail'))\n",
        "                      customerId_col.append(okra_webhook.get('customerId'))\n",
        "                      ended_at_col.append(okra_webhook.get('ended_at'))\n",
        "                      env_col.append(okra_webhook.get('env'))\n",
        "                      extras_col.append(okra_webhook.get('extras'))\n",
        "                      identityType_col.append(okra_webhook.get('identityType'))\n",
        "                      login_type_col.append(okra_webhook.get('login_type'))\n",
        "                      message_col.append(okra_webhook.get('message'))\n",
        "                      meta_col.append(okra_webhook.get('meta'))\n",
        "                      method_col.append(okra_webhook.get('method'))\n",
        "                      options_col.append(okra_webhook.get('options'))\n",
        "                      owner_col.append(okra_webhook.get('owner'))\n",
        "                      record_col.append(okra_webhook.get('record'))\n",
        "                      recordId_col.append(okra_webhook.get('recordId'))\n",
        "                      started_at_col.append(okra_webhook.get('started_at'))\n",
        "                      status_webhook_col.append(okra_webhook.get('status'))\n",
        "                      token_col.append(okra_webhook.get('token'))\n",
        "                      type_col.append(okra_webhook.get('type'))\n",
        "                      accountId_col.append(None)\n",
        "                      authorization_v_col.append(None)\n",
        "                      authorization_id_col.append(None)\n",
        "                      authorization_customer_col.append(None)\n",
        "                      authorization_account_col.append(None)\n",
        "                      authorization_account_id_col.append(None)\n",
        "                      authorization_account_manual_col.append(None)\n",
        "                      authorization_account_name_col.append(None)\n",
        "                      authorization_account_nuban_col.append(None)\n",
        "                      authorization_account_bank_col.append(None)\n",
        "                      authorization_account_created_at_col.append(None)\n",
        "                      authorization_account_last_updated_col.append(None)\n",
        "                      authorization_account_balance_col.append(None)\n",
        "                      authorization_account_customer_col.append(None)\n",
        "                      authorization_account_type_col.append(None)\n",
        "                      authorization_account_currency_col.append(None)\n",
        "                      authorization_accounts_col.append(None)\n",
        "                      authorization_amount_col.append(None)\n",
        "                      authorization_bank_col.append(None)\n",
        "                      authorization_created_at_col.append(None)\n",
        "                      authorization_currency_col.append(None)\n",
        "                      authorization_customerDetails_col.append(None)\n",
        "                      authorization_disconnect_col.append(None)\n",
        "                      authorization_disconnected_at_col.append(None)\n",
        "                      authorization_duration_col.append(None)\n",
        "                      authorization_env_col.append(None)\n",
        "                      authorization_garnish_col.append(None)\n",
        "                      authorization_initialAmount_col.append(None)\n",
        "                      authorization_initiated_col.append(None)\n",
        "                      authorization_last_updated_col.append(None)\n",
        "                      authorization_link_col.append(None)\n",
        "                      authorization_next_payment_col.append(None)\n",
        "                      authorization_owner_col.append(None)\n",
        "                      authorization_payLink_col.append(None)\n",
        "                      authorization_type_col.append(None)\n",
        "                      authorization_used_col.append(None)\n",
        "                      try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                      except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                  \n",
        "                  for n in range(len(list_column_none_okra_webhook)):\n",
        "                    list_column_none_okra_webhook[n].append(None) \n",
        "\n",
        "       \n",
        "  df_okra['Type_Request'] = type_request_col\n",
        "  df_okra['Phone_Number'] = phone_Col\n",
        "  df_okra['Date'] = date_col\n",
        "  df_okra['EndPoint'] = endpoint_Col\n",
        "  df_okra['Log_Level'] = log_level_col\n",
        "  df_okra['Email'] = email_col\n",
        "  df_okra['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_okra['Total Sent'] = totalsent_col\n",
        "  df_okra['Cost'] = cost_col\n",
        "  df_okra['Status'] = status_col\n",
        "  df_okra['Account Number'] = account_number_col\n",
        "  df_okra['Account Name'] = account_name_col\n",
        "  df_okra['BVN'] = bvn_col\n",
        "  df_okra['Request Successful'] = requestSuccessful_col\n",
        "  df_okra['Response Message'] = responseMessage_col\n",
        "  df_okra['Response Code'] = responseCode_col\n",
        "  df_okra['Account Id'] = accountId_col\n",
        "  df_okra['Authorization_V'] = authorization_v_col\n",
        "  df_okra['Authorization_Id'] = authorization_id_col\n",
        "  df_okra['Authorization_Customer'] = authorization_customer_col\n",
        "  df_okra['Authorization_Owner'] = authorization_owner_col\n",
        "  df_okra['Authorization_Account'] = authorization_account_col\n",
        "  df_okra['Authorization_account_Id'] = authorization_account_id_col\n",
        "  df_okra['Authorization_account_manual'] = authorization_account_manual_col\n",
        "  df_okra['Authorization_account_name'] = authorization_account_name_col\n",
        "  df_okra['Authorization_account_nuban'] = authorization_account_nuban_col\n",
        "  df_okra['Authorization_account_bank'] = authorization_account_bank_col\n",
        "  df_okra['Authorization_account_created_at'] = authorization_account_created_at_col\n",
        "  df_okra['Authorization_account_last_updated'] = authorization_account_last_updated_col\n",
        "  df_okra['Authorization_account_balance'] = authorization_account_balance_col\n",
        "  df_okra['Authorization_account_customer'] = authorization_account_customer_col\n",
        "  df_okra['Authorization_account_type'] = authorization_account_type_col\n",
        "  df_okra['Authorization_account_currency'] = authorization_account_currency_col\n",
        "  df_okra['Authorization_accounts'] = authorization_accounts_col\n",
        "  df_okra['Authorization_amount'] = authorization_amount_col\n",
        "  df_okra['Authorization_bank'] = authorization_bank_col\n",
        "  df_okra['Authorization_created_at'] = authorization_created_at_col\n",
        "  df_okra['Authorization_currency'] = authorization_currency_col \n",
        "  df_okra['Authorization_customerDetails'] = authorization_customerDetails_col\n",
        "  df_okra['Authorization_disconnect'] = authorization_disconnect_col\n",
        "  df_okra['Authorization_disconnected_at'] = authorization_disconnected_at_col\n",
        "  df_okra['Authorization_duration'] = authorization_duration_col\n",
        "  df_okra['Authorization_env'] = authorization_env_col\n",
        "  df_okra['Authorization_garnish'] = authorization_garnish_col\n",
        "  df_okra['Authorization_initialAmount'] = authorization_initialAmount_col\n",
        "  df_okra['Authorization_initiated'] = authorization_initiated_col\n",
        "  df_okra['Authorization_last_updated'] = authorization_last_updated_col\n",
        "  df_okra['Authorization_link'] = authorization_link_col\n",
        "  df_okra['Authorization_next_payment'] = authorization_next_payment_col\n",
        "  df_okra['Authorization_payLink'] = authorization_payLink_col\n",
        "  df_okra['Authorization_type'] = authorization_type_col\n",
        "  df_okra['Authorization_used'] = authorization_used_col\n",
        "  df_okra['AuthorizationId'] = authorizationId_col\n",
        "  df_okra['BankId'] = bankId_col\n",
        "  df_okra['BankName'] = bankName_col\n",
        "  df_okra['bankSlug'] = bankSlug_col\n",
        "  df_okra['bankType'] = bankType_col\n",
        "  df_okra['callbackURL'] = callbackURL_col\n",
        "  df_okra['callback_code'] = callback_code_col\n",
        "  df_okra['callback_type'] = callback_type_col \n",
        "  df_okra['callback_url'] = callback_url_col\n",
        "  df_okra['code'] = code_col\n",
        "  df_okra['country'] = country_col\n",
        "  df_okra['current_project'] = current_project_col\n",
        "  df_okra['customerEmail'] = customerEmail_col\n",
        "  df_okra['customerId'] = customerId_col\n",
        "  df_okra['ended_at'] = ended_at_col\n",
        "  df_okra['env'] = env_col\n",
        "  df_okra['extras'] = extras_col\n",
        "  df_okra['identityType'] = identityType_col\n",
        "  df_okra['login_type'] = login_type_col\n",
        "  df_okra['message'] = message_col\n",
        "  df_okra['meta'] = meta_col\n",
        "  df_okra['method'] = method_col\n",
        "  df_okra['options'] = options_col\n",
        "  df_okra['owner'] = owner_col\n",
        "  df_okra['record'] = record_col \n",
        "  df_okra['recordId'] = recordId_col\n",
        "  df_okra['started_at'] = started_at_col\n",
        "  df_okra['status_webhook'] = status_webhook_col\n",
        "  df_okra['token'] = token_col\n",
        "  df_okra['type'] = type_col\n",
        " \n",
        "  return df_okra\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uiWq1Np8Rxx"
      },
      "source": [
        "df_raw['text'].fillna('', inplace=True)\n",
        "df_okra_webhook = parse_row_okra_webhook_function(df_raw[df_raw['text'].str.contains('OKRA WEBHOOK')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBybNxmhkfV"
      },
      "source": [
        "df_okra_webhook.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDihv11g8lnJ"
      },
      "source": [
        "Handle DataFrame for LEADWAY SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s46Md54R8phm"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_leadway_function(df_leadway_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_leadway_success = []\n",
        "  list_column_none_leadway_success = [message_sms_payload_col, totalsent_col, cost_col, status_col, \n",
        "                                     account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                     responseCode_col, account_name_col, email_col, phone_Col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_leadway_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        date_col.append(row['ts'])\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('LEADWAY SUCCESS', str_text):\n",
        "                  type_of_request = re.search('LEADWAY SUCCESS', str_text)\n",
        "                  leadway_success_concat_text, index_first_succ, index_last_succ = parse_and_concatenate_Leadway_Success_Rows(df_)\n",
        "                  res_text_leadway = parse_Leadway_Success_Row(leadway_success_concat_text)\n",
        "                  for o in range(len(list_column_none_leadway_success)):\n",
        "                    list_column_none_leadway_success[o].append(None)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "                   \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)  \n",
        "\n",
        "  df_leadway_success['Type_Request'] = type_request_col\n",
        "  df_leadway_success['Phone_Number'] = phone_Col\n",
        "  df_leadway_success['Date'] = date_col\n",
        "  df_leadway_success['EndPoint'] = endpoint_Col\n",
        "  df_leadway_success['Log_Level'] = log_level_col\n",
        "  df_leadway_success['Email'] = email_col\n",
        "  df_leadway_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_leadway_success['Total Sent'] = totalsent_col\n",
        "  df_leadway_success['Cost'] = cost_col\n",
        "  df_leadway_success['Status'] = status_col\n",
        "  df_leadway_success['Account Number'] = account_number_col\n",
        "  df_leadway_success['Account Name'] = account_name_col\n",
        "  df_leadway_success['BVN'] = bvn_col\n",
        "  df_leadway_success['Request Successful'] = requestSuccessful_col\n",
        "  df_leadway_success['Response Message'] = responseMessage_col\n",
        "  df_leadway_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_leadway_success\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUIQgzNCcraR"
      },
      "source": [
        "Handle DataFrame for PROVIDUS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSI2WLjFYyn"
      },
      "source": [
        "def parse_PROVIDUS_PAYLOAD_DF(df_providus_payload):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS PAYLOAD\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    beneficiary_account_name_col = []\n",
        "    beneficiary_account_number_col = []\n",
        "    beneficiary_bank_col = []\n",
        "    currency_code_col = []\n",
        "    narration_col = []\n",
        "    source_account_name_col = []\n",
        "    transaction_amount_col = []\n",
        "    transaction_reference_col = []\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "\n",
        "    list_column_none_providus_payload = []\n",
        "    list_column_none_providus_payload = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseMessage_col,\n",
        "                                        responseCode_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    beneficiary_account_name_col, beneficiary_account_number_col, beneficiary_bank_col,\n",
        "                    currency_code_col, narration_col, source_account_name_col, transaction_amount_col,\n",
        "                    transaction_reference_col]\n",
        "\n",
        "    for index, row in df_providus_payload.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS PAYLOAD', str_text):               \n",
        "                    providus_payload = parse_wallet_sms_payload_success(str_text)\n",
        "                    type_of_request = re.search('PROVIDUS PAYLOAD', str_text)\n",
        "                       \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        beneficiary_account_name_col.append(providus_payload.get('beneficiaryAccountName'))\n",
        "                    except AttributeError:\n",
        "                        beneficiary_account_name_col.append(None)\n",
        "                    try:\n",
        "                        beneficiary_account_number_col.append(providus_payload.get('beneficiaryAccountNumber'))\n",
        "                    except AttributeError:\n",
        "                        beneficiary_account_number_col.append(None)\n",
        "                    try:\n",
        "                        beneficiary_bank_col.append(providus_payload.get('beneficiaryBank'))\n",
        "                    except AttributeError:\n",
        "                        beneficiary_bank_col.append(None)\n",
        "                    try:\n",
        "                        currency_code_col.append(providus_payload.get('currencyCode'))\n",
        "                    except AttributeError:\n",
        "                        currency_code_col.append(None)\n",
        "                    try:\n",
        "                        narration_col.append((providus_payload.get('narration')))\n",
        "                    except AttributeError:\n",
        "                        narration_col.append(None)\n",
        "                    try:\n",
        "                        source_account_name_col.append((providus_payload.get('sourceAccountName')))\n",
        "                    except AttributeError:\n",
        "                        source_account_name_col.append(None)\n",
        "                    try:\n",
        "                        transaction_amount_col.append((providus_payload.get('transactionAmount')))\n",
        "                    except AttributeError:\n",
        "                        transaction_amount_col.append(None)\n",
        "                    try:\n",
        "                        transaction_reference_col.append((providus_payload.get('transactionReference')))\n",
        "                    except AttributeError:\n",
        "                        transaction_reference_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_payload)):\n",
        "                        list_column_none_providus_payload[n].append(None) \n",
        "              \n",
        "            elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "                type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "\n",
        "            elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "\n",
        "            elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('VTPASS SUCCESS', str_text)\n",
        "\n",
        "   \n",
        "    df_providus_payload['Beneficiary_Account_Name'] = beneficiary_account_name_col\n",
        "    df_providus_payload['Beneficiary_Account_Number'] = beneficiary_account_number_col\n",
        "    df_providus_payload['Beneficiary_Bank'] = beneficiary_bank_col\n",
        "    df_providus_payload['Currency_Code'] = currency_code_col\n",
        "    df_providus_payload['Narration'] = narration_col\n",
        "    df_providus_payload['Source_Account_Name'] = source_account_name_col\n",
        "    df_providus_payload['Transaction_Amount'] = transaction_amount_col\n",
        "    df_providus_payload['Transaction_Reference'] = transaction_reference_col\n",
        "    df_providus_payload['Type_Request'] = type_request_col\n",
        "    df_providus_payload['Phone_Number'] = phone_col\n",
        "    df_providus_payload['Date'] = date_col\n",
        "    df_providus_payload['EndPoint'] = endpoint_col\n",
        "    df_providus_payload['Log_Level'] = log_level_col\n",
        "    df_providus_payload['Email'] = email_col\n",
        "    df_providus_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_payload['Total Sent'] = totalsent_col\n",
        "    df_providus_payload['Cost'] = cost_col\n",
        "    df_providus_payload['Status'] = status_col\n",
        "    df_providus_payload['Account Number'] = account_number_col\n",
        "    df_providus_payload['Account Name'] = account_name_col\n",
        "    df_providus_payload['BVN'] = bvn_col\n",
        "    df_providus_payload['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_payload['Response Message'] = responseMessage_col\n",
        "    df_providus_payload['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_payload\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZo3IJz2Fc4S"
      },
      "source": [
        "providus_payload_df = df_raw[df_raw['text'].str.contains('PROVIDUS PAYLOAD')]\n",
        "df_providus_payload = parse_PROVIDUS_PAYLOAD_DF(providus_payload_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1vjwaKWiDYF"
      },
      "source": [
        "df_providus_payload.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4vA4j3kdCeX"
      },
      "source": [
        "Handle DataFrame for PROVIDUS SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEg3vA6Fc7Ym"
      },
      "source": [
        "def parse_PROVIDUS_SUCCESS_DF(df_providus_success):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS SUCCESS\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    transaction_reference_col = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, transaction_reference_col]\n",
        "\n",
        "    for index, row in df_providus_success.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS SUCCESS', str_text):\n",
        "                    providus_success = parse_wallet_sms_payload_success(str_text)\n",
        "                    type_of_request = re.search('PROVIDUS SUCCESS', str_text)\n",
        "                       \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        responseCode_col.append(providus_success.get('responseCode'))\n",
        "                    except AttributeError:\n",
        "                        responseCode_col.append(None)\n",
        "                    try:\n",
        "                        responseMessage_col.append(providus_success.get('responseMessage'))\n",
        "                    except AttributeError:\n",
        "                        responseMessage_col.append(None)\n",
        "                    try:\n",
        "                        sessionId_col.append(providus_success.get('sessionId'))\n",
        "                    except AttributeError:\n",
        "                        sessionId_col.append(None)\n",
        "                    try:\n",
        "                        transaction_reference_col.append(providus_success.get('transactionReference'))\n",
        "                    except AttributeError:\n",
        "                        transaction_reference_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None) \n",
        "              \n",
        "            elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "                type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "\n",
        "            elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "\n",
        "            elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "                type_of_request = re.search('VTPASS SUCCESS', str_text)\n",
        "\n",
        "    \n",
        "    df_providus_success['SessionId'] = sessionId_col\n",
        "    df_providus_success['Transaction_Reference'] = transaction_reference_col\n",
        "    df_providus_success['Type_Request'] = type_request_col\n",
        "    df_providus_success['Phone_Number'] = phone_col\n",
        "    df_providus_success['Date'] = date_col\n",
        "    df_providus_success['EndPoint'] = endpoint_col\n",
        "    df_providus_success['Log_Level'] = log_level_col\n",
        "    df_providus_success['Email'] = email_col\n",
        "    df_providus_success['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_success['Total Sent'] = totalsent_col\n",
        "    df_providus_success['Cost'] = cost_col\n",
        "    df_providus_success['Status'] = status_col\n",
        "    df_providus_success['Account Number'] = account_number_col\n",
        "    df_providus_success['Account Name'] = account_name_col\n",
        "    df_providus_success['BVN'] = bvn_col\n",
        "    df_providus_success['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_success['Response Message'] = responseMessage_col\n",
        "    df_providus_success['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_success\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWCAd27TdGCe"
      },
      "source": [
        "providus_success_df = df_raw[df_raw['text'].str.contains('PROVIDUS SUCCESS')]\n",
        "df_providus_success = parse_PROVIDUS_SUCCESS_DF(providus_success_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqmXhbXFiW29"
      },
      "source": [
        "df_providus_success.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5AkmLaNdbcS"
      },
      "source": [
        "Handle DataFrame for VTPASS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5XJfcXdb40"
      },
      "source": [
        "def parse_VTPASS_PAYLOAD_DF(df_vtpass_payload):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"VTPASS PAYLOAD\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    amount_col = []\n",
        "    request_id_col = []\n",
        "    serviceID_col = []\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    amount_col, request_id_col, serviceID_col]\n",
        "\n",
        "    for index, row in df_vtpass_payload.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('VTPASS PAYLOAD', str_text):\n",
        "                    providus_success = parse_wallet_sms_payload_success(str_text)\n",
        "                    type_of_request = re.search('VTPASS PAYLOAD', str_text)\n",
        "                       \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        amount_col.append(providus_success.get('amount'))\n",
        "                    except AttributeError:\n",
        "                        amount_col.append(None)\n",
        "                    try:\n",
        "                        phone_col.append(providus_success.get('phone'))\n",
        "                    except AttributeError:\n",
        "                        phone_col.append(None)\n",
        "                    try:\n",
        "                        request_id_col.append(providus_success.get('request_id'))\n",
        "                    except AttributeError:\n",
        "                        request_id_col.append(None)\n",
        "                    try:\n",
        "                        serviceID_col.append(providus_success.get('serviceID'))\n",
        "                    except AttributeError:\n",
        "                        serviceID_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None) \n",
        "              \n",
        "\n",
        "\n",
        "    df_vtpass_payload['Service_ID'] = serviceID_col\n",
        "    df_vtpass_payload['Amount'] = amount_col\n",
        "    df_vtpass_payload['Request_Id'] = request_id_col\n",
        "    df_vtpass_payload['Type_Request'] = type_request_col\n",
        "    df_vtpass_payload['Phone_Number'] = phone_col\n",
        "    df_vtpass_payload['Date'] = date_col\n",
        "    df_vtpass_payload['EndPoint'] = endpoint_col\n",
        "    df_vtpass_payload['Log_Level'] = log_level_col\n",
        "    df_vtpass_payload['Email'] = email_col\n",
        "    df_vtpass_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_vtpass_payload['Total Sent'] = totalsent_col\n",
        "    df_vtpass_payload['Cost'] = cost_col\n",
        "    df_vtpass_payload['Status'] = status_col\n",
        "    df_vtpass_payload['Account Number'] = account_number_col\n",
        "    df_vtpass_payload['Account Name'] = account_name_col\n",
        "    df_vtpass_payload['BVN'] = bvn_col\n",
        "    df_vtpass_payload['Request Successful'] = requestSuccessful_col\n",
        "    df_vtpass_payload['Response Message'] = responseMessage_col\n",
        "    df_vtpass_payload['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_vtpass_payload\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPHvlp86deaH"
      },
      "source": [
        "vtpass_payload = df_raw[df_raw['text'].str.contains('VTPASS PAYLOAD')]\n",
        "df_vtpass_payload = parse_VTPASS_PAYLOAD_DF(vtpass_payload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJPBtSi_im1d"
      },
      "source": [
        "df_vtpass_payload.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV2OBj_kxLGp"
      },
      "source": [
        "Handle DataFrame for LEADWAY ERROR Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWThX6Dhd-4G"
      },
      "source": [
        "def parse_LEADWAY_ERROR_DF(df_leadway_error):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"LEADWAY ERROR\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    error_code_col = []\n",
        "    error_number_col = []\n",
        "    port_col = []\n",
        "    syscall_col = []\n",
        "    address_col = []\n",
        "  \n",
        "    #list_column_none_level_log_error = []\n",
        "    # columns set to None\n",
        "    list_column_none = [message_sms_payload_col, totalsent_col, cost_col, status_col, email_col,\n",
        "                        endpoint_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                        responseCode_col, account_name_col, account_number_col, phone_col]\n",
        "    \n",
        "\n",
        "    # columns which will be populated\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    error_code_col, error_number_col, port_col, syscall_col, \n",
        "                    address_col]\n",
        "\n",
        "    for index, row in df_leadway_error.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        if re.search('error', str_text):\n",
        "            log_level = re.search('error', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "                \n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)\n",
        "\n",
        "            if re.search('LEADWAY ERROR', str_text):\n",
        "                type_of_request = re.search('LEADWAY ERROR', str_text)\n",
        "                leadway_error = parse_wallet_sms_payload_success(str_text)\n",
        "                try:\n",
        "                    error_code_col.append(leadway_error.get('code'))\n",
        "                except AttributeError:\n",
        "                    error_code_col.append(None)\n",
        "                try:\n",
        "                    error_number_col.append(leadway_error.get('errno'))\n",
        "                except AttributeError:\n",
        "                    error_number_col.append(None)\n",
        "                try:\n",
        "                    address_col.append(leadway_error.get('address'))\n",
        "                except AttributeError:\n",
        "                    address_col.append(None)\n",
        "                try:\n",
        "                    port_col.append(leadway_error.get('port'))\n",
        "                except AttributeError:\n",
        "                    port_col.append(None)\n",
        "                try:\n",
        "                    syscall_col.append(leadway_error.get('syscall'))\n",
        "                except AttributeError:\n",
        "                    syscall_col.append(None)      \n",
        "                try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "\n",
        "                for p in range(len(list_column_none)):\n",
        "                    list_column_none[p].append(None)\n",
        "\n",
        "    # set columns to their corresponding list values\n",
        "\n",
        "    df_leadway_error['Type_Request'] = type_request_col\n",
        "    df_leadway_error['Phone_Number'] =phone_col\n",
        "    df_leadway_error['Date'] = date_col\n",
        "    df_leadway_error['EndPoint'] = endpoint_col\n",
        "    df_leadway_error['Log_Level'] = log_level_col\n",
        "    df_leadway_error['Email'] = email_col\n",
        "    df_leadway_error['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_leadway_error['Total Sent'] = totalsent_col\n",
        "    df_leadway_error['Cost'] = cost_col\n",
        "    df_leadway_error['Status'] = status_col\n",
        "    df_leadway_error['Account Number'] = account_number_col\n",
        "    df_leadway_error['Account Name'] = account_name_col\n",
        "    df_leadway_error['BVN'] = bvn_col\n",
        "    df_leadway_error['Request Successful'] = requestSuccessful_col\n",
        "    df_leadway_error['Response Message'] = responseMessage_col\n",
        "    df_leadway_error['Response Code'] = responseCode_col\n",
        "    df_leadway_error['Error Code'] = error_code_col\n",
        "    df_leadway_error['Error Number'] = error_number_col\n",
        "    df_leadway_error['Error Port'] = port_col\n",
        "    df_leadway_error['Error Syscall'] = syscall_col\n",
        "    df_leadway_error['Error Address'] = address_col\n",
        "\n",
        "\n",
        "    return df_leadway_error\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viASPHzLeBOc"
      },
      "source": [
        "lead_error = df_raw[df_raw['text'].str.contains('LEADWAY ERROR')]\n",
        "df_lead_error = parse_LEADWAY_ERROR_DF(lead_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b136VEIsiyUv"
      },
      "source": [
        "df_lead_error.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg7u65AZxluN"
      },
      "source": [
        "Handle DataFrame for PROVIDUS TRANSFER ERROR Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsgEs6lNxYET"
      },
      "source": [
        "def parse_PROVIDUS_TRANSFER_ERROR_DF(df_providus_transfer_error):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS TRANSFER ERROR\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    error_code_col = []\n",
        "    error_number_col = []\n",
        "    port_col = []\n",
        "    syscall_col = []\n",
        "    address_col = []\n",
        "  \n",
        "    #list_column_none_level_log_error = []\n",
        "    # columns set to None\n",
        "    list_column_none = [message_sms_payload_col, totalsent_col, cost_col, status_col, email_col,\n",
        "                        endpoint_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                        responseCode_col, account_name_col, account_number_col, phone_col]\n",
        "    \n",
        "\n",
        "    # columns which will be populated\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    error_code_col, error_number_col, port_col, syscall_col, \n",
        "                    address_col]  \n",
        "\n",
        "    for index, row in df_providus_transfer_error.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        if re.search('error', str_text):\n",
        "            log_level = re.search('error', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)\n",
        "\n",
        "            if re.search('PROVIDUS TRANSFER ERROR', str_text):\n",
        "                type_of_request = re.search('PROVIDUS TRANSFER ERROR', str_text)\n",
        "                leadway_error = parse_providus_transfer_error_function(str_text)\n",
        "                try:\n",
        "                    error_code_col.append(leadway_error.get('code'))\n",
        "                except AttributeError:\n",
        "                    error_code_col.append(None)\n",
        "                try:\n",
        "                    error_number_col.append(leadway_error.get('errno'))\n",
        "                except AttributeError:\n",
        "                    error_number_col.append(None)\n",
        "                try:\n",
        "                    address_col.append(leadway_error.get('address'))\n",
        "                except AttributeError:\n",
        "                    address_col.append(None)\n",
        "                try:\n",
        "                    port_col.append(leadway_error.get('port'))\n",
        "                except AttributeError:\n",
        "                    port_col.append(None)\n",
        "                try:\n",
        "                    syscall_col.append(leadway_error.get('syscall'))\n",
        "                except AttributeError:\n",
        "                    syscall_col.append(None)      \n",
        "                try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "\n",
        "                for p in range(len(list_column_none)):\n",
        "                    list_column_none[p].append(None)\n",
        "\n",
        "    # set columns to their corresponding list values\n",
        "\n",
        "    df_providus_transfer_error['Type_Request'] = type_request_col\n",
        "    df_providus_transfer_error['Phone_Number'] =phone_col\n",
        "    df_providus_transfer_error['Date'] = date_col\n",
        "    df_providus_transfer_error['EndPoint'] = endpoint_col\n",
        "    df_providus_transfer_error['Log_Level'] = log_level_col\n",
        "    df_providus_transfer_error['Email'] = email_col\n",
        "    df_providus_transfer_error['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_transfer_error['Total Sent'] = totalsent_col\n",
        "    df_providus_transfer_error['Cost'] = cost_col\n",
        "    df_providus_transfer_error['Status'] = status_col\n",
        "    df_providus_transfer_error['Account Number'] = account_number_col\n",
        "    df_providus_transfer_error['Account Name'] = account_name_col\n",
        "    df_providus_transfer_error['BVN'] = bvn_col\n",
        "    df_providus_transfer_error['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_transfer_error['Response Message'] = responseMessage_col\n",
        "    df_providus_transfer_error['Response Code'] = responseCode_col\n",
        "    df_providus_transfer_error['Error Code'] = error_code_col\n",
        "    df_providus_transfer_error['Error Number'] = error_number_col\n",
        "    df_providus_transfer_error['Error Port'] = port_col\n",
        "    df_providus_transfer_error['Error Syscall'] = syscall_col\n",
        "    df_providus_transfer_error['Error Address'] = address_col\n",
        "\n",
        "\n",
        "    return df_providus_transfer_error\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLJ1mqMJxbCn"
      },
      "source": [
        "providus_transfer_error = df_raw[df_raw['text'].str.contains('PROVIDUS TRANSFER ERROR')]\n",
        "df_providus_transfer_error = parse_PROVIDUS_TRANSFER_ERROR_DF(providus_transfer_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXv7dx4wi7a0"
      },
      "source": [
        "df_providus_transfer_error.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjy32dtmyQJS"
      },
      "source": [
        "Handle DataFrame for PROVIDUS TRANSFER SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmn8hk_ZyGAz"
      },
      "source": [
        "\n",
        "def parse_PROVIDUS_TRANSFER_SUCCESS_DF(df_providus_transfer_success):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS TRANSFER SUCCESS\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    transaction_reference_col = []\n",
        "    message_error_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_number_col, account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col]\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, transaction_reference_col]\n",
        "\n",
        "    for index, row in df_providus_transfer_success.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            date_col.append(row['ts'])\n",
        "            log_level = re.search('info', str_text)\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None) \n",
        "\n",
        "            if re.search('PROVIDUS TRANSFER SUCCESS', str_text):\n",
        "              \n",
        "              if 'Faithfully yours, nginx' in str_text:\n",
        "                  error_text = 'Sorry, the page you are looking for is currently unavailable, Please try again later.'\n",
        "                  message_error_col.append(error_text)\n",
        "                  responseCode_col.append(None)\n",
        "                  responseMessage_col.append(None)\n",
        "                  sessionId_col.append(None)\n",
        "                  transaction_reference_col.append(None)\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "              else:\n",
        "                  message_error_col.append(None)\n",
        "                  try:\n",
        "                    providus_transfer_success = parse_wallet_sms_payload_success(str_text)\n",
        "                  except json.decoder.JSONDecodeError:\n",
        "                    print(str_text)\n",
        "                    raise\n",
        "\n",
        "                  type_of_request = re.search('PROVIDUS TRANSFER SUCCESS', str_text)\n",
        "                      \n",
        "                  try:\n",
        "                      type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                      type_request_col.append(None)\n",
        "                  try:\n",
        "                      responseCode_col.append(providus_transfer_success.get('responseCode'))\n",
        "                  except AttributeError:\n",
        "                      responseCode_col.append(None)\n",
        "                  try:\n",
        "                      responseMessage_col.append(providus_transfer_success.get('responseMessage'))\n",
        "                  except AttributeError:\n",
        "                      responseMessage_col.append(None)\n",
        "                  try:\n",
        "                      sessionId_col.append(providus_transfer_success.get('sessionId'))\n",
        "                  except AttributeError:\n",
        "                      sessionId_col.append(None)\n",
        "                  try:\n",
        "                      transaction_reference_col.append(providus_transfer_success.get('transactionReference'))\n",
        "                  except AttributeError:\n",
        "                      transaction_reference_col.append(None)\n",
        "              for n in range(len(list_column_none_providus_success)):\n",
        "                  list_column_none_providus_success[n].append(None) \n",
        "    \n",
        "    df_providus_transfer_success['SessionId'] = sessionId_col\n",
        "    df_providus_transfer_success['Transaction_Reference'] = transaction_reference_col\n",
        "    df_providus_transfer_success['Type_Request'] = type_request_col\n",
        "    df_providus_transfer_success['Phone_Number'] = phone_col\n",
        "    df_providus_transfer_success['Date'] = date_col\n",
        "    df_providus_transfer_success['EndPoint'] = endpoint_col\n",
        "    df_providus_transfer_success['Log_Level'] = log_level_col\n",
        "    df_providus_transfer_success['Email'] = email_col\n",
        "    df_providus_transfer_success['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_transfer_success['Total Sent'] = totalsent_col\n",
        "    df_providus_transfer_success['Cost'] = cost_col\n",
        "    df_providus_transfer_success['Status'] = status_col\n",
        "    df_providus_transfer_success['Account Number'] = account_number_col\n",
        "    df_providus_transfer_success['Account Name'] = account_name_col\n",
        "    df_providus_transfer_success['BVN'] = bvn_col\n",
        "    df_providus_transfer_success['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_transfer_success['Response Message'] = responseMessage_col\n",
        "    df_providus_transfer_success['Response Code'] = responseCode_col\n",
        "    df_providus_transfer_success['Error Message Providus Transfer'] = message_error_col\n",
        " \n",
        "    return df_providus_transfer_success\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZPumx0gyI9w"
      },
      "source": [
        "providus_transfer_success = df_raw[df_raw['text'].str.contains('PROVIDUS TRANSFER SUCCESS')]\n",
        "df_providus_transfer_success = parse_PROVIDUS_TRANSFER_SUCCESS_DF(providus_transfer_success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqVwdhijKr0"
      },
      "source": [
        "df_providus_transfer_success.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYqmcPFSyvo4"
      },
      "source": [
        "def parse_PROVIDUS_SETTLEMENT_INFO_DF(df_providus_settlement_info):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS SETTLEMENT INFO\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    api_request_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    channelId_col = []\n",
        "    feeAmount_col = []\n",
        "    currency_col = []\n",
        "    initiation_tranRef_col = []\n",
        "    settled_amount_col = []\n",
        "    settlementId_col = []\n",
        "    source_account_name_col = []\n",
        "    source_account_number_col = []\n",
        "    source_bank_name_col = []\n",
        "    tran_date_time_col = []\n",
        "    tran_remarks_col = []\n",
        "    transaction_amount_col = []\n",
        "    vat_amount_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col,\n",
        "                                        account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseCode_col, responseMessage_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, channelId_col, feeAmount_col, currency_col, initiation_tranRef_col,\n",
        "                    settled_amount_col, settlementId_col, source_account_name_col, source_account_number_col,\n",
        "                    source_bank_name_col, tran_date_time_col, tran_remarks_col, transaction_amount_col, vat_amount_col]\n",
        "\n",
        "    for index, row in df_providus_settlement_info.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            log_level = re.search('info', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS SETTLEMENT INFO', str_text):\n",
        "                    providus_settlement_info = parse_wallet_sms_payload_success(str_text)\n",
        "                    #print(providus_settlement_info)\n",
        "                    type_of_request = re.search('PROVIDUS SETTLEMENT INFO', str_text)\n",
        "          \n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        account_number_col.append(providus_settlement_info.get('accountNumber'))\n",
        "                    except AttributeError:\n",
        "                        account_number_col.append(None)\n",
        "                    try:\n",
        "                        channelId_col.append(providus_settlement_info.get('channelId'))\n",
        "                    except AttributeError:\n",
        "                        channelId_col.append(None)\n",
        "                    try:\n",
        "                        sessionId_col.append(providus_settlement_info.get('sessionId'))\n",
        "                    except AttributeError:\n",
        "                        sessionId_col.append(None)\n",
        "                    try:\n",
        "                        currency_col.append(providus_settlement_info.get('currency'))\n",
        "                    except AttributeError:\n",
        "                        currency_col.append(None)\n",
        "                    try:\n",
        "                        feeAmount_col.append(providus_settlement_info.get('feeAmount'))\n",
        "                    except AttributeError:\n",
        "                        feeAmount_col.append(None)\n",
        "                    try:\n",
        "                        initiation_tranRef_col.append(providus_settlement_info.get('initiationTranRef'))\n",
        "                    except AttributeError:\n",
        "                        initiation_tranRef_col.append(None)\n",
        "                    try:\n",
        "                        settled_amount_col.append(providus_settlement_info.get('settledAmount'))\n",
        "                    except AttributeError:\n",
        "                        settled_amount_col.append(None)\n",
        "                    try:\n",
        "                        settlementId_col.append(providus_settlement_info.get('settlementId'))\n",
        "                    except AttributeError:\n",
        "                        settlementId_col.append(None)\n",
        "                    try:\n",
        "                        source_account_name_col.append(providus_settlement_info.get('sourceAccountName'))\n",
        "                    except AttributeError:\n",
        "                        source_account_name_col.append(None)\n",
        "                    try:\n",
        "                        source_account_number_col.append(providus_settlement_info.get('sourceAccountNumber'))\n",
        "                    except AttributeError:\n",
        "                        source_account_number_col.append(None)\n",
        "                    try:\n",
        "                        source_bank_name_col.append(providus_settlement_info.get('sourceBankName'))\n",
        "                    except AttributeError:\n",
        "                        source_bank_name_col.append(None)\n",
        "                    try:\n",
        "                        tran_date_time_col.append(providus_settlement_info.get('tranDateTime'))\n",
        "                    except AttributeError:\n",
        "                        tran_date_time_col.append(None)\n",
        "                    try:\n",
        "                        tran_remarks_col.append(providus_settlement_info.get('tranRemarks'))\n",
        "                    except AttributeError:\n",
        "                        tran_remarks_col.append(None)\n",
        "                    try:\n",
        "                        transaction_amount_col.append(providus_settlement_info.get('transactionAmount'))\n",
        "                    except AttributeError:\n",
        "                        transaction_amount_col.append(None)\n",
        "                    try:\n",
        "                        vat_amount_col.append(providus_settlement_info.get('vatAmount'))\n",
        "                    except AttributeError:\n",
        "                        vat_amount_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None) \n",
        "\n",
        "    \n",
        "    df_providus_settlement_info['Vat_Amount'] = vat_amount_col\n",
        "    df_providus_settlement_info['Transaction_Amount'] = transaction_amount_col\n",
        "    df_providus_settlement_info['Tran_Remarks'] = tran_remarks_col\n",
        "    df_providus_settlement_info['Tran_Date_Time'] = tran_date_time_col\n",
        "    df_providus_settlement_info['Source_Bank_Name'] = source_bank_name_col\n",
        "    df_providus_settlement_info['Source_Account_Number'] = source_account_number_col\n",
        "    df_providus_settlement_info['Source_Account_Name'] = source_account_name_col\n",
        "    df_providus_settlement_info['SettlementId'] = settlementId_col\n",
        "    df_providus_settlement_info['settled_Amount'] = settled_amount_col\n",
        "    df_providus_settlement_info['Initiation_TranRef'] = initiation_tranRef_col\n",
        "    df_providus_settlement_info['FeeAmount'] = feeAmount_col\n",
        "    df_providus_settlement_info['Currency'] = currency_col\n",
        "    df_providus_settlement_info['ChannelId'] = channelId_col\n",
        "    df_providus_settlement_info['SessionId'] = sessionId_col\n",
        "    df_providus_settlement_info['Type_Request'] = type_request_col\n",
        "    df_providus_settlement_info['Phone_Number'] = phone_col\n",
        "    df_providus_settlement_info['Date'] = date_col\n",
        "    df_providus_settlement_info['EndPoint'] = endpoint_col\n",
        "    df_providus_settlement_info['Log_Level'] = log_level_col\n",
        "    df_providus_settlement_info['Email'] = email_col\n",
        "    df_providus_settlement_info['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_settlement_info['Total Sent'] = totalsent_col\n",
        "    df_providus_settlement_info['Cost'] = cost_col\n",
        "    df_providus_settlement_info['Status'] = status_col\n",
        "    df_providus_settlement_info['Account Number'] = account_number_col\n",
        "    df_providus_settlement_info['Account Name'] = account_name_col\n",
        "    df_providus_settlement_info['BVN'] = bvn_col\n",
        "    df_providus_settlement_info['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_settlement_info['Response Message'] = responseMessage_col\n",
        "    df_providus_settlement_info['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_settlement_info\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxUKvX0yzEj"
      },
      "source": [
        "providus_settlement_info = df_raw[df_raw['text'].str.contains('PROVIDUS SETTLEMENT INFO')]\n",
        "df_providus_settlement_info = parse_PROVIDUS_SETTLEMENT_INFO_DF(providus_settlement_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxmYw6dGjSYv"
      },
      "source": [
        "df_providus_settlement_info.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-g1LdhAzRIy"
      },
      "source": [
        "def parse_PROVIDUS_VERIFY_SETTLEMENT_INFO_DF(df_providus_verify_settlement_info):\n",
        "    \"\"\"\n",
        "    la fonction permet de parser les types de requete \"PROVIDUS VERIFY SETTLEMENT INFO\" sur tout le dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    log_level_col = []\n",
        "    type_request_col = []\n",
        "    phone_col = []\n",
        "    date_col = []\n",
        "    endpoint_col = []\n",
        "    email_col = []\n",
        "    message_sms_payload_col = []\n",
        "    totalsent_col = []\n",
        "    cost_col = []\n",
        "    status_col = []\n",
        "    account_number_col = []\n",
        "    account_name_col = []\n",
        "    bvn_col = []\n",
        "    requestSuccessful_col = []\n",
        "    responseMessage_col = []\n",
        "    responseCode_col = []\n",
        "    sessionId_col = []\n",
        "    channelId_col = []\n",
        "    feeAmount_col = []\n",
        "    currency_col = []\n",
        "    initiation_tranRef_col = []\n",
        "    settled_amount_col = []\n",
        "    settlementId_col = []\n",
        "    source_account_name_col = []\n",
        "    source_account_number_col = []\n",
        "    source_bank_name_col = []\n",
        "    tran_date_time_col = []\n",
        "    tran_remarks_col = []\n",
        "    transaction_amount_col = []\n",
        "    vat_amount_col = []\n",
        "\n",
        "\n",
        "    list_column_none_providus_success = []\n",
        "    list_column_none_providus_success = [totalsent_col, \n",
        "                                        message_sms_payload_col, \n",
        "                                        cost_col, \n",
        "                                        status_col, \n",
        "                                        email_col, \n",
        "                                        phone_col, \n",
        "                                        endpoint_col, \n",
        "                                        account_name_col, bvn_col,\n",
        "                                        requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "\n",
        "    list_all_colum = []\n",
        "    list_all_colum = [type_request_col,phone_col, date_col, endpoint_col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col, \n",
        "                    sessionId_col, channelId_col, feeAmount_col, currency_col, initiation_tranRef_col,\n",
        "                    settled_amount_col, settlementId_col, source_account_name_col, source_account_number_col,\n",
        "                    source_bank_name_col, tran_date_time_col, tran_remarks_col, transaction_amount_col, vat_amount_col]\n",
        "\n",
        "    for index, row in df_providus_verify_settlement_info.iterrows():\n",
        "        str_text = row['text']\n",
        "\n",
        "        if not str_text.startswith('['):\n",
        "            for i in range(len(list_all_colum)):\n",
        "                list_all_colum[i].append(None)\n",
        "\n",
        "        # check if the row contains \"info\" string\n",
        "        if re.search('info', str_text):\n",
        "            log_level = re.search('info', str_text)\n",
        "            date_col.append(row['ts'])\n",
        "            try:\n",
        "                log_level_col.append(log_level.group(0))\n",
        "            except AttributeError:\n",
        "                log_level_col.append(None)             \n",
        "            if 'mailto' not in str_text:\n",
        "                if re.search('PROVIDUS VERIFY SETTLEMENT INFO', str_text):\n",
        "                    providus_verify_settlement_info = parse_wallet_sms_payload_success(str_text)\n",
        "                    print(providus_verify_settlement_info)\n",
        "                    type_of_request = re.search('PROVIDUS VERIFY SETTLEMENT INFO', str_text)\n",
        "\n",
        "\n",
        "                    try:\n",
        "                        type_request_col.append(type_of_request.group(0))\n",
        "                    except AttributeError:\n",
        "                        type_request_col.append(None)\n",
        "                    try:\n",
        "                        account_number_col.append(providus_verify_settlement_info.get('accountNumber'))\n",
        "                    except AttributeError:\n",
        "                        account_number_col.append(None)\n",
        "                    try:\n",
        "                        channelId_col.append(providus_verify_settlement_info.get('channelId'))\n",
        "                    except AttributeError:\n",
        "                        channelId_col.append(None)\n",
        "                    try:\n",
        "                        sessionId_col.append(providus_verify_settlement_info.get('sessionId'))\n",
        "                    except AttributeError:\n",
        "                        sessionId_col.append(None)\n",
        "                    try:\n",
        "                        currency_col.append(providus_verify_settlement_info.get('currency'))\n",
        "                    except AttributeError:\n",
        "                        currency_col.append(None)\n",
        "                    try:\n",
        "                        feeAmount_col.append(providus_verify_settlement_info.get('feeAmount'))\n",
        "                    except AttributeError:\n",
        "                        feeAmount_col.append(None)\n",
        "                    try:\n",
        "                        initiation_tranRef_col.append(providus_verify_settlement_info.get('initiationTranRef'))\n",
        "                    except AttributeError:\n",
        "                        initiation_tranRef_col.append(None)\n",
        "                    try:\n",
        "                        settled_amount_col.append(providus_verify_settlement_info.get('settledAmount'))\n",
        "                    except AttributeError:\n",
        "                        settled_amount_col.append(None)\n",
        "                    try:\n",
        "                        settlementId_col.append(providus_verify_settlement_info.get('settlementId'))\n",
        "                    except AttributeError:\n",
        "                        settlementId_col.append(None)\n",
        "                    try:\n",
        "                        source_account_name_col.append(providus_verify_settlement_info.get('sourceAccountName'))\n",
        "                    except AttributeError:\n",
        "                        source_account_name_col.append(None)\n",
        "                    try:\n",
        "                        source_account_number_col.append(providus_verify_settlement_info.get('sourceAccountNumber'))\n",
        "                    except AttributeError:\n",
        "                        source_account_number_col.append(None)\n",
        "                    try:\n",
        "                        source_bank_name_col.append(providus_verify_settlement_info.get('sourceBankName'))\n",
        "                    except AttributeError:\n",
        "                        source_bank_name_col.append(None)\n",
        "                    try:\n",
        "                        tran_date_time_col.append(providus_verify_settlement_info.get('tranDateTime'))\n",
        "                    except AttributeError:\n",
        "                        tran_date_time_col.append(None)\n",
        "                    try:\n",
        "                        tran_remarks_col.append(providus_verify_settlement_info.get('tranRemarks'))\n",
        "                    except AttributeError:\n",
        "                        tran_remarks_col.append(None)\n",
        "                    try:\n",
        "                        transaction_amount_col.append(providus_verify_settlement_info.get('transactionAmount'))\n",
        "                    except AttributeError:\n",
        "                        transaction_amount_col.append(None)\n",
        "                    try:\n",
        "                        vat_amount_col.append(providus_verify_settlement_info.get('vatAmount'))\n",
        "                    except AttributeError:\n",
        "                        vat_amount_col.append(None)\n",
        "\n",
        "                    for n in range(len(list_column_none_providus_success)):\n",
        "                        list_column_none_providus_success[n].append(None)  \n",
        "    \n",
        "    df_providus_verify_settlement_info['Vat_Amount'] = vat_amount_col\n",
        "    df_providus_verify_settlement_info['Transaction_Amount'] = transaction_amount_col\n",
        "    df_providus_verify_settlement_info['Tran_Remarks'] = tran_remarks_col\n",
        "    df_providus_verify_settlement_info['Tran_Date_Time'] = tran_date_time_col\n",
        "    df_providus_verify_settlement_info['Source_Bank_Name'] = source_bank_name_col\n",
        "    df_providus_verify_settlement_info['Source_Account_Number'] = source_account_number_col\n",
        "    df_providus_verify_settlement_info['Source_Account_Name'] = source_account_name_col\n",
        "    df_providus_verify_settlement_info['SettlementId'] = settlementId_col\n",
        "    df_providus_verify_settlement_info['settled_Amount'] = settled_amount_col\n",
        "    df_providus_verify_settlement_info['Initiation_TranRef'] = initiation_tranRef_col\n",
        "    df_providus_verify_settlement_info['FeeAmount'] = feeAmount_col\n",
        "    df_providus_verify_settlement_info['Currency'] = currency_col\n",
        "    df_providus_verify_settlement_info['ChannelId'] = channelId_col\n",
        "    df_providus_verify_settlement_info['SessionId'] = sessionId_col\n",
        "    df_providus_verify_settlement_info['Type_Request'] = type_request_col\n",
        "    df_providus_verify_settlement_info['Phone_Number'] = phone_col\n",
        "    df_providus_verify_settlement_info['Date'] = date_col\n",
        "    df_providus_verify_settlement_info['EndPoint'] = endpoint_col\n",
        "    df_providus_verify_settlement_info['Log_Level'] = log_level_col\n",
        "    df_providus_verify_settlement_info['Email'] = email_col\n",
        "    df_providus_verify_settlement_info['Message SMS Payload'] = message_sms_payload_col\n",
        "    df_providus_verify_settlement_info['Total Sent'] = totalsent_col\n",
        "    df_providus_verify_settlement_info['Cost'] = cost_col\n",
        "    df_providus_verify_settlement_info['Status'] = status_col\n",
        "    df_providus_verify_settlement_info['Account Number'] = account_number_col\n",
        "    df_providus_verify_settlement_info['Account Name'] = account_name_col\n",
        "    df_providus_verify_settlement_info['BVN'] = bvn_col\n",
        "    df_providus_verify_settlement_info['Request Successful'] = requestSuccessful_col\n",
        "    df_providus_verify_settlement_info['Response Message'] = responseMessage_col\n",
        "    df_providus_verify_settlement_info['Response Code'] = responseCode_col\n",
        " \n",
        "    return df_providus_verify_settlement_info\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRUSLfr4zVJR"
      },
      "source": [
        "providus_verify_settlement_info = df_raw[df_raw['text'].str.contains('PROVIDUS VERIFY SETTLEMENT INFO')]\n",
        "df_providus_verify_settlement_info = parse_PROVIDUS_VERIFY_SETTLEMENT_INFO_DF(providus_verify_settlement_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcWUJ829jaOY"
      },
      "source": [
        "df_providus_verify_settlement_info.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0EsxMifCEU"
      },
      "source": [
        "### on fait la concatenation par ligne car dans chaque df on les "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRCfDmlrlBTd"
      },
      "source": [
        "pdList = [df_api_request, df_client_mobile_login, df_loan_payload, df_error, df_lead_error, df_okra_webhook, df_providus_payload, df_providus_settlement_info,\n",
        "          df_providus_success, df_providus_transfer_error, df_providus_transfer_success, df_providus_verify_settlement_info,\n",
        "          df_sms_payload, df_sms_success, df_vtpass_payload, df_wallet_success]  # List of our dataframes\n",
        "df_final = pd.concat(pdList, axis=0)"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8grT-h74Wth"
      },
      "source": [
        "df_final.sort_values('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YloqA19OgRLS"
      },
      "source": [
        "df_phone_number = df_final['Phone_Number'].unique()"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOvXUqm5zfvk"
      },
      "source": [
        "#df_final.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA3mWTUB1JMB"
      },
      "source": [
        "df_with_loan_amount = df_final[df_final['text'].str.contains('loan_amount')].copy()"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GW60fb9CSF2"
      },
      "source": [
        "df_final.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDLgeXIC16i-"
      },
      "source": [
        "df_with_loan_amount.to_csv('/content/drive/MyDrive/datasets/final/loan_amount/loan_amount.csv', index=True)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8SSc9xahps_"
      },
      "source": [
        "list_index_loan_amount = list(df_with_loan_amount.index)"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD7uzxvnaO26"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def difference_ts(date_in_timestamp):\n",
        "  dt = datetime.datetime.fromtimestamp(date_in_timestamp)\n",
        "  res = dt - datetime.timedelta(minutes=30) \n",
        "  return datetime.datetime.timestamp(res)"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5fim9hpa1Bm",
        "outputId": "b14f5bc9-33bb-40c3-e32f-0f2556973bad"
      },
      "source": [
        "t = 1616891445.0048\n",
        "dt_res = difference_ts(t)\n",
        "dt_res"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616889645.0048"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUMy_hKClCIW"
      },
      "source": [
        "df_with_loan_amount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDNOuVS5nXqp",
        "outputId": "a36ce816-d27b-4b6d-d155-4703bcd69cc7"
      },
      "source": [
        "print(list_index_loan_amount)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[162, 179, 461, 653, 836, 846, 949, 1030, 1039, 1071, 1079, 1113, 1131, 1143, 1779, 1787, 1797, 1805, 1813, 1849, 1865, 1873, 1881, 1889, 1897, 1905, 1916, 1925, 1939, 1957, 1965, 1973, 1982, 1989, 2007, 2016, 2023, 2056, 2070, 2080, 2094, 2108, 2130, 2142, 2154, 2162, 2170, 2178, 2188, 2226, 2236, 2244, 2258, 2268, 2276, 2290, 2300, 2318, 2326, 2335, 2354, 2371, 2389, 2397, 2405, 2417, 2425, 2433, 2442, 2471, 2479, 2489, 2497, 2505, 2513, 2529, 2537, 2548, 2558, 2578, 2586, 2594, 2604, 2618, 2632, 2640, 2648, 2656, 2664, 2674, 2683, 2694, 2698, 2708, 2726, 2736, 2758, 849, 1135, 1890, 1958, 1974, 2131, 2171, 2434, 2472, 2549, 2579, 2649, 2808, 2905, 2963, 3159, 3258, 3370, 3410, 3424, 3625, 3647, 3721, 3745, 3837, 3871, 3899, 3960, 3968, 3986, 3996, 4010, 4039, 4071, 4111, 4292, 4555, 4713, 4759, 4767, 4791, 4804, 4812, 4836, 4869, 4897, 4943, 4959, 4997, 5076, 5212, 5246, 5308, 5324, 5338, 5344, 5366, 5374, 5382, 5412, 5434, 5438, 5455, 5481, 5485, 5513, 5543, 5583, 5609, 5629, 5663, 5671, 5689, 5711, 5725, 5811, 5837, 5847, 5920, 5934, 5940, 5994, 6006, 6095, 6123, 6129, 6164, 6189, 6224, 6254, 6270, 6413, 6471, 6495, 6509, 6577, 6581, 6740, 6752, 6768, 6772, 6780, 6788, 6820, 6836, 6863, 6981, 7011, 7029, 7033, 7101, 7105, 7119, 7142, 7204, 7218, 7256, 7270, 7298, 7332, 7366, 7439, 7443, 7455, 7501, 7551, 7579, 7583, 7643, 7649, 7706, 7716, 7830, 7886, 7896, 7912, 7930, 7950, 8058, 8110, 8157, 8175, 8228, 8274, 8282, 8322, 8352, 8476, 8490, 8504, 8534, 8572, 8607, 8617, 8623, 8635, 8673, 8723, 8735, 8751, 8759, 8801, 8867, 8879, 8909, 8925, 8949, 9013, 9025, 9040, 9095, 9185, 9285, 9337, 9359, 9371, 9410, 9464, 9474, 9544, 9560, 9584, 9629, 9641, 9701, 9725, 9749, 9793, 9804, 9824, 9854, 9864, 9872, 9938, 9942, 10002, 10078, 10142, 10212, 10258, 10328, 10346, 10374, 10380, 10424, 10428, 10436, 10472, 10506, 10618, 10644, 10658, 10682, 10704, 10740, 10840, 10844, 10858, 10898, 10930, 10944, 10954, 10972, 10990, 11046, 11070, 11102, 11186, 11196, 11306, 11330, 11372, 11378, 11396, 11430, 11440, 11448, 11454, 11470, 11548, 11624, 11628, 11642, 11684, 11710, 11764, 11782, 11830, 11844, 11874, 11878, 11894, 11898, 11922, 11988, 12030, 12048, 12064, 12076, 12088, 12120, 12126, 12140, 12160, 12232, 12302, 12394, 12406, 12410, 12442, 12450, 12474, 12504, 12538, 12546, 12550, 12642, 12666, 12676, 12702, 12810, 12842, 12973, 12979, 13015, 13067, 13073, 13089, 13136, 13176, 13180, 13242, 13246, 13280, 13304, 13354, 13448, 13659, 13704, 13710, 13744, 13770, 13818, 13846, 13850, 13904, 13942, 14004, 14222, 14234, 14248, 14254, 14274, 14306, 14324, 14348, 14461, 14481, 14507, 14513, 14593, 14663, 14743, 14751, 14787, 14919, 14923, 14943, 14961, 14977, 14993, 15001, 15079, 15099, 15143, 15309, 15359, 15377, 15381, 15409, 15446, 15506, 15512, 15566, 15574, 15582, 15602, 15606, 15748, 15816, 15883, 15899, 15959, 15977, 16109, 16115, 16129, 16145, 16233, 16305, 16329, 16347, 16411, 16417, 16425, 16471, 16472, 16583, 16663, 16677, 16821, 16884, 16894, 16940, 17048, 17074, 17090, 17100, 17134, 17210, 17238, 17356, 17376, 17431, 17537, 17547, 17605, 17625, 17688, 17745, 17765, 17771, 17783, 17815, 17854, 17894, 18004, 18034, 18038, 18154, 18183, 18201, 18223, 18229, 18327, 18333, 18403, 18409, 18447, 18471, 18495, 18499, 18507, 18550, 18558, 18650, 18744, 18840, 18912, 18918, 18928, 19008, 19048, 19110, 19118, 19236, 19293, 19309, 19319, 19416, 19458, 19544, 19664, 19690, 19708, 19886, 19950, 20202, 20206, 20219, 20362, 20424, 20432, 20670, 20694, 20890, 20896, 20904, 20939, 20987, 21011, 21069, 21166, 21203, 21232, 21236, 21294, 21312, 21318, 21458, 21459, 21460, 21463, 21464, 21465, 21466, 21467, 21468, 21469, 21470, 21471, 21472, 21473, 21476, 21479, 21480, 21481, 21510, 21600, 21608, 21634, 21749, 21755, 21783, 21802, 21898, 21930, 21980, 22005, 22057, 22165, 22222, 22266, 22384, 22398, 22456, 22490, 22963, 23031, 23039, 23086, 23126, 23171, 23244, 23263, 23301, 23313, 23319, 23357, 23393, 23411, 23541, 23549, 23561, 23713, 24157, 24219, 24273, 24297, 24307, 24418, 24620, 24635, 24672, 24808, 24836, 24888, 25033, 25202, 25224, 25276, 25394, 25425, 25627, 25674, 25687, 25751, 25785, 25799, 25881, 25907, 26094, 26282, 26304, 26415, 26467, 26536, 26630, 26690, 26742, 26788, 26806, 26812, 26882, 26950, 27058, 27200, 33226, 34096, 34431, 37234, 58620, 63822, 68981, 79130, 85405, 86636, 87797, 89190, 92191]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvTyCSM6h66x",
        "outputId": "5c708589-292b-42ce-9518-2bdcda7f4bd9"
      },
      "source": [
        "df_final.loc[179, 'ts']"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616891445.0048"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydrC_YLJku1p"
      },
      "source": [
        "df_rr = df_final[(df_final['ts'] <= df_final.loc[1113, 'ts']) & (df_final['ts'] > difference_ts(df_final.loc[1113, 'ts']))]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UefOGWOOnuIO",
        "outputId": "25c0829b-9af9-4193-d688-3ac38fd99a4a"
      },
      "source": [
        "df_rr.loc[1113, 'Phone_Number']"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dYiyqpV4zio"
      },
      "source": [
        "#df_final.iloc[66624:66630]\n",
        "#1- filtrer le dataset sur la colonne Phone_Number avec le filtre != null ou != o\n",
        "#2- ensuite on récupère les index\n",
        "#3- on prend le plus grand index \n",
        "#4- on récupère le Phone_Number du grand index \n",
        "#5- on met ce phone number dans la colonne Phone_Number de la ligne contenant  \n",
        "# \"LOAN PAYLOAD\" ou \"LOAN ERROR\" sans numéro de téléphone\n",
        "# ensuite on repart dans la liste index_loan_amount, on récupère le prochain\n",
        "# indexe pour exécuter les précédantes opérations"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztwIFc6NhBvo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "phone_list = []\n",
        "for index in tqdm(list_index_loan_amount):\n",
        "  df_res = df_final[(df_final['ts'] <= df_final.loc[index, 'ts']) & (df_final['ts'] > difference_ts(df_final.loc[index, 'ts']))]\n",
        "  list_index = df_res[df_res['Phone_Number'].isnull() == False].index.values\n",
        "  if len(list_index) > 0:\n",
        "    min_index = np.min(list_index)\n",
        "    df_final['Phone_Number'][index] = df_res.loc[min_index, 'Phone_Number']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ho22lWd3-sfQ",
        "outputId": "f7527cd9-c350-4d75-d248-1b3e1b8b5877"
      },
      "source": [
        "df_final['Phone_Number'][649]"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2348068802264 '"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HH3TASA5_L"
      },
      "source": [
        "#df_final[df_final['Loan Amount'] == 50000.0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klzQhg_I2iIv"
      },
      "source": [
        "import matplotlib.pyplot\n",
        "#df_final.plot.bar()"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gURUbWDcWkO"
      },
      "source": [
        "### Supprimer les colonnes peu remplies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcX5DFI46pyw"
      },
      "source": [
        "pct_null = df_final.isnull().sum() / len(df_final)"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1faUt6C6yEX"
      },
      "source": [
        "#missing_features = pct_null[pct_null > 0.998].index\n",
        "#df_final.drop(missing_features, axis=1, inplace=True)"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0RjV7oPcES-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmWjA-7c2oKe"
      },
      "source": [
        "#master_df.info(verbose=True, null_counts=True)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILWbhc724al"
      },
      "source": [
        "# prendre toute une ligne du dataframe qui contient des éléments de types object (String),\n",
        "# partout où on trouve des NaN, les remplacer par string vide '', ensuite où on trouve\n",
        "# une valeur, on fait la concatenation ou le Join.\n",
        "#df_final.apply(lambda row: ('').join(row['type'].fillna('').values), axis=1)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6YhR06BVJgL"
      },
      "source": [
        "df_final.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6rhrpxL1Ruj"
      },
      "source": [
        "#df_final.apply(lambda row: ('').join(row['ts'].fillna('').values), axis=1)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNuyqcMM1bCU"
      },
      "source": [
        "#df_final['ts'].fillna(0).sum(axis=1)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpdCBMBQ6uNR"
      },
      "source": [
        "#df_final.apply(lambda row: ('').join(row['subtype'].fillna('').values), axis=1)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ywNqkPT7jbB"
      },
      "source": [
        "#df_final['Loan Amount'].fillna(0).sum(axis=1)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3RrsGciOYX3",
        "outputId": "40e04631-8bf6-4eed-808c-e348c032a0f5"
      },
      "source": [
        "cd /content/drive/My Drive/datasets/final"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/datasets/final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipUndaAClsTk"
      },
      "source": [
        "### Export DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-OennYvluQn"
      },
      "source": [
        "df_final.to_csv('/content/drive/MyDrive/datasets/final/nirra_log_bot_final.csv', index=None)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smVnWy7aOAxM",
        "outputId": "a4a68de3-11ad-4c49-9da6-8ebb9f79e1f1"
      },
      "source": [
        "len(df_final['Phone_Number'].unique())"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5764"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSp3sqTVOSC9"
      },
      "source": [
        "df_final['Phone_Number'].unique() # récupérer la liste des différents numéros de téléphone\n",
        "all_users_phone_number = [element for element in df_final['Phone_Number'].unique() if element != None] # all userId\n",
        "#all_users_phone_number # liste de tous les différents numéros de téléphone."
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLi__VfhQyTH",
        "outputId": "52901b35-b569-4297-e798-beed8900857c"
      },
      "source": [
        "len(all_users_phone_number)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5763"
            ]
          },
          "metadata": {},
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCdVwWuhOype",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ffc5e6-5fa3-4c73-a1d2-a83498e92bf0"
      },
      "source": [
        "all_df_phone_number = [] # to get list of dataframes\n",
        "for phoneNumber in tqdm(all_users_phone_number):\n",
        "  df_phone_number = df_final[df_final['Phone_Number'] == phoneNumber] # récupérer les dataframes avec seulement les numéros de téléphone\n",
        "  all_df_phone_number.append(df_phone_number)  # mettre chaque dataframe dans la"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5763/5763 [00:27<00:00, 207.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j70xFNNuSHJK"
      },
      "source": [
        "all_df_phone_number[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFhigDtPIgb"
      },
      "source": [
        "### sauvegarder les dataframes sur disque"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQrfml-3OsO9",
        "outputId": "217e628d-a2a3-47c7-bbf8-9a778b219675"
      },
      "source": [
        "cd /content/drive/My Drive/datasets/final"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/datasets/final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9aFFxtofAch"
      },
      "source": [
        "### Sauvegarder sur le disque les dataframes de chaque utilisateur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OJHQyIyQv66",
        "outputId": "b6a8477c-277a-4875-979c-4fcafa173c38"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# tqdm permet\n",
        "for phoneNumber in tqdm(all_users_phone_number):\n",
        "  df_phone_number = df_final[df_final['Phone_Number'] == phoneNumber]\n",
        "  df_phone_number.to_csv(f'/content/drive/MyDrive/datasets/final/files/{phoneNumber}.csv', index=None)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5763/5763 [01:14<00:00, 77.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72zTYx0we3VF"
      },
      "source": [
        "### se déplacer dans le dossier contenant les dataframes de chaque utilisateur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZh2y-x8PdIv",
        "outputId": "fa091ec5-280c-4160-91ee-7665a3aa5233"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/final/files/"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/final/files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jg94X1ZPzDf",
        "outputId": "0c75788d-5c1e-4f5d-bd87-0f3c639c2caf"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# use glob to get all the csv files \n",
        "# in the folder\n",
        "path = os.getcwd()\n",
        "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "all_df_list = [] # list of all dataframe\n",
        "# loop over the list of csv files\n",
        "number_transactions = []\n",
        "all_df_list_with_number_transaction = []\n",
        "list_df_with_diff_ts = []\n",
        "\n",
        "for f in tqdm(csv_files):    \n",
        "    # read the csv file\n",
        "    df = pd.read_csv(f)  \n",
        "    all_df_list.append(df)   \n",
        "\n",
        "for element in all_df_list:\n",
        "  element['Number_Transactions'] = element.shape[0] # faire la somme des lignes et mettre dans la colonne Number_Transactions\n",
        "  all_df_list_with_number_transaction.append(element)# mettre tous les éléments dans une nouvelle liste\n",
        "\n",
        "#Faire la différence entre les timestamp\n",
        "for ele_with_num_trans in all_df_list_with_number_transaction:\n",
        "  df_ts = pd.DataFrame(ele_with_num_trans['ts'])\n",
        "  df_ts.sort_values('ts')\n",
        "  ele_with_num_trans['ts_diff'] = df_ts.diff(axis=0)\n",
        "  list_df_with_diff_ts.append(ele_with_num_trans)\n"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5723/5723 [00:43<00:00, 131.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAoCpJmGgKrP"
      },
      "source": [
        "list_df_with_diff_ts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2pcP8awSqNi"
      },
      "source": [
        "### Entrainer premièrement le modèle global"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UElUbtcVSxG3",
        "outputId": "b0a15bb0-1d06-4d2e-9d14-4be57faa5638"
      },
      "source": [
        "# avoir la distribution des nombres de transactions de tous les utilisateurs et faire un histogramme\n",
        "list_number_transactions = []\n",
        "for ele in tqdm(list_df_with_diff_ts):\n",
        "  if len(ele) > 0:\n",
        "    list_number_transactions.append(ele['Number_Transactions'][0])"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5723/5723 [00:00<00:00, 14795.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NWWTqVcZ8OU",
        "outputId": "f8da70a7-fa79-48df-c441-57095689749b"
      },
      "source": [
        "max(list_number_transactions)"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "939"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGmvbJ6Mh5iL"
      },
      "source": [
        "### Tracer l'histogramme logarithme en fonction de la liste contenant le nombre de transactions pour chaque utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN-ZVXZUXI4W"
      },
      "source": [
        "df_liste_transactions = pd.DataFrame(list_number_transactions, columns=['Number_Transactions_per_user'])"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u6rCPgthh6b",
        "outputId": "e91c25fb-6612-4f62-d2fb-716729776db7"
      },
      "source": [
        "len(list_df_with_diff_ts)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5723"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "KvGEP4sHX-XP",
        "outputId": "f90e9853-7d3c-4307-992a-e77ab0d7c1ad"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# tracer un histogramme logarithmique\n",
        "y = pd.Series(list_number_transactions)\n",
        "hist, bins, _ = plt.hist(y, bins=8)\n",
        "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
        "plt.yscale('log', nonposy='clip')\n",
        "plt.hist(y, bins=logbins)\n",
        "plt.show()"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBFF5uVtouAo"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure(\n",
        "    data=[go.Bar(y=list_number_transactions)],\n",
        "    layout_title_text=\"A Figure Displayed with fig.show()\"\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcETXq5fXD6"
      },
      "source": [
        "### Pour chaque utilisateur, calculons la fréquence d'opérations soit par minute, ou toutes les 5 minutes ou plus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyOeCnWMfUnj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrbjuOgVggif"
      },
      "source": [
        "### récupérer la liste des utilisateurs qui pourraient etre problématiques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwmkUFm-ggIK",
        "outputId": "a001020f-6f5e-4b1d-aebf-e3b89be2f64d"
      },
      "source": [
        "### selon l'histogramme on a environs une vingtaine d'utilisateurs qui ont effectué entre 400 et plus de 800 transactions\n",
        "# on peut essayer de récupérer la liste de ces utilisateurs et étudier leurs comportements\n",
        "# sauvegarder sur le disque la liste des utilisateurs ayant plus de 400 transactions\n",
        "liste_utilisateurs_problematiques = []\n",
        "for one_df_diff_ts in tqdm(list_df_with_diff_ts):\n",
        "  if one_df_diff_ts.shape[0] >= 300:\n",
        "    liste_utilisateurs_problematiques.append(one_df_diff_ts)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5723/5723 [00:00<00:00, 342166.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW-g7UoJU_jW",
        "outputId": "6084d278-1bc1-4281-d2b8-ef16c54c2e27"
      },
      "source": [
        "len(liste_utilisateurs_problematiques)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFDwJY2XVGAk"
      },
      "source": [
        "liste_utilisateurs_problematiques[0].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7RQoVbkxDN9"
      },
      "source": [
        "### Consolidation de tous les dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTo0M5oBtT92"
      },
      "source": [
        "liste_utilisateurs_problematiques[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDG_gn8wjLMd"
      },
      "source": [
        "### sauvegarder ces utilisateurs dans le disque en local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAAWQ3KJjKT3",
        "outputId": "53f1fa8f-a026-48a2-de03-1ef0bfdb3cfc"
      },
      "source": [
        "for elem in tqdm(liste_utilisateurs_problematiques):\n",
        "  phone = elem['Phone_Number'][0]\n",
        "  elem.to_csv(f'/content/drive/MyDrive/datasets/final/users_problematics/{phone}.csv', index=None)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 32.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8w2z-JgRnGz"
      },
      "source": [
        "### Création des times series sur tous les utilisateurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OEzZ5khRodR",
        "outputId": "9984fdf1-0ea3-479e-8abe-d7ebac8bcc58"
      },
      "source": [
        "all_df_res = []\n",
        "new_endpoint_col = []\n",
        "for new_data_frame in tqdm(list_df_with_diff_ts):\n",
        "  new_data_frame['EndPoint'].fillna('', inplace=True) # modifie la colonne\n",
        "\n",
        "  new_data_frame['Appplication_get'] = new_data_frame['EndPoint'].str.contains('applications/get').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Payment'] = new_data_frame['EndPoint'].str.contains('payment').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Declined'] = new_data_frame['EndPoint'].str.contains('decline').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Preapprouved'] = new_data_frame['EndPoint'].str.contains('preapproved').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Accepted'] = new_data_frame['EndPoint'].str.contains('accept').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Lifestyle'] = new_data_frame['EndPoint'].str.contains('lifestyle').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Wallet_balance'] = new_data_frame['EndPoint'].str.contains('wallet/balance').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Adverts'] = new_data_frame['EndPoint'].str.contains('adverts').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Bank'] = new_data_frame['EndPoint'].str.contains('bank').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Updated'] = new_data_frame['EndPoint'].str.contains('update').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Products'] = new_data_frame['EndPoint'].str.contains('products').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Locations'] = new_data_frame['EndPoint'].str.contains('locations').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Client_Get'] = new_data_frame['EndPoint'].str.contains('client/get').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Client_V2_Create'] = new_data_frame['EndPoint'].str.contains('client/v2/create').astype(np.float).astype('Int32')\n",
        "  \n",
        "  \n",
        "  new_data_frame['Dummy'] = 0\n",
        "  new_data_frame['Payment_cumulative'] = new_data_frame.groupby(['Dummy'])['Payment'].cumsum()\n",
        "  new_data_frame['Decline_cumulative'] = new_data_frame.groupby(['Dummy'])['Declined'].cumsum()\n",
        "  new_data_frame['Application_get_cumulative'] = new_data_frame.groupby(['Dummy'])['Appplication_get'].cumsum()\n",
        "  new_data_frame['Preapprouved_cumulative'] = new_data_frame.groupby(['Dummy'])['Preapprouved'].cumsum()\n",
        "  new_data_frame['Accepted_cumulative'] = new_data_frame.groupby(['Dummy'])['Accepted'].cumsum()\n",
        "  new_data_frame['Lifestyle_cumulative'] = new_data_frame.groupby(['Dummy'])['Lifestyle'].cumsum()\n",
        "  new_data_frame['Wallet_balance_cumulative'] = new_data_frame.groupby(['Dummy'])['Wallet_balance'].cumsum()\n",
        "  new_data_frame['Adverts_cumulative'] = new_data_frame.groupby(['Dummy'])['Adverts'].cumsum()\n",
        "  new_data_frame['Bank_cumulative'] = new_data_frame.groupby(['Dummy'])['Bank'].cumsum()\n",
        "  new_data_frame['Update_cumulative'] = new_data_frame.groupby(['Dummy'])['Updated'].cumsum()\n",
        "  new_data_frame['Products_cumulative'] = new_data_frame.groupby(['Dummy'])['Products'].cumsum()\n",
        "  new_data_frame['Locations_cumulative'] = new_data_frame.groupby(['Dummy'])['Locations'].cumsum()\n",
        "  new_data_frame['Client_Get_cumulative'] = new_data_frame.groupby(['Dummy'])['Client_Get'].cumsum()\n",
        "  new_data_frame['Client_Get_cumulative'] = new_data_frame.groupby(['Dummy'])['Client_V2_Create'].cumsum()\n",
        "  all_df_res.append(new_data_frame)\n",
        "        "
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5723/5723 [04:13<00:00, 22.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJeRHJ4z7CO"
      },
      "source": [
        "all_df_res[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A9RzsG_vI1G",
        "outputId": "7608015f-2cb4-4fec-9e38-a433cfeaf9e0"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/final/files_with_new_characteristics"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/final/files_with_new_characteristics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQTeMCGpvkfG"
      },
      "source": [
        "### Sauvegarder les nouveaux datasets dans un nouveau dossier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvdxND9lkdak"
      },
      "source": [
        "all_df_res[427]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZqSV2HDvdew",
        "outputId": "577c86c8-f9c2-493d-a249-eef13da07d66"
      },
      "source": [
        "from tqdm import tqdm\n",
        " #tqdm permet\n",
        "for elem in tqdm(all_df_res):\n",
        "  if elem.shape[0] != 0:\n",
        "    try:\n",
        "      phone = elem['Phone_Number'][0]\n",
        "      elem.to_csv(f'/content/drive/MyDrive/datasets/final/files_with_new_characteristics/{phone}.csv', index=None)\n",
        "    except IndexError:\n",
        "      print(elem)\n",
        "      raise"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5723/5723 [00:58<00:00, 98.33it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UBFCkJyv9DV"
      },
      "source": [
        "### Parcourir le dossier contenant les dataframes et convertir dans une nouvelle colonne \"TS_to_DateTime\" la colonne \"ts\" en Datetime et mettre dans la liste all_df_list_new"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HxU7fWRvxPd"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def list_load_all_df(path='/content/drive/My Drive/datasets/final/files_with_new_characteristics'):\n",
        "  #path = os.getcwd()\n",
        "  csv_files_new = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "  all_df_list_new = []\n",
        "  # loop over the list of csv files\n",
        "  for f in tqdm(csv_files_new):    \n",
        "      # read the csv file\n",
        "      df_new = pd.read_csv(f)\n",
        "      df_new['TS_to_DateTime'] = df_new['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "      all_df_list_new.append(df_new)\n",
        "  return all_df_list_new"
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5vGSKHgwt51",
        "outputId": "a0d400da-f86b-4a0c-8bfc-eeba49f7aec8"
      },
      "source": [
        "res_all_df = list_load_all_df()"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5704/5704 [00:59<00:00, 96.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1VwDwHl-et"
      },
      "source": [
        "res_all_df[1].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oavoJKfUwkeW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Récupérer le nombre d'opérations consécutives (opérations qui se déroulent dans l'interval de 5 min)\n",
        "def df_nombre_operations_consecutives(each_df_2):\n",
        "  '''\n",
        "    cette fonction prend comme paramètre un dataframe et retourne un dataframe avec les nouvelles colonnes\n",
        "  '''\n",
        "  each_df = each_df_2.copy() \n",
        "  size_df = each_df.shape[0]\n",
        "  ts_delta=0\n",
        "  count = 0\n",
        "  last_ts = 0\n",
        "  number_consecutive_operations = []\n",
        "  time_consecutive_operations = []\n",
        "  phone_number_each_df = []\n",
        "  # on récupère la 1ere date ou la 1ère lige de la colonne \"TS_to_DateTime\"\n",
        "  start_ts = each_df.iloc[0]['TS_to_DateTime']\n",
        "  result = pd.DataFrame(columns=['Phone_Number', 'Number_consecutive_operations', 'Time_consecutive_operations',\n",
        "                                                                'Number_operations_per_minute', 'Max_number_operation_per_minute',\n",
        "                                                                'Min_number_operation_per_minute', 'Mean_operation_per_minute',\n",
        "                                                                'Median_operation_per_minute'])\n",
        "  \n",
        "  for i in range(size_df-1):\n",
        "    ts_delta = each_df.iloc[i+1]['TS_to_DateTime'] - each_df.iloc[i]['TS_to_DateTime']\n",
        "    # si l'interval de temps pendant lequel les opérations se sont effectuées est <= à 5 min \n",
        "    if (ts_delta.total_seconds()/60) <= 5:\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      # on compte les opérations effectuées en 5 minutes\n",
        "      count +=1\n",
        "    else :\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      ts_delta = (last_ts - start_ts).total_seconds()/60\n",
        "      # liste des temps en minutes pendant lequel les opérations se sont déroulées\n",
        "      time_consecutive_operations.append(ts_delta)\n",
        "      # nombres d'opérations consécutives (c-à-d opérations qui se déroulées dans un interval de 5 min)\n",
        "      number_consecutive_operations.append(count)\n",
        "      phone_number_each_df.append(each_df.iloc[i+1]['Phone_Number'])\n",
        "      count = 0\n",
        "      start_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "\n",
        "      df_phone_number_each_df = pd.DataFrame(phone_number_each_df)  \n",
        "      df_number_consecutive_operations = pd.DataFrame(number_consecutive_operations)\n",
        "      df_time_consecutive_operations = pd.DataFrame(time_consecutive_operations)\n",
        "      pdList_consecutive_operations = [df_phone_number_each_df, df_number_consecutive_operations, df_time_consecutive_operations] # List of our dataframes\n",
        "      df_final_consecutive_operations = pd.concat(pdList_consecutive_operations, axis=1)\n",
        "      df_final_consecutive_operations.columns=['Phone_Number', 'Number_consecutive_operations', 'Time_consecutive_operations']\n",
        "      df_final_consecutive_operations['Number_operations_per_minute'] = df_final_consecutive_operations['Number_consecutive_operations']/df_final_consecutive_operations['Time_consecutive_operations']\n",
        "      df_final_consecutive_operations['Max_number_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].max()\n",
        "      df_final_consecutive_operations['Min_number_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].min()\n",
        "      df_final_consecutive_operations['Mean_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].mean()\n",
        "      df_final_consecutive_operations['Median_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].median()\n",
        "      result = df_final_consecutive_operations\n",
        "  return result\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8PzeNFmwqUe"
      },
      "source": [
        "def df_one_operation(each_df):\n",
        "\n",
        "  copy_df = each_df.copy()\n",
        "  copy_df['Number_consecutive_operations'] = 0\n",
        "  copy_df['Time_consecutive_operations'] = 0\n",
        "  copy_df['Number_operations_per_minute'] = 0\n",
        "  copy_df['Max_number_operation_per_minute'] = 0\n",
        "  copy_df['Min_number_operation_per_minute'] = 0\n",
        "  copy_df['Mean_operation_per_minute'] = 0\n",
        "  copy_df['Median_operation_per_minute'] = 0\n",
        "  return copy_df['Phone_Number'][0]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEi2T7Q8mrxh"
      },
      "source": [
        "### Récupérer le nombre d'opérations par minute ou toutes les 5 minutes pour chaque utilisateur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KobGvvDxZsx",
        "outputId": "6274bc00-46dd-4d1b-cbfb-31b3f2668542"
      },
      "source": [
        "list_all_df_with_time_series = []\n",
        "for element in tqdm(res_all_df):\n",
        "  # pour debugger il faut utiliser try -  except et utiliser pass\n",
        "  # ainsi, en cas de problème on pourrait savoir dans quel DataFrame il se trouve\n",
        "  if element.shape[0] > 1:\n",
        "    list_all_df_with_time_series.append(df_nombre_operations_consecutives(element))"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5704/5704 [01:16<00:00, 74.95it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKxolzfxg7S"
      },
      "source": [
        "list_all_df_with_time_series[20].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er6gBTX7x5TK"
      },
      "source": [
        "def give_dataframe(phone_number, path_df='/content/drive/My Drive/datasets/final/files_with_new_characteristics'):\n",
        "  df = pd.read_csv(f'{path_df}/{phone_number}.csv')\n",
        "  df['TS_to_DateTime'] = df['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "  return df"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3OlWvAOzJBt"
      },
      "source": [
        "def give_dataframe_complete_path(path_complet_df='/content/drive/MyDrive/datasets/final/files_with_new_characteristics/2349152152920.csv'):\n",
        "  df_2 = pd.read_csv(path_complet_df)\n",
        "  df_2['TS_to_DateTime'] = df_2['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "  return df_2"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3BvIFfzrIJ"
      },
      "source": [
        "import statistics\n",
        "\n",
        "def calculate_metric(list_df, str_key_word='/client/'):\n",
        "  '''cette fonction permet de calculer la moyenne, la mediane, la valeur min, max et le seuil\n",
        "    en se référant sur la liste contenant la somme des opérations effectuées dans chaque dataframe. ''' \n",
        "  sum_operation = []\n",
        "  list_phone_number_user = []\n",
        "  for one_df in list_df:\n",
        "    one_df['EndPoint'].fillna('', inplace=True)\n",
        "    # tenir compte du cast d'un boolean en float avec la propriété astype(float)\n",
        "    sum_operation.append(one_df['EndPoint'].str.contains(str_key_word).astype(float).sum())\n",
        "    list_phone_number_user.append(one_df['Phone_Number'][0])\n",
        "\n",
        "  mini = min(sum_operation)\n",
        "  maxi = max(sum_operation)\n",
        "  mediane = statistics.median(sum_operation)\n",
        "  moyenne = statistics.mean(sum_operation)\n",
        "  ecart_type = statistics.pstdev(sum_operation)\n",
        "  threshold = mediane + (3*ecart_type) # les valeurs au délà de 3* écart type sont considérées comme outliers, on considère cette valeur comme notre seuil\n",
        "  print(f' print 3 écart-type: {3*ecart_type}')\n",
        "  print(f'somme operation est: {sum_operation}')\n",
        "  print(f'la liste des numéros de téléphone des utilisateur est: {list_phone_number_user}')\n",
        "  print(f'le max est: {maxi}\\nle min est: {mini}\\nla mediane est: {mediane}\\nla moyenne est {moyenne}\\necart type est {ecart_type}\\nle seuil est {threshold}')\n",
        "  return list_phone_number_user, sum_operation, mini, maxi, mediane, moyenne, ecart_type, threshold"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdh4eBWpzxw9"
      },
      "source": [
        "list_phone_number_user_client, sum_operation_client, min_client, max_client, mediane_client, moyenne_client, et_client, threshold_client = calculate_metric(res_all_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU8D4zdvzxVx"
      },
      "source": [
        "list_phone_number_user_client, sum_operation_client, min_client, max_client, mediane_client, moyenne_client, et_client, threshold_client = calculate_metric(res_all_df)\n",
        "list_phone_number_user_decline, sum_operation_decline, mini_decline, maxi_decline, mediane_decline, moyenne_decline, et_decline, threshold_decline = calculate_metric(res_all_df, str_key_word='/decline/')\n",
        "list_phone_number_user_lifestyle, sum_operation_lifestyle, min_lifestyle, max_lifestyle, mediane_lifestyle, moyenne_lifestyle, et_lifestyle, threshold_lifestyle = calculate_metric(res_all_df, str_key_word='lifestyle')\n",
        "list_phone_number_user_payment, sum_operation_payment, min_payment, max_payment, mediane_payment, moyenne_payment, et_payment, threshold_payment = calculate_metric(res_all_df, str_key_word='payment')\n",
        "list_phone_number_user_app_get, sum_operation_app_get, min_app_get, max_app_get, mediane_app_get, moyenne_app_get, et_app_get, threshold_app_get = calculate_metric(res_all_df, str_key_word='applications/get')\n",
        "list_phone_number_user_preapproved, sum_operation_preapproved, min_preapproved, max_preapproved, mediane_preapproved, moyenne_preapproved, et_preapproved, threshold_preapproved = calculate_metric(res_all_df, str_key_word='preapproved')\n",
        "list_phone_number_user_accept, sum_operation_accept, min_accept, max_accept, mediane_accept, moyenne_accept, et_accept, threshold_accept = calculate_metric(res_all_df, str_key_word='accept')\n",
        "list_phone_number_user_wallet_balance, sum_operation_wallet_balance, min_wallet_balance, max_wallet_balance, mediane_wallet_balance, moyenne_wallet_balance, et_wallet_balance, threshold_wallet_balance = calculate_metric(res_all_df, str_key_word='wallet/balance')\n",
        "list_phone_number_user_adverts, sum_operation_adverts, min_adverts, max_adverts, mediane_adverts, moyenne_adverts, et_adverts, threshold_adverts = calculate_metric(res_all_df, str_key_word='adverts')\n",
        "list_phone_number_user_bank, sum_operation_bank, min_bank, max_bank, mediane_bank, moyenne_bank, et_bank, threshold_bank = calculate_metric(res_all_df, str_key_word='bank')\n",
        "list_phone_number_user_update, sum_operation_update, min_update, max_update, mediane_update, moyenne_update, et_update, threshold_update = calculate_metric(res_all_df, str_key_word='update')\n",
        "list_phone_number_user_product, sum_operation_product, min_product, max_product, mediane_product, moyenne_product, et_product, threshold_product = calculate_metric(res_all_df, str_key_word='products')\n",
        "list_phone_number_user_location, sum_operation_Location, min_Location, max_Location, mediane_Location, moyenne_Location, et_Location, threshold_Location = calculate_metric(res_all_df, str_key_word='locations')\n",
        "list_phone_number_user_client_get, sum_operation_client_get, min_client_get, max_client_get, mediane_client_get, moyenne_client_get, et_client_get, threshold_client_get = calculate_metric(res_all_df, str_key_word='client/get')\n",
        "list_phone_number_user_client_v2_create, sum_operation_client_v2_create, min_client_v2_create, max_client_v2_create, mediane_client_v2_create, moyenne_client_v2_create, et_client_v2_create, threshold_client_v2_create = calculate_metric(res_all_df, str_key_word='client/v2/create')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Z73BNJ1Ogm"
      },
      "source": [
        "### Plot des utilisateurs en fonctions des opérations effectuées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzMB3Fbd1QdL"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_users(list_operations, title_operation='lifestyle', threshold=threshold_lifestyle):\n",
        "  bins = np.linspace(math.ceil(min(list_operations)), \n",
        "                    math.floor(max(list_operations)),\n",
        "                    10) # fixed number of bins\n",
        "  plt.axvline(x=threshold, color='red') # pour tracer le threshold \n",
        "  plt.xlim([min(list_operations)-5, max(list_operations)+5])\n",
        "\n",
        "  plt.hist(list_operations, bins=bins, alpha=0.5)\n",
        "  plt.title(f'Distribution des operations ({title_operation}) avec seuil {threshold}')\n",
        "  plt.xlabel(f'somme des opérations {title_operation}')\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvyPtS8x1YBn"
      },
      "source": [
        "### si les données sont à la moyenne plus 3 écart-types alors, il y'a de fortes chances qu'elles soient des outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vom5kIGP1afx"
      },
      "source": [
        "plot_users(sum_operation_lifestyle, 'lifestyle', threshold_lifestyle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvQhV7Vm1xxb"
      },
      "source": [
        "plot_users(sum_operation_accept, 'accept', threshold_accept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7slwD3Gy16tj"
      },
      "source": [
        "plot_users(sum_operation_adverts, 'adverts', threshold_adverts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhnkbV6T2Aa1"
      },
      "source": [
        "plot_users(sum_operation_app_get, 'app_get', threshold_app_get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnsCiTVD2DX2"
      },
      "source": [
        "plot_users(sum_operation_bank, 'bank', threshold_bank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ED97932Giq"
      },
      "source": [
        "plot_users(sum_operation_client, 'client', threshold_client)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7072ck_R3usA"
      },
      "source": [
        "plot_users(sum_operation_client_get, 'client_get', threshold_client_get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rIyxztg3x7U"
      },
      "source": [
        "plot_users(sum_operation_client_v2_create, 'client_v2_create', threshold_client_v2_create)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nMsQmV33eg"
      },
      "source": [
        "plot_users(sum_operation_decline, 'decline', threshold_decline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwP81Hk936kh"
      },
      "source": [
        "plot_users(sum_operation_Location, 'locations', threshold_Location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPoISSKQ3_Ht"
      },
      "source": [
        "plot_users(sum_operation_payment, 'payment', threshold_payment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO3v9dec4F8G"
      },
      "source": [
        "plot_users(sum_operation_preapproved, 'preapproved', threshold_preapproved)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ffhf9lq4JIY"
      },
      "source": [
        "plot_users(sum_operation_product, 'products', threshold_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Np4Usg4f8v"
      },
      "source": [
        "plot_users(sum_operation_update, 'updated', threshold_update)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX3TDdnp4lQ-"
      },
      "source": [
        "plot_users(sum_operation_wallet_balance, 'wallet_balance', threshold_wallet_balance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hznCVyQo404A"
      },
      "source": [
        "### Regrouper les utilisateurs en fonction du seuil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU3WdW7h4xjj"
      },
      "source": [
        "def group_users(list_users, list_phone_number_user, threshold):\n",
        "  '''cette fonction permet de regrouper les utilisateurs\n",
        "    en fonction de si leurs nombres d'opérations sont supérieurs ou inférieurs\n",
        "    à un seuil.  '''\n",
        "\n",
        "  list_users_great_than_threshold = []\n",
        "  list_users_less_than_threshold = []\n",
        "  dict_res = {}\n",
        "\n",
        "  for i, element in enumerate(list_users):\n",
        "    if element > threshold:\n",
        "      dict_res[list_phone_number_user[i]] = 1\n",
        "      list_users_great_than_threshold.append(list_phone_number_user[i])\n",
        "    elif element < threshold:\n",
        "      dict_res[list_phone_number_user[i]] = 0\n",
        "      list_users_less_than_threshold.append(list_phone_number_user[i])\n",
        "  \n",
        "  return dict_res"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY03FvSv45-N"
      },
      "source": [
        "dict_client = group_users(sum_operation_client, list_phone_number_user_client, threshold_client)\n",
        "dict_accept = group_users(sum_operation_accept, list_phone_number_user_accept, threshold_accept)\n",
        "dict_adverts = group_users(sum_operation_adverts, list_phone_number_user_adverts, threshold_adverts)\n",
        "dict_app_get = group_users(sum_operation_app_get, list_phone_number_user_app_get, threshold_app_get)\n",
        "dict_bank = group_users(sum_operation_bank, list_phone_number_user_bank, threshold_bank)\n",
        "dict_client_get = group_users(sum_operation_client_get, list_phone_number_user_client_get, threshold_client_get)\n",
        "dict_client_v2_create = group_users(sum_operation_client_v2_create, list_phone_number_user_client_v2_create, threshold_client_v2_create)\n",
        "dict_decline = group_users(sum_operation_decline, list_phone_number_user_decline, threshold_decline)\n",
        "dict_lifestyle = group_users(sum_operation_lifestyle, list_phone_number_user_lifestyle, threshold_lifestyle)\n",
        "dict_location = group_users(sum_operation_Location, list_phone_number_user_location, threshold_Location)\n",
        "dict_payment = group_users(sum_operation_payment, list_phone_number_user_payment, threshold_payment)\n",
        "dict_preapproved = group_users(sum_operation_preapproved, list_phone_number_user_preapproved, threshold_preapproved)\n",
        "dict_product = group_users(sum_operation_product, list_phone_number_user_product, threshold_product)\n",
        "dict_update = group_users(sum_operation_update, list_phone_number_user_update, threshold_update)\n",
        "dict_wallet_balance = group_users(sum_operation_wallet_balance, list_phone_number_user_wallet_balance, threshold_wallet_balance)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U09ItskE5DOC"
      },
      "source": [
        "### Créer des DataFrame en fonction des utilisateurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXr7KFmF5AaP"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_df(one_dict, key_word='great_less_than_threshold'):\n",
        "\n",
        "  list_key = []\n",
        "  list_value = []\n",
        "  for key in one_dict:\n",
        "    list_key.append(key)\n",
        "    list_value.append(one_dict[key])\n",
        "\n",
        "  dict_res ={'phone_number_user': list_key, key_word: list_value}\n",
        "  return pd.DataFrame(dict_res)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou7-50Kq5Ldx"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de endpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txh-7vCc5Mxv"
      },
      "source": [
        "# Création du dataframe en fonction de l'utilisation du threshold de l'endpoint \"client\"\n",
        "df_client=create_df(dict_client, key_word='great_less_than_threshold_client')\n",
        "df_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SApUu8Bp5Saq"
      },
      "source": [
        "# vérifions les utilisateurs qui ont un seuil = 1\n",
        "df_client[df_client['great_less_than_threshold_client'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2_T0AH05YxB"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"accept\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGaTERP5WKZ"
      },
      "source": [
        "df_accept = create_df(dict_accept, key_word='great_less_than_threshold_accept')\n",
        "df_accept"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZacxHO45eOe"
      },
      "source": [
        "df_accept[df_accept['great_less_than_threshold_accept'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2zredbj8K40"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"adverts\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSEJeQR5kW2"
      },
      "source": [
        "df_adverts = create_df(dict_adverts, key_word='great_less_than_threshold_adverts')\n",
        "df_adverts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GibGOIjC5kCg"
      },
      "source": [
        "df_adverts[df_adverts['great_less_than_threshold_adverts'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Mmdg9p8ROy"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"application get\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDZ47vW08M6T"
      },
      "source": [
        "df_app_get = create_df(dict_app_get, key_word='great_less_than_threshold_app_get')\n",
        "df_app_get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnIduftg8axr"
      },
      "source": [
        "df_app_get[df_app_get['great_less_than_threshold_app_get'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJ0gVoA8k1m"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"bank\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhiDjkYI8efA"
      },
      "source": [
        "df_bank = create_df(dict_bank, key_word='great_less_than_threshold_bank')\n",
        "df_bank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y8gnSOV8iLb"
      },
      "source": [
        "df_bank[df_bank['great_less_than_threshold_bank'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd3D586P8mad"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"client get\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdzM04Hc8mvi"
      },
      "source": [
        "df_client_get = create_df(dict_client_get, key_word='great_less_than_threshold_client_get')\n",
        "df_client_get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrhkn9Us8m7-"
      },
      "source": [
        "df_client_get[df_client_get['great_less_than_threshold_client_get'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKtVk0PH86t5"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"client v2 create\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRJg6XBs8z_W"
      },
      "source": [
        "df_client_v2_create = create_df(dict_client_v2_create, key_word='great_less_than_threshold_client_v2_create')\n",
        "df_client_v2_create"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1DvYDet88Du"
      },
      "source": [
        "df_client_v2_create[df_client_v2_create['great_less_than_threshold_client_v2_create'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUFI0Vj7-bZ9"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"decline\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PRTGobv-cwc"
      },
      "source": [
        "df_decline = create_df(dict_decline, key_word='great_less_than_threshold_decline')\n",
        "df_decline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a94eiVsr-dTL"
      },
      "source": [
        "df_decline[df_decline['great_less_than_threshold_decline'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9wjmMr-nLn"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"payment\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSg9VTBU-oj2"
      },
      "source": [
        "df_payment = create_df(dict_payment, key_word='great_less_than_threshold_payment')"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_5Osjaw-o5X"
      },
      "source": [
        "df_payment[df_payment['great_less_than_threshold_payment'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YDBIWRD-1Tr"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"lifestyle\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__1LTJe-2d5"
      },
      "source": [
        "df_lifestyle = create_df(dict_lifestyle, key_word='great_less_than_threshold_lifestyle')"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VTLGzfN-8tW"
      },
      "source": [
        "df_lifestyle[df_lifestyle['great_less_than_threshold_lifestyle'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jVJm8Tu_AoS"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"location\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kYIq2JJ_CBA"
      },
      "source": [
        "df_location = create_df(dict_location, key_word='great_less_than_threshold_location')"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E21MZd6R_JM6"
      },
      "source": [
        "df_location[df_location['great_less_than_threshold_location'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkfG2CB6_XBv"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"preapproved\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ9INCO9_ckB"
      },
      "source": [
        "df_preapproved = create_df(dict_preapproved, key_word='great_less_than_threshold_preapproved')"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTC_hNd_dCJ"
      },
      "source": [
        "df_preapproved[df_preapproved['great_less_than_threshold_preapproved'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O38pbzk_ptf"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"products\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk9t1WVj_rNl"
      },
      "source": [
        "df_product = create_df(dict_product, key_word='great_less_than_threshold_products')"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fVgcWSi_rqu"
      },
      "source": [
        "df_product[df_product['great_less_than_threshold_products'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8YAlnKR_2RQ"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"update\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvO6a3Ld_yiW"
      },
      "source": [
        "df_update = create_df(dict_update, key_word='great_less_than_threshold_update')"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWLKwkBO_yR4"
      },
      "source": [
        "df_update[df_update['great_less_than_threshold_update'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGnSW9U3AA-S"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"wallet balance\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDTYrFu4ACHv"
      },
      "source": [
        "df_wallet_balance = create_df(dict_wallet_balance, key_word='great_less_than_threshold_wallet_balance')"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lcN60vqACri"
      },
      "source": [
        "df_wallet_balance[df_wallet_balance['great_less_than_threshold_wallet_balance'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfSD3B6VAMB3"
      },
      "source": [
        "## Faire la jointure des dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAJQl-i2AQKL"
      },
      "source": [
        "### premièrement: ordonner les colonnes de numéros de téléphone par ordre croissant\n",
        "### NB: Toujours re-indexer (.reset_index(drop=True)) les dataframe avant de faire des JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXdKXnZTARSa"
      },
      "source": [
        "# Ordonnons les dataset, ensuite ré-indexons les index ce qui nous facilitera l'opération de concatenation.\n",
        "sorted_df_client = df_client.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_accept = df_accept.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_adverts = df_adverts.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_app_get = df_app_get.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_bank = df_bank.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_client_get = df_client_get.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_client_v2_create = df_client_v2_create.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_decline = df_decline.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_payment = df_payment.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_lifestyle = df_lifestyle.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_location = df_location.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_preapproved = df_preapproved.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_product = df_product.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_update = df_update.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_wallet_balance = df_wallet_balance.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwfw3annAZ-U"
      },
      "source": [
        "### Concatenons tous les dataframes pour avoir un dataframe avec lequel on va entrainer nos modèles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch652n2RAcAo"
      },
      "source": [
        "list_users = [sorted_df_client, sorted_df_accept['great_less_than_threshold_accept'], sorted_df_adverts['great_less_than_threshold_adverts'], sorted_df_app_get['great_less_than_threshold_app_get'], \n",
        "              sorted_df_bank['great_less_than_threshold_bank'], sorted_df_client_get['great_less_than_threshold_client_get'], \n",
        "              sorted_df_client_v2_create['great_less_than_threshold_client_v2_create'], sorted_df_decline['great_less_than_threshold_decline'], \n",
        "              sorted_df_payment['great_less_than_threshold_payment'], sorted_df_lifestyle['great_less_than_threshold_lifestyle'], \n",
        "              sorted_df_location['great_less_than_threshold_location'], sorted_df_preapproved['great_less_than_threshold_preapproved'], \n",
        "              sorted_df_product['great_less_than_threshold_products'], sorted_df_update['great_less_than_threshold_update'], \n",
        "              sorted_df_wallet_balance['great_less_than_threshold_wallet_balance']]  # List of our dataframes\n",
        "df_final_users = pd.concat(list_users, axis=1)\n",
        "df_final_users.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zX8XsnlApVy"
      },
      "source": [
        "# Faire la somme de toutes les colonnes et mettre dans le résultat dans la colonne \"score_total\"\n",
        "df_final_users['score_total'] = df_final_users.iloc[:, 1:16].sum(axis=1)\n",
        "df_final_users['score_total'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0CkLTfOAtTB"
      },
      "source": [
        "list_score_total = df_final_users['score_total'].values # convertir une series en list"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7rMOpssA5pY",
        "outputId": "21132aa9-f571-4d08-d4e7-56fc4067f582"
      },
      "source": [
        "list_score_total"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBc7YPRFA_Aj",
        "outputId": "ea07515b-ab63-4879-92e1-0c3bbef84d7d"
      },
      "source": [
        "import numpy as np\n",
        "med = np.median(list_score_total)\n",
        "moy = np.mean(list_score_total)\n",
        "ecart_type_score_total = np.std(list_score_total)\n",
        "threshold_score_total = med + 3*ecart_type_score_total\n",
        "threshold_score_total"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5682822597245236"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "mbiwciKDBCoI",
        "outputId": "ad0cceea-9990-4e41-db41-d5d26a2d5511"
      },
      "source": [
        "plot_users(df_final_users['score_total'].values, 'score_total' , threshold_score_total)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hem3YJT0BLst"
      },
      "source": [
        "### Récupérer les utilisateurs qui sont au delà du seuil.\n",
        "### les utilisateurs qui sont au délà du seuil peuvent etre considérés comme ceux qui ont effectué des transactions douteuses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "0IvgcXr0BKc_",
        "outputId": "7c6dd0dd-e57a-4e65-8740-5bc9fdd897b0"
      },
      "source": [
        "df_resultats = df_final_users[df_final_users['score_total'] > threshold_score_total]\n",
        "df_resultats"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phone_number_user</th>\n",
              "      <th>great_less_than_threshold_client</th>\n",
              "      <th>great_less_than_threshold_accept</th>\n",
              "      <th>great_less_than_threshold_adverts</th>\n",
              "      <th>great_less_than_threshold_app_get</th>\n",
              "      <th>great_less_than_threshold_bank</th>\n",
              "      <th>great_less_than_threshold_client_get</th>\n",
              "      <th>great_less_than_threshold_client_v2_create</th>\n",
              "      <th>great_less_than_threshold_decline</th>\n",
              "      <th>great_less_than_threshold_payment</th>\n",
              "      <th>great_less_than_threshold_lifestyle</th>\n",
              "      <th>great_less_than_threshold_location</th>\n",
              "      <th>great_less_than_threshold_preapproved</th>\n",
              "      <th>great_less_than_threshold_products</th>\n",
              "      <th>great_less_than_threshold_update</th>\n",
              "      <th>great_less_than_threshold_wallet_balance</th>\n",
              "      <th>score_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7035359155</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   phone_number_user  ...  score_total\n",
              "4         7035359155  ...          5.0\n",
              "\n",
              "[1 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhUHOLPeBcLF"
      },
      "source": [
        "df_final_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlAGp5R3Bh-U"
      },
      "source": [
        "### Selon les résultats affichés, la composantes pc2 de PCA répresente les personnes qui ont effectués beaucoup d'achats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYOxOMUBqhd"
      },
      "source": [
        "### on drop la prémière colonne parce que les données qu'on veut passer à notre modèle de clustering doivent etre des nombres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27MlLI8kBjJ2"
      },
      "source": [
        "df_result = df_final_users.drop('phone_number_user', 1)"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71lzaD3sByAP"
      },
      "source": [
        "### Ensuite on convertit notre dataframe en liste pour le passer au modèle de clustering KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WgG9WtB0aZ"
      },
      "source": [
        "# convertir le dataframe en list\n",
        "list_resultats = df_result.values.tolist()"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Ho0joWD56E",
        "outputId": "2a5ed653-0ec1-4c96-d3f7-680cc57fbf33"
      },
      "source": [
        "print(list_resultats)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, nan, 1.0, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 5.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, nan, 0.0, 0.0, 0.0, 0.0, 1.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PS-K60zE83R"
      },
      "source": [
        "cleanedList = [[x for x in y if not np.isnan(x)] for y in list_resultats]"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOBiT2AEb3_",
        "outputId": "af65be51-6672-4b69-eb78-c148e51bcd95"
      },
      "source": [
        "print(cleanedList)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMJVdL1hB6Iz"
      },
      "source": [
        "### Algorithme de clustering KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjgbYe9B5Fk"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkDMcq4qCEuP"
      },
      "source": [
        "### on fait la standardisation parce que la dernière colonne du dataframe a des valeurs prépondérantes (9, 3, etc...) par rapport aux autres pour que toutes les valeurs du dataframe soient comprises entre -1 et 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EEvryCgCGHN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "Scaled_data = StandardScaler().fit_transform(cleanedList)"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "kHYNP33FCMzo",
        "outputId": "92d33759-713e-4d3f-8f41-0b683543d8ad"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import random\n",
        "\n",
        "# Scale data before applying PCA\n",
        "#scaling=StandardScaler()\n",
        " \n",
        "# Use fit and transform method\n",
        "#scaling.fit(df1)\n",
        "#Scaled_data=scaling.transform(df1)\n",
        " \n",
        "# Set the n_components=2\n",
        "principal=PCA(n_components=2)\n",
        "principal.fit(Scaled_data)\n",
        "x=principal.transform(Scaled_data)\n",
        " \n",
        "# Check the dimensions of data after PCA\n",
        "#print(x.shape)\n",
        "\n",
        "principal.components_\n",
        "\n",
        "df_sortie = pd.DataFrame(x, columns=['pc1', 'pc2'])\n",
        "#print(df_sortie)\n",
        "df_filtre = df_sortie[(df_sortie['pc1'] >= 7) & (df_sortie['pc2'] >= 6)]\n",
        "print(df_filtre)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(x[:,0]+random.uniform(0, 1)/10, x[:,1]+random.uniform(0, 1)/100, c='blue', cmap='plasma', marker='x', alpha=0.4)\n",
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [pc1, pc2]\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'pc2')"
            ]
          },
          "metadata": {},
          "execution_count": 250
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAE9CAYAAAB3MsvyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQRUlEQVR4nO3dfYxldX3H8fenrIwuUp926gOLWWIUixst7KiIrVQwikqkfzQpthptm5A0VdGYbrGmbRrTxFDjQ6zVbqDWCtHalVpDUgs+1MZOis4uPsyyWqkPsLiUMY2C1A4Qv/3j3uvO/lh2Zt177rkz+34lk5lz5u79/pidvDnn3JmzqSokSYf8XN8LkKRpYxglqWEYJalhGCWpYRglqWEYJamxqe8FrLRly5batm1b38uQtMHs2bPn+1U1u9bHT1UYt23bxsLCQt/LkLTBJPnusTzeU2lJahhGSWp0GsYkb0qyL8liko8keXiX8yRpHDoLY5LTgDcAc1W1HTgJuLSreZI0Ll2fSm8CHpFkE7AZ+F7H8yTpuHUWxqq6A3gHcBtwEPhhVd3Q1TxJGpcuT6UfA1wCnAE8CTglyauO8LjLkiwkWVhaWupqOZK0Zl2eSr8I+HZVLVXV/cB1wHntg6pqV1XNVdXc7Oyaf/7yuOzZA/Pzh++bnx/sl6Quw3gbcG6SzUkCXAjs73Demi0vw+LioTjOzw+2l5f7XZek6dDZb75U1U1JdgN7gQeAm4FdXc07FucNj1sXFwdvANu3H9ov6cTW6avSVfWnVfX0qtpeVa+uqqk5JmsjaBQljZywv/lypGuMkgRTdhOJSRldUxydPo+2wSNHSSdoGGdmDr+mOHo/M9PfmiRNjxMyjDt2PHifR4qSRk7Ya4yS9FAMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNQyjJDUMoyQ1DKMkNToNY5JHJ9md5OtJ9id5XpfzJGkcNnX8/O8BPlVVv57kZGBzx/Mk6bh1FsYkjwJeALwWoKruA+7rap4kjUuXp9JnAEvAB5PcnOSqJKd0OE+SxqLLMG4CzgHeX1VnA/cCV7QPSnJZkoUkC0tLSx0uR5LWpsswHgAOVNVNw+3dDEJ5mKraVVVzVTU3Ozvb4XIkaW06C2NV3QncnuTM4a4LgVu6midJ49L1q9KvB64dviL9LeC3O54nScet0zBW1ZeBuS5nSNK4+ZsvktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLUMIyS1DCMktQwjJLU6DyMSU5KcnOS67ueJUnjMIkjxsuB/ROYI0lj0WkYk2wFXg5c1eUcSRqnro8Y3w3sBH7S8RxJGpvOwpjkYuCuqtqzyuMuS7KQZGFpaamr5UjSmnV5xPh84BVJvgN8FLggyTXtg6pqV1XNVdXc7Oxsh8uRpLXpLIxV9Zaq2lpV24BLgc9W1au6midJ4+LPMUpSY01hTPKwI+zbstYhVfWvVXXxsSxMkvpy1DAmeWGSA8DBJDck2bbi0zd0uTBJ6stqR4xXAi+pqi3ALuDGJOcOP5dOVyZJPdm0yudPrqp9AFW1O8l+4LokfwhU56uTpB6sFsb7kzyhqu4EqKp9SS4Ergee0vnqJKkHq51KXwE8fuWOqjoAnA+8vatFSVKfjnrEWFWfBkhyCvDjqhr9at89wLs6Xpsk9WKtP8f4GWDziu3NwKfHvxxJ6t9aw/jwqvrRaGP48eajPF6S1q21hvHeJOeMNpLMAT/uZkmS1K/VXpUeeSPwD0m+N9x+IvAb3SxJkvq11iPGrwEfAJaBJeCvgX1dLUqS+rTWMP4dcCbw58B7gacBH+5qUZLUp7WeSm+vqrNWbH8uyS1dLEiS+rbWI8a9K35HmiTPBRa6WZIk9WutR4w7gPkktw23nwx8I8nXgKqqZ3ayOknqwVrDeFGnq5CkKbKmMFbVd7teyLjs2QPLy3DeeYf2zc/DzAzs2NHfuiStHxvunzZYXobFxUEMYfB+cXGwX5LWYq2n0uvG6EhxcXHwBrB9++FHkJJ0NBvuiBEeHEGjKOlYbMgwjk6jH2pbko5mw51Kj64pjk6fR9vgkaOktdlwYZyZOfya4uj9zEx/a5K0vmy4MB7pR3I8UpR0LDbkNUZJOh6GUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqdhTHJ6Uk+l+SWJPuSXN7VLEkap00dPvcDwJuram+SU4E9SW6sqls6nClJx62zI8aqOlhVe4cf3wPsB07rap4kjctErjEm2QacDdw0iXmSdDw6D2OSRwIfB95YVXcf4fOXJVlIsrC0tNT1ciRpVZ2GMcnDGETx2qq67kiPqapdVTVXVXOzs7NdLkeS1qTLV6UDXA3sr6p3djVHksatyyPG5wOvBi5I8uXh28s6nCdJY9HZj+tU1ReAdPX8ktQVf/NFkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIahlGSGoZRkhqGUZIanYYxyUVJvpHk1iRXdDlLksalszAmOQl4H/BS4CzglUnO6moewNveBjt3Hr5v587Bfklaqy6PGJ8D3FpV36qq+4CPApd0OI977oHPf/5QHHfuHGzfc0+XUyVtNF2G8TTg9hXbB4b7DpPksiQLSRaWlpaOa+CVV8L55w9i+NznDt6ff/5gvyStVe8vvlTVrqqaq6q52dnZ436+NoJGUdKx6jKMdwCnr9jeOtzXqSNdY5SkY9FlGL8EPDXJGUlOBi4FPtnhvJ9eUzz/fLjppkOn1cZR0rHoLIxV9QDwOuBfgP3Ax6pqX1fzAE499fBriqNrjqee2uVUSRtNqqrvNfzU3NxcLSws9L0MSRtMkj1VNbfWx/f+4suxSgZvq+2TpJ/VugvjyCiEBlHSuK27MK48818ZxSm6IiBpnVt3YYQHR9AoShqndRnGI11jlKRxWXdhfKjTZ+MoaVzWXRhHRlH0NFrSuG3qewHH6kghNI6SxmndHjFKUlcMo6Spt2cPzM8fvm9+frC/C4ZR0tRbXobFxUNxnJ8fbC8vdzNv3V1jlHTiOe+8wfvFxcEbwPbth/aPm0eMktaFNoJdRREMo6R14kjXGLviqbSkqTe6pjg6fR5tQzdHjoZR0tSbmTn8muLo/cxMN/MMo6Spt2PHg/d5jVGSJsgwSlLDMEpSwzBKUsMwSlLDMEpSwzBKUsMwSlIjNUW3v06yBHx3DE+1Bfj+GJ5nPa+h7/nTsIa+50/DGvqePw1r2AKcUlWza/0DUxXGcUmyUFVzJ/Ia+p4/DWvoe/40rKHv+dOwhp9lvqfSktQwjJLU2Khh3NX3Auh/DX3Ph/7X0Pd86H8Nfc+H/tdwzPM35DVGSToeG/WIUZJ+ZhsujEkuSvKNJLcmuWLCs09P8rkktyTZl+TySc5v1nJSkpuTXN/D7Ecn2Z3k60n2J3leD2t40/DvYDHJR5I8fAIz/ybJXUkWV+x7bJIbk3xz+P4xE57/F8O/h68m+cckj+5q/kOtYcXn3pykkmyZ9Pwkrx9+HfYluXK159lQYUxyEvA+4KXAWcArk5w1wSU8ALy5qs4CzgV+f8LzV7oc2N/T7PcAn6qqpwPPmvQ6kpwGvAGYq6rtwEnApRMY/bfARc2+K4DPVNVTgc8Mtyc5/0Zge1U9E/hP4C0dzn+oNZDkdODFwG2Tnp/khcAlwLOq6hnAO1Z7kg0VRuA5wK1V9a2qug/4KIMvyERU1cGq2jv8+B4GQThtUvNHkmwFXg5c1cPsRwEvAK4GqKr7quoHk14Hg7vTPyLJJmAz8L2uB1bVvwH/0+y+BPjQ8OMPAb82yflVdUNVPTDc/A9ga1fzH2oNQ+8CdgKdvqjxEPN/D3h7VS0PH3PXas+z0cJ4GnD7iu0D9BAmgCTbgLOBm3oY/24G34Q/6WH2GcAS8MHhqfxVSU6Z5AKq6g4GRwW3AQeBH1bVDZNcwwqPr6qDw4/vBB7f0zoAfgf450kPTXIJcEdVfWXSs4eeBvxKkpuSfD7Js1f7AxstjFMhySOBjwNvrKq7Jzz7YuCuqtozybkrbALOAd5fVWcD99Lt6eODDK/jXcIg0k8CTknyqkmu4Uhq8CMgvfwYSJK3MrjUc+2E524G/gj4k0nObWwCHsvg8tYfAB9LkqP9gY0WxjuA01dsbx3um5gkD2MQxWur6rpJzh56PvCKJN9hcCnhgiTXTHD+AeBAVY2OlHczCOUkvQj4dlUtVdX9wHVAh/900lH9d5InAgzfr3oaN25JXgtcDPxWTf7n857C4H9QXxl+T24F9iZ5wgTXcAC4rga+yOBM6qgvAG20MH4JeGqSM5KczOCC+ycnNXz4f6Grgf1V9c5JzV2pqt5SVVurahuD//7PVtXEjpaq6k7g9iRnDnddCNwyqflDtwHnJtk8/Du5kP5eiPok8Jrhx68B/mmSw5NcxOCyyiuq6n8nORugqr5WVb9QVduG35MHgHOG3yeT8gnghQBJngaczGo3taiqDfUGvIzBq2//Bbx1wrN/mcGp0leBLw/fXtbj1+JXget7mPtLwMLw6/AJ4DE9rOHPgK8Di8CHgZkJzPwIg2ua9zMIwO8Cj2PwavQ3gU8Dj53w/FsZXHcffT9+YNJfg+bz3wG2TPhrcDJwzfB7YS9wwWrP42++SFJjo51KS9JxM4yS1DCMktQwjJLUMIyS1DCMWveSPG54V6MfJfnLvtej9W9T3wuQxuD/gD8Gtg/fpOPiEaOmUpJtw/vnXTu8p+Pu4W+yPDvJfJKvJPliklOr6t6q+gKDQErHzTBqmp0J/FVV/SJwN/A64O+By6vqWQx+J/rHPa5PG5Rh1DS7var+ffjxNcBLgINV9SWAqrq7Dt1rUBobw6hp1v6+6kRv4aYTl2HUNHvyin8v5jcZ3IH6iaMbjSY5dXiHbmmsvImEptLwDuifYnCXnh0Mbl32auAZwHuBRzC4vviiqvrR8F5/P8/gTio/AF5cVZO+3Zk2CMOoqTQM4/U1+MespInyVFqSGh4xSlLDI0ZJahhGSWoYRklqGEZJahhGSWoYRklq/D9lIjWPFqgz1QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMBr6KTbCx61",
        "outputId": "95b36468-03bb-4d87-869e-0fd33c35e007"
      },
      "source": [
        "df_filtre_1 = df_sortie[(df_sortie['pc1'] <= 5) & (df_sortie['pc2'] < -1)]\n",
        "print(df_filtre_1)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [pc1, pc2]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhk7v4WhC-io"
      },
      "source": [
        "### Fonction pour faire le plot de tous nos points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12hJ1ndDHMj"
      },
      "source": [
        "cleanedList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCRmlkb6C9jN"
      },
      "source": [
        "def scatter_plot(list_points):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in list_points:\n",
        "    x.append(i[0])\n",
        "    y.append(i[1])\n",
        "\n",
        "  plt.scatter(x,y, s=200)\n",
        "  plt.show()"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qti72J6fDEfH"
      },
      "source": [
        "scatter_plot(cleanedList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxBE-OyYDQOr"
      },
      "source": [
        "# on choisit trois clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXHKKxe6DTZE",
        "outputId": "8f2f3245-c92c-4b62-f194-1a110fb9fe9a"
      },
      "source": [
        "y_pred = kmeans.fit_predict(cleanedList)\n",
        "y_pred"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2,\n",
              "       0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    }
  ]
}